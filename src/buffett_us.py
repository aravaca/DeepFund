# SPDX-FileCopyrightText: Â© 2025 Hyungsuk Choi <chs_3411@naver[dot]com>, University of Maryland
# SPDX-License-Identifier: MIT

import yfinance as yf
import pandas as pd
import numpy as np
import requests
import string
import datetime as dt
import openpyxl
import math
from queue import Queue
import threading
import time
import polars as pl
import io
from yahooquery import Ticker

# import shelve
from bs4 import BeautifulSoup
from urllib.request import urlopen
import smtplib
from email.message import EmailMessage
from email.headerregistry import Address
import os
import ta  # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬
import re
import matplotlib.pyplot as plt
from scipy.optimize import minimize
from scipy.stats import norm
from scipy.stats import skew, kurtosis
from scipy.stats.mstats import gmean
from scipy.stats import linregress
from datetime import datetime, timedelta, date
from google import genai
from google.genai import types
import json
import markdown

# í˜„ì¬ íŒŒì¼ (src/buffett_us.py) ê¸°ì¤€ìœ¼ë¡œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
project_root = os.path.dirname(os.path.dirname(__file__))
# backend ê²½ë¡œë¡œ ì—‘ì…€ ì €ì¥
excel_path = os.path.join(project_root, "backend", "deep_fund.xlsx")

################ DEPENDENCIES ###########################

# pip install -r requirements.txt

#########################################################
recipients = ["chs_3411@naver.com", "eljm2080@gmail.com", "hyungsukchoi3411@gmail.com"]

# JSONì—ì„œ ì´ë©”ì¼ ë¶ˆëŸ¬ì˜¤ê¸°

try:
    recipients_json_path = os.path.join(project_root, "backend", "recipients.json")
    with open(recipients_json_path, "r") as f:
        loaded_emails = json.load(f)
        for email in loaded_emails:
            if email not in recipients:
                recipients.append(email)  # append í˜•íƒœë¡œ ì¶”ê°€
except (FileNotFoundError, json.JSONDecodeError):
    print("âš ï¸ recipients.json íŒŒì¼ì´ ì—†ê±°ë‚˜ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.")

recipients = list(set(recipients))

################ PREDETERMINED FIELDS ###################

EMAIL = os.environ["EMAIL_ADDRESS"]
PASSWORD = os.environ["EMAIL_PASSWORD"]
fmp_key = os.environ["FMP_API_KEY"]
marketaux_api = os.environ["MARKETAUX_API"]
NUM_THREADS = 2  # multithreading

country = "US"

## IMPORTANT!! limit must match NYSE + NASDAQ #######

limit = 250  # GEMINI FLASH 2.5 -->>> max 250 requests/day #
NYSE= 175
NASDAQ = 75

## IMPORTANT!! limit must match NYSE + NASDAQ #######


sp500 = True

# top X tickers to optimize
opt = 20

# for news
news_lookup = 100

# for moat
moat_limit = 200
#########################################################


##########################################################################################################
# Initialize the client (picks up your API key automatically from env vars, or pass api_key explicitly)
client = genai.Client(api_key=os.environ["GEMINI_API_KEY"])


# Define the grounding tool
grounding_tool = types.Tool(google_search=types.GoogleSearch())

# Configure generation settings
config = types.GenerateContentConfig(tools=[grounding_tool])

##########################################################################################################


# print('May take up to few minutes...')

today = dt.datetime.today().weekday()
weekend = today - 4  # returns 1 for saturday, 2 for sunday
formattedDate = (
    (dt.datetime.today() - dt.timedelta(days=weekend)).strftime("%Y%m%d")
    if today >= 5
    else dt.datetime.today().strftime("%Y%m%d")
)

three_months_approx = dt.datetime.today() - dt.timedelta(days=90)
formattedDate_3m_ago = three_months_approx.strftime("%Y%m%d")

date_kr = dt.datetime.strptime(formattedDate, "%Y%m%d").strftime("%-mì›” %-dì¼")
date_kr_month = dt.datetime.strptime(formattedDate, "%Y%m%d").strftime("%-mì›”")
date_kr_ymd = dt.datetime.strptime(formattedDate, "%Y%m%d").strftime(
    "%Yë…„ %-mì›” %-dì¼"
)  # Unix

esg_dict = {
    "LAG_PERF": "ë¯¸í¡",
    "AVG_PERF": "ë³´í†µ",
    "LEAD_PERF": "ìš°ìˆ˜",
}

data = []
data_lock = threading.Lock()


def get_tickers(country: str, limit: int, sp500: bool):
    if country is not None:
        return get_tickers_by_country(country, limit, fmp_key)  # US, JP, KR
    elif sp500:
        return pl.read_csv(
            "https://datahub.io/core/s-and-p-500-companies/r/constituents.csv"
        )["Symbol"].to_list()
    elif not sp500:
        nasdaq100_url = "https://en.wikipedia.org/wiki/NASDAQ-100"
        nasdaq100 = pd.read_html(nasdaq100_url, header=0)[
            4
        ]  # Might need to adjust index (5th table on the page)
        return nasdaq100["Ticker"].tolist()
    else:
        raise Exception("No tickers list satisfies the given parameter")

NASDAQ_LISTED_URL = "https://www.nasdaqtrader.com/dynamic/symdir/nasdaqlisted.txt"
OTHER_LISTED_URL = "https://www.nasdaqtrader.com/dynamic/symdir/otherlisted.txt"

def _read_symbol_file(url: str) -> pd.DataFrame:
    txt = requests.get(url, timeout=20).text
    # drop footer lines like "File Creation Time: ..."
    lines = [ln for ln in txt.splitlines() if not ln.startswith("File Creation")]
    return pd.read_csv(io.StringIO("\n".join(lines)), sep="|")

def load_nyse_symbols() -> list[str]:
    df = _read_symbol_file(OTHER_LISTED_URL)
    # 'N' = NYSE, exclude ETFs/test issues
    nyse = df[(df["Exchange"] == "N") & (df["ETF"] == "N") & (df["Test Issue"] == "N")]
    return sorted(nyse["ACT Symbol"].dropna().astype(str).unique().tolist())

def load_nasdaq_symbols() -> list[str]:
    df = _read_symbol_file(NASDAQ_LISTED_URL)
    # exclude ETFs/test issues; NASDAQ file has flags
    # Some files have "ETF" column; if missing, default False
    if "ETF" in df.columns:
        nas = df[(df["ETF"] == "N") & (df["Test Issue"] == "N")]
    else:
        nas = df[df["Test Issue"] == "N"]
    return sorted(nas["Symbol"].dropna().astype(str).unique().tolist())

def _chunks(lst, size=400):
    for i in range(0, len(lst), size):
        yield lst[i:i+size]

def fetch_market_caps(tickers: list[str]) -> pd.DataFrame:
    rows = []
    for chunk in _chunks(tickers, 400):
        yq = Ticker(chunk, asynchronous=True)
        price = yq.price  # dict: {symbol: {...}}
        for sym, info in (price or {}).items():
            if not isinstance(info, dict):
                continue
            mc = info.get("marketCap")
            if isinstance(mc, (int, float)) and mc > 0:
                rows.append((sym, mc))
    return pd.DataFrame(rows, columns=["Ticker", "MarketCap"])

def top_by_marketcap(symbols: list[str], n: int | None = None) -> pd.DataFrame:
    df = fetch_market_caps(symbols)
    df = df.sort_values("MarketCap", ascending=False).reset_index(drop=True)
    return df if n is None else df.head(n).copy()

def get_ordered_lists(n_nyse: int = 500, n_nasdaq: int = 500):
    nyse_syms = load_nyse_symbols()
    nasdaq_syms = load_nasdaq_symbols()

    nyse_top = top_by_marketcap(nyse_syms, n_nyse) # DataFrame: Ticker, MarketCap
    nasdaq_top = top_by_marketcap(nasdaq_syms, n_nasdaq) # DataFrame: Ticker, MarketCap

    # If you want one combined ranking across both:
    combined = pd.concat([
        nyse_top.assign(Exchange="NYSE"),
        nasdaq_top.assign(Exchange="NASDAQ")
    ], ignore_index=True).sort_values("MarketCap", ascending=False).reset_index(drop=True)

    # Also return as Python lists if you only need tickers
    nyse_list = nyse_top["Ticker"].tolist()
    nasdaq_list = nasdaq_top["Ticker"].tolist()
    return nyse_list, nasdaq_list, nyse_top, nasdaq_top, combined

# Example:
def get_tickers_by_country(country: str, limit: int, apikey: str):

    nyse_500, nasdaq_500, nyse_df, nasdaq_df, combined_df = get_ordered_lists(NYSE, NASDAQ)

    return nyse_500 + nasdaq_500


def safe_check(val):
    return val is not None and not (isinstance(val, float) and np.isnan(val))


def quant_style_score(
    price_vs_fair_upper=None,
    price_vs_fair_lower=None,
    fcf_yield_rank=None,
    fcf_vs_treasury_spread=None,
    per=None,
    per_rank=None,
    pbr_rank=None,
    de=None,
    cr=None,
    industry_per=None,
    roe_z=None,
    roa_z=None,
    roe=None,
    roa=None,
    icr=None,
    fcf_cagr_rank=None,
    eps_cagr_rank=None,
    div_cagr_rank=None,
    eps=None,
    div_yield=None,
    opinc_yoy=None,
    opinc_qoq=None,
    industry_roe=None,
    industry_roa=None,
):
    valuation_score = 0
    earnings_momentum_score = 0

    # === Valuation (30ì ) ===
    if safe_check(price_vs_fair_upper) and price_vs_fair_upper > 0:
        valuation_score += min(price_vs_fair_upper * 5, 1.5)  # DCF 5%
    if safe_check(price_vs_fair_lower) and price_vs_fair_lower > 0:
        valuation_score += min(price_vs_fair_lower * 6, 1.5)  # ë³´ìˆ˜ì  DCF
    if safe_check(fcf_vs_treasury_spread):
        if fcf_vs_treasury_spread > 0:
            valuation_score += min(fcf_vs_treasury_spread * 10, 1.5)  # FCF spread
        else:
            valuation_score -= 0.5
    if safe_check(fcf_yield_rank):
        valuation_score += fcf_yield_rank * 1.5  # FCF ìˆ˜ìµë¥  5%
    if safe_check(per_rank):
        valuation_score += (1 - per_rank) * 3.0  # PER/EY 10%
    if safe_check(per) and safe_check(industry_per):
        if per < industry_per * 0.7:
            valuation_score += 0.5
        elif per > industry_per * 1.3:
            valuation_score -= 0.5
    if safe_check(pbr_rank):
        valuation_score += (1 - pbr_rank) * 1.5  # PBR 5%
    if safe_check(de):
        if 0 < de <= 0.5:
            valuation_score += 0.75
        elif de > 1.0:
            valuation_score -= 0.5
    if safe_check(cr):
        if 1.5 <= cr <= 2.5:
            valuation_score += 0.75
        elif cr < 1.0:
            valuation_score -= 0.5

    # === Fundamental Momentum (20ì ) ===
    if safe_check(roe_z):
        earnings_momentum_score += min(max(roe_z, -2), 2) * 1.25  # ROE ê°œì„ 
    if safe_check(roa_z):
        earnings_momentum_score += min(max(roa_z, -2), 2) * 1.25  # ROA ê°œì„ 
    if not safe_check(roe_z) and safe_check(roe) and safe_check(industry_roe):
        if roe > industry_roe:
            earnings_momentum_score += 0.5
    if not safe_check(roa_z) and safe_check(roa) and safe_check(industry_roa):
        if roa > industry_roa:
            earnings_momentum_score += 0.5
    if safe_check(icr):
        if icr >= 10:
            earnings_momentum_score += 1
        elif icr >= 5:
            earnings_momentum_score += 0.5
        elif icr < 1:
            earnings_momentum_score -= 0.5
    if safe_check(fcf_cagr_rank):
        earnings_momentum_score += fcf_cagr_rank * 1.5
    if safe_check(eps_cagr_rank):
        earnings_momentum_score += eps_cagr_rank * 1.5
    if safe_check(div_cagr_rank):
        earnings_momentum_score += div_cagr_rank * 1.0
    if safe_check(eps):
        if eps >= 0.3:
            earnings_momentum_score += 1.5
        elif eps >= 0.1:
            earnings_momentum_score += 0.75
        elif eps < 0:
            earnings_momentum_score -= 1
    if safe_check(div_yield):
        if div_yield >= 0.1:
            earnings_momentum_score += 0.75
        elif div_yield >= 0.08:
            earnings_momentum_score += 0.5
        elif div_yield >= 0.06:
            earnings_momentum_score += 0.25
        elif div_yield < 0.02:
            earnings_momentum_score -= 0.5
    if safe_check(opinc_yoy):
        if opinc_yoy > 0.2:
            earnings_momentum_score += 1.5
        elif opinc_yoy > 0.05:
            earnings_momentum_score += 1
        elif opinc_yoy < 0:
            earnings_momentum_score -= 1
    if safe_check(opinc_qoq):
        if opinc_qoq > 0.1:
            earnings_momentum_score += 1
        elif opinc_qoq > 0:
            earnings_momentum_score += 0.5
        elif opinc_qoq < 0:
            earnings_momentum_score -= 0.5

    return round(valuation_score, 2), round(earnings_momentum_score, 2)


def get_fcf_yield_and_cagr(ticker, yf_ticker, api_key="YOUR_API_KEY"):
    def try_fmp(ticker, api_key):
        try:
            # 1. Market cap
            profile_url = f"https://financialmodelingprep.com/api/v3/profile/{ticker}?apikey={api_key}"
            profile_resp = requests.get(profile_url)
            if profile_resp.status_code != 200 or not profile_resp.json():
                return (None, None, [])

            market_cap = profile_resp.json()[0].get("mktCap")
            if market_cap is None or market_cap == 0:
                return (None, None, [])

            # 2. FCF list
            url = f"https://financialmodelingprep.com/api/v3/cash-flow-statement/{ticker}?limit=10&apikey={api_key}"
            response = requests.get(url)
            if response.status_code != 200 or not response.json():
                return (None, None, [])

            data = response.json()
            fcf_list = [
                item["freeCashFlow"]
                for item in data
                if item.get("freeCashFlow") is not None
            ]
            fcf_list = fcf_list[::-1]

            if len(fcf_list) < 2:
                return (None, None, fcf_list)

            # 3. FCF Yield
            latest_fcf = fcf_list[-1]
            fcf_yield = round((latest_fcf / market_cap) * 100, 2) if latest_fcf else 0.0

            # 4. FCF CAGR
            initial_fcf, final_fcf = fcf_list[0], fcf_list[-1]
            n_years = len(fcf_list) - 1
            if initial_fcf <= 0 or final_fcf <= 0:
                fcf_cagr = None
            else:
                fcf_cagr = round(
                    ((final_fcf / initial_fcf) ** (1 / n_years) - 1) * 100, 2
                )

            return (fcf_yield, fcf_cagr, fcf_list)

        except Exception:
            return (None, None, [])

    def try_yf(ticker):
        try:

            market_cap = ticker.info.get("marketCap")
            if market_cap is None or market_cap == 0:
                return (None, None, [])

            cashflow_df = ticker.cashflow
            if cashflow_df.empty or "Free Cash Flow" not in cashflow_df.index:
                return (None, None, [])

            fcf_series = cashflow_df.loc["Free Cash Flow"].dropna()[::-1]
            if len(fcf_series) < 2:
                return (None, None, fcf_series.tolist())

            fcf_list = fcf_series.tolist()
            latest_fcf = fcf_series.iloc[-1]
            fcf_yield = (
                round((latest_fcf / market_cap) * 100, 2) if latest_fcf else None
            )

            initial_fcf, final_fcf = fcf_series.iloc[0], fcf_series.iloc[-1]
            n_years = len(fcf_series) - 1
            if initial_fcf <= 0 or final_fcf <= 0:
                fcf_cagr = None
            else:
                fcf_cagr = round(
                    ((final_fcf / initial_fcf) ** (1 / n_years) - 1) * 100, 2
                )

            return (fcf_yield, fcf_cagr, fcf_list)

        except Exception:
            return (None, None, [])

    result = try_yf(yf_ticker)
    if result is not None and (
        result[0] is not None
        or result[1] is not None
        or (result[2] and len(result[2]) > 0)
    ):
        return result

    result = try_fmp(ticker, api_key)
    # FMP ê²°ê³¼ê°€ ëª¨ë‘ None/ë¹ˆ ë¦¬ìŠ¤íŠ¸ë©´ yfinance ì‹œë„
    if result is not None and (
        result[0] is not None
        or result[1] is not None
        or (result[2] and len(result[2]) > 0)
    ):
        return result

    return (None, None, [])  # Always return a tuple


def get_10yr_treasury_yield():
    try:
        tnx = yf.Ticker("^TNX")
        tnx_data = tnx.history(period="1d")
        latest_yield = tnx_data["Close"].iloc[-1]
        return round(latest_yield, 2)  # Already in percent
    except Exception as e:
        return f"Error fetching yield: {e}"


def dcf_valuation(
    fcf_history,
    discount_rate,
    long_term_growth,
    years=10,
    shares_outstanding=None,
    cagr=0.05,
):

    if not fcf_history or len(fcf_history) < 2:
        return (None, None)

    start_fcf = fcf_history[0]  # oldest
    end_fcf = fcf_history[-1]  # most recent

    if start_fcf <= 0 or end_fcf <= 0:
        return (None, None)

    if discount_rate <= long_term_growth:
        return (None, None)  # Invalid terminal growth assumption

    if cagr is None or cagr <= 0:
        return (None, None)  # Invalid CAGR assumption
    est_cagr = cagr / 100.0 * 0.6  # conservative
    growth_rate = min(est_cagr, discount_rate)  # Ensure growth rate is reasonable

    # Project FCFs
    projected_fcfs = [end_fcf * (1 + growth_rate) ** i for i in range(1, years + 1)]

    # Terminal Value
    terminal_value = (
        projected_fcfs[-1] * (1 + long_term_growth) / (discount_rate - long_term_growth)
    )

    # Discounting
    discounted_fcfs = [
        fcf / ((1 + discount_rate) ** i) for i, fcf in enumerate(projected_fcfs, 1)
    ]
    discounted_terminal_value = terminal_value / ((1 + discount_rate) ** years)

    enterprise_value = sum(discounted_fcfs) + discounted_terminal_value

    if shares_outstanding is None or shares_outstanding <= 0:
        return (None, None)

    intrinsic_value = enterprise_value / shares_outstanding

    return (
        float(intrinsic_value),
        growth_rate * 100,
    )  # Return intrinsic value and growth rate in percentage


def get_trading_volume_vs_avg20(ticker_symbol: str) -> float:
    try:
        # Fetch 21 days of data
        ticker = yf.Ticker(ticker_symbol)
        hist = ticker.history(period="21d")

        if len(hist) < 21:
            return None  # Not enough data

        today_volume = hist["Volume"].iloc[-1]
        avg_volume_20 = hist["Volume"].iloc[:-1].mean()

        if avg_volume_20 == 0 or pd.isna(today_volume) or pd.isna(avg_volume_20):
            return None  # Invalid data

        # Return ratio (e.g., 1.5 means 150% of avg volume)
        return round(today_volume / avg_volume_20, 1)

    except Exception as e:
        print(f"[Error] {ticker_symbol}: {e}")
        return None


def has_stable_dividend_growth_cagr(ticker):
    try:
        stock = ticker
        divs = stock.dividends

        if divs.empty:
            return None

        # Sum dividends annually
        annual_divs = divs.groupby(divs.index.year).sum()

        # Need at least 10 full years
        if len(annual_divs) < 10:
            return None

        # Get last full year (last year completed)
        last_year = dt.datetime.today().year - 1

        # Ensure we have dividends data up to last_year
        if last_year not in annual_divs.index:
            return None

        # Select exactly 10 years ending at last_year
        recent_years = sorted(annual_divs.index)
        recent_years = [
            year for year in recent_years if last_year - 9 <= year <= last_year
        ]

        if len(recent_years) < 10:
            return None

        last_10_divs = [annual_divs[year] for year in recent_years]

        div_start = last_10_divs[0]
        div_end = last_10_divs[-1]

        # Validate data
        if div_start <= 0 or div_end <= 0:
            return None

        periods = len(last_10_divs) - 1  # 9 periods for 10 years

        cagr = (div_end / div_start) ** (1 / periods) - 1

        return cagr  # returns float (e.g., 0.05 for 5%)

    except Exception:
        return None


def compute_eps_growth_slope(ticker):
    try:
        income_stmt = ticker.financials  # Annual financials DataFrame

        if "Diluted EPS" not in income_stmt.index:
            return None

        eps_series = income_stmt.loc["Diluted EPS"].dropna()
        eps_series = eps_series.sort_index()  # Oldest to newest

        # Keep last 5 years
        current_year = dt.datetime.today().year
        eps_series = eps_series[
            [col for col in eps_series.index if col.year >= current_year - 5]
        ]

        if len(eps_series) < 2:
            return None

        eps_list = eps_series.tolist()

        x = list(range(len(eps_list)))
        slope, _, _, _, _ = linregress(x, eps_list)
        return slope

    except Exception:
        return None


# gets the most recent interest coverage ratio available
def get_interest_coverage_ratio(ticker):
    financials = (
        ticker.financials
    )  # Annual financials, columns = dates (most recent first)
    ratio = None
    if not financials.columns.empty:
        for date in financials.columns:
            if date.year < dt.datetime.today().year - 5:  # sift out old data
                return None

            try:
                ebit = financials.loc["Operating Income", date]
                interest_expense = financials.loc["Interest Expense", date]
                if (
                    math.isnan(interest_expense)
                    or math.isnan(ebit)
                    or not interest_expense
                    or ebit is None
                    or interest_expense is None
                ):
                    continue  # Avoid division by zero
                else:
                    ratio = round((ebit / abs(interest_expense)), 2)
                    break
            except KeyError:
                continue
        return ratio
    else:
        return None


### 1mo ver.
def get_percentage_change(ticker):
    ticker_obj = yf.Ticker(ticker)

    # Get last 1 month of price data
    data = ticker_obj.history(period="1mo")

    if len(data) >= 2:
        start_close = data["Close"].iloc[0]
        end_close = data["Close"].iloc[-1]

        if start_close != 0:
            percent_change = ((end_close - start_close) / start_close) * 100
            sign = "+" if percent_change >= 0 else ""
            return f" ({sign}{percent_change:.2f}%)"
        else:
            return " ()"
    else:
        return " ()"


def get_percentage_change_ttm(ticker):
    ticker_obj = yf.Ticker(ticker)
    today = date.today()
    start_of_month = today.replace(day=1)

    # Fetch price data from start of month to today

    data = ticker_obj.history(start=start_of_month, end=today + timedelta(days=1))
    print(data)

    if len(data) >= 2:
        # Use the first available trading day as the start
        start_close = data["Close"].iloc[0]
        end_close = data["Close"].iloc[-1]

        if start_close and not pd.isna(start_close):
            percent_change = ((end_close - start_close) / start_close) * 100
            sign = "+" if percent_change >= 0 else ""
            return f" ({sign}{percent_change:.2f}%)"
        else:
            return " ()"
    else:
        return " ()"



debug_download_fullratio: bool = False  

def _parse_threecol_text(text: str, value_colname: str) -> pl.DataFrame:
    """
    'Industry Value Count' í˜•ì‹ì˜ í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±.
    value_colname: "ROE", "ROA", "P/E Ratio" ë“±
    """
    lines = [ln.strip() for ln in text.strip().splitlines() if ln.strip()]

    # --- 1) í—¤ë” ì²˜ë¦¬ (average ë“± ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±° í›„ íŒë³„)
    if lines:
        hdr = lines[0].lower()
        hdr = hdr.replace("average", "").strip() # ğŸ”‘ average ì œê±°
        if "industry" in hdr and (value_colname.lower() in hdr or "ratio" in hdr):
            lines = lines[1:]

    rows = []
    pat = re.compile(r"^(?P<industry>.+?)\s+(?P<val>[-+]?\d+(?:\.\d+)?)\s+(?P<count>\d+)\s*$")

    for ln in lines:
        parts = re.split(r"\t+", ln)
        if len(parts) >= 3:
            industry = parts[0].strip()
            val = float(parts[1].replace(",", ""))
            count = int(parts[2].replace(",", ""))
        else:
            m = pat.match(ln)
            if not m:
                continue
            industry = m.group("industry").strip()
            val = float(m.group("val"))
            count = int(m.group("count"))
        rows.append({"Industry": industry, value_colname: val, "Count": count})

    if not rows:
        raise ValueError(f"ìœ íš¨í•œ ë°ì´í„° í–‰ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ({value_colname})")

    return pl.DataFrame(rows)

def parse_industry_per_from_text(text: str) -> pl.DataFrame:    
    return _parse_threecol_text(text, "P/E Ratio")

def parse_industry_roe_from_text(text: str) -> pl.DataFrame:
    return _parse_threecol_text(text, "ROE")

def parse_industry_roa_from_text(text: str) -> pl.DataFrame:
    return _parse_threecol_text(text, "ROA")
#
#### SOURCE: https://fullratio.com/pe-ratio-by-industry ####
#### 2025ë…„ 9ì›” ê¸°ì¤€ ì—…ë°ì´íŠ¸ ì™„ë£Œ ####
df_per  = parse_industry_per_from_text("""Industry	Average P/E ratio	Number of companies
Advertising Agencies	31.13	27
Aerospace & Defense	35.32	57
Agricultural Inputs	22.15	11
Airlines	17.9	13
Aluminum	27.48	4
Apparel Manufacturing	20.28	15
Apparel Retail	17.71	29
Asset Management	13.42	83
Auto Manufacturers	8.32	17
Auto Parts	17.72	44
Auto & Truck Dealerships	17.86	20
Banks - Diversified	14.64	5
Banks - Regional	14.06	288
Beverages - Non-Alcoholic	26.32	12
Biotechnology	19.45	476
Broadcasting	12.21	10
Building Materials	24.47	10
Building Products & Equipment	23.95	27
Business Equipment & Supplies	12.79	5
Capital Markets	21.16	48
Chemicals	11.47	15
Communication Equipment	30.59	43
Computer Hardware	20.81	28
Conglomerates	27.15	15
Consulting Services	27.05	14
Copper	21.3	4
Credit Services	13.35	40
Diagnostics & Research	26.33	46
Discount Stores	29.54	8
Drug Manufacturers - General	20.05	14
Drug Manufacturers - Specialty & Generic	21.72	48
Education & Training Services	21.63	19
Electrical Equipment & Parts	25.48	39
Electronic Components	40.9	35
Electronic Gaming & Multimedia	16.36	12
Electronics & Computer Distribution	19.27	9
Engineering & Construction	35.22	36
Entertainment	38.74	35
Farm & Heavy Construction Machinery	19.74	19
Farm Products	23.85	15
Financial Data & Stock Exchanges	26.43	11
Food Distribution	32.77	11
Footwear & Accessories	21.24	9
Furnishings, Fixtures & Appliances	18.82	25
Gambling	17.69	9
Gold	31.94	31
Grocery Stores	17.38	9
Healthcare Plans	14.28	11
Health Information Services	43.24	40
Home Improvement Retail	26.52	8
Household & Personal Products	22.03	24
Industrial Distribution	31.71	17
Information Technology Services	24.53	49
Insurance Brokers	22.57	13
Insurance - Diversified	11.96	9
Insurance - Life	15.09	15
Insurance - Property & Casualty	14.1	36
Insurance - Reinsurance	12.99	8
Insurance - Specialty	11.52	20
Integrated Freight & Logistics	19.04	17
Internet Content & Information	22.34	45
Internet Retail	22.8	26
Leisure	24.03	23
Lodging	27.76	8
Marine Shipping	14.75	24
Medical Care Facilities	19.7	38
Medical Devices	30.13	110
Medical Distribution	25.52	6
Medical Instruments & Supplies	32.54	42
Metal Fabrication	22.2	15
Mortgage Finance	15.81	15
Oil & Gas E&P	12.74	60
Oil & Gas Equipment & Services	15.92	42
Oil & Gas Integrated	19.45	6
Oil & Gas Midstream	14.93	36
Oil & Gas Refining & Marketing	20.44	17
Packaged Foods	20.11	46
Packaging & Containers	19.38	20
Personal Services	24.38	10
Pollution & Treatment Controls	32.96	12
Railroads	17.85	8
Real Estate - Development	20.08	8
Real Estate Services	39.49	30
Recreational Vehicles	28.26	11
REIT - Diversified	27.55	16
REIT - Healthcare Facilities	31.77	16
REIT - Hotel & Motel	27.27	14
REIT - Industrial	27.25	17
REIT - Mortgage	16.53	40
REIT - Office	41.79	22
REIT - Residential	31.23	20
REIT - Retail	27.9	26
REIT - Specialty	38.16	19
Rental & Leasing Services	20.28	18
Residential Construction	12.48	22
Resorts & Casinos	23.48	16
Restaurants	20.35	43
Scientific & Technical Instruments	36.82	24
Security & Protection Services	22.75	15
Semiconductor Equipment & Materials	29.99	27
Semiconductors	46.61	60
Software - Application	40.95	169
Software - Infrastructure	31.79	119
Solar	21.22	19
Specialty Business Services	28.52	31
Specialty Chemicals	26.9	50
Specialty Industrial Machinery	28.79	68
Specialty Retail	22.98	36
Staffing & Employment Services	26.53	21
Steel	18.52	11
Telecom Services	18.76	33
Thermal Coal	17.34	6
Tobacco	19.69	8
Tools & Accessories	24.89	9
Travel Services	25.58	12
Trucking	28.15	13
Utilities - Diversified	15.44	10
Utilities - Regulated Electric	21.06	32
Utilities - Regulated Gas	20.15	16
Utilities - Regulated Water	21.83	13""")


#### SOURCE: https://fullratio.com/roe-by-industry ####
#### 2025ë…„ 9ì›” ê¸°ì¤€ ì—…ë°ì´íŠ¸ ì™„ë£Œ ####

df_roe = parse_industry_roe_from_text("""Industry	Average ROE	Number of companies
Advertising Agencies	-0.8	27
Aerospace & Defense	8.4	57
Agricultural Inputs	1.9	11
Airlines	5.3	13
Aluminum	11.8	4
Apparel Manufacturing	4	15
Apparel Retail	12.9	29
Asset Management	9.3	83
Auto Parts	2.5	44
Auto & Truck Dealerships	10.3	20
Banks - Diversified	11.5	5
Banks - Regional	8.3	288
Beverages - Non-Alcoholic	21.3	12
Beverages - Wineries & Distilleries	6.6	6
Biotechnology	-66.3	476
Broadcasting	9.6	10
Building Materials	18.7	10
Building Products & Equipment	12.9	27
Business Equipment & Supplies	9.9	5
Capital Markets	13.2	48
Chemicals	-5.4	15
Coking Coal	1.2	5
Communication Equipment	-1.3	43
Computer Hardware	-15.3	28
Conglomerates	5.9	15
Consulting Services	11	14
Consumer Electronics	-8.9	8
Copper	10.1	4
Credit Services	10.1	40
Diagnostics & Research	-21.6	46
Discount Stores	21.8	8
Drug Manufacturers - General	48.7	14
Drug Manufacturers - Specialty & Generic	-14.5	48
Education & Training Services	14.6	19
Electrical Equipment & Parts	5.9	39
Electronic Components	-1.1	35
Electronic Gaming & Multimedia	8.3	12
Electronics & Computer Distribution	1	9
Engineering & Construction	15.6	36
Entertainment	0.9	35
Farm & Heavy Construction Machinery	7.6	19
Farm Products	10.6	15
Financial Data & Stock Exchanges	16.6	11
Food Distribution	2.9	11
Footwear & Accessories	15.3	9
Furnishings, Fixtures & Appliances	5.7	25
Gambling	15.8	9
Gold	7.3	31
Grocery Stores	17.2	9
Healthcare Plans	2.4	11
Health Information Services	-9.5	40
Home Improvement Retail	7.7	8
Household & Personal Products	6.9	24
Industrial Distribution	14.6	17
Information Technology Services	8.6	49
Insurance Brokers	6.6	13
Insurance - Diversified	13.1	9
Insurance - Life	8.1	15
Insurance - Property & Casualty	13	36
Insurance - Reinsurance	9.2	8
Insurance - Specialty	10.9	20
Integrated Freight & Logistics	10.2	17
Internet Content & Information	0.9	45
Internet Retail	13.2	26
Leisure	1.2	23
Luxury Goods	-0.8	8
Marine Shipping	7.5	24
Medical Care Facilities	-13.3	38
Medical Devices	-46.7	110
Medical Instruments & Supplies	-24	42
Metal Fabrication	7.7	15
Mortgage Finance	5.6	15
Oil & Gas Drilling	-0.2	8
Oil & Gas E&P	9.8	60
Oil & Gas Equipment & Services	9.6	42
Oil & Gas Integrated	8.4	6
Oil & Gas Midstream	14.5	36
Oil & Gas Refining & Marketing	1.1	17
Other Industrial Metals & Mining	-9.9	17
Other Precious Metals & Mining	-2.4	10
Packaged Foods	7.3	46
Packaging & Containers	11.6	20
Paper & Paper Products	3.9	4
Personal Services	15.4	10
Pollution & Treatment Controls	13.2	12
Publishing	2.5	7
Railroads	21	8
Real Estate - Development	3.3	8
Real Estate Services	1.6	30
Recreational Vehicles	0.1	11
REIT - Diversified	1.3	16
REIT - Healthcare Facilities	2.6	16
REIT - Hotel & Motel	2.4	14
REIT - Industrial	5.8	17
REIT - Mortgage	1.6	40
REIT - Office	-1.4	22
REIT - Residential	4.1	20
REIT - Retail	6.7	26
REIT - Specialty	6.9	19
Rental & Leasing Services	13.2	18
Residential Construction	17.6	22
Resorts & Casinos	15	16
Restaurants	9.5	43
Scientific & Technical Instruments	10	24
Security & Protection Services	12.8	15
Semiconductor Equipment & Materials	4.6	27
Semiconductors	0.9	60
Software - Application	-1.4	169
Software - Infrastructure	1.9	119
Solar	-6	19
Specialty Business Services	8.1	31
Specialty Chemicals	5.5	50
Specialty Industrial Machinery	9.8	68
Specialty Retail	9.4	36
Staffing & Employment Services	12.3	21
Steel	1.6	11
Telecom Services	4.7	33
Thermal Coal	10.2	6
Tools & Accessories	11.6	9
Travel Services	29.4	12
Trucking	4.7	13
Utilities - Diversified	7.3	10
Utilities - Regulated Electric	10	32
Utilities - Regulated Gas	9.3	16
Utilities - Regulated Water	9.2	13
Utilities - Renewable	7.8	15
Waste Management	10.5	13""")
                                      

#### SOURCE: https://fullratio.com/roa-by-industry ####
#### 2025ë…„ 9ì›” ê¸°ì¤€ ì—…ë°ì´íŠ¸ ì™„ë£Œ ####

df_roa = parse_industry_roa_from_text("""Industry	Average ROA	Number of companies
Advertising Agencies	-0.5	27
Aerospace & Defense	3.9	57
Agricultural Inputs	0	11
Airlines	0.8	13
Aluminum	4	4
Apparel Manufacturing	3	15
Apparel Retail	3.3	29
Asset Management	2.6	83
Auto Manufacturers	2.5	17
Auto Parts	2.9	44
Auto & Truck Dealerships	1.4	20
Banks - Diversified	1.1	5
Banks - Regional	0.9	288
Beverages - Non-Alcoholic	9.2	12
Beverages - Wineries & Distilleries	3.3	6
Biotechnology	-46.6	476
Broadcasting	-0.6	10
Building Materials	10.7	10
Building Products & Equipment	6.3	27
Business Equipment & Supplies	5	5
Capital Markets	1.8	48
Chemicals	-4.8	15
Coking Coal	-3.9	5
Communication Equipment	-0.6	43
Computer Hardware	-4.6	28
Conglomerates	1.7	15
Consulting Services	3.5	14
Consumer Electronics	-2.1	8
Copper	2.7	4
Credit Services	2.1	40
Diagnostics & Research	-14.5	46
Discount Stores	4.6	8
Drug Manufacturers - General	-3.4	14
Drug Manufacturers - Specialty & Generic	-9	48
Education & Training Services	6	19
Electrical Equipment & Parts	2	39
Electronic Components	-0.6	35
Electronic Gaming & Multimedia	-2.4	12
Electronics & Computer Distribution	3.3	9
Engineering & Construction	5.3	36
Entertainment	-0.4	35
Farm & Heavy Construction Machinery	2.9	19
Farm Products	2	15
Financial Data & Stock Exchanges	3.2	11
Food Distribution	1.7	11
Footwear & Accessories	5	9
Furnishings, Fixtures & Appliances	2.1	25
Gambling	4.1	9
Gold	4.4	31
Grocery Stores	2.9	9
Healthcare Plans	-0.5	11
Health Information Services	-9.2	40
Home Improvement Retail	3.8	8
Household & Personal Products	5.3	24
Industrial Distribution	6.9	17
Information Technology Services	3.3	49
Insurance Brokers	1.8	13
Insurance - Diversified	2.7	9
Insurance - Life	1.1	15
Insurance - Property & Casualty	3	36
Insurance - Reinsurance	2	8
Insurance - Specialty	1.5	20
Integrated Freight & Logistics	2.8	17
Internet Content & Information	-0.3	45
Internet Retail	4	26
Leisure	0	23
Lodging	4.7	8
Luxury Goods	-2.4	8
Marine Shipping	5	24
Medical Care Facilities	-1.8	38
Medical Devices	-25.5	110
Medical Distribution	-6.3	6
Medical Instruments & Supplies	-14.1	42
Metal Fabrication	3.9	15
Mortgage Finance	0.6	15
Oil & Gas Drilling	-0.5	8
Oil & Gas E&P	3.6	60
Oil & Gas Equipment & Services	3.4	42
Oil & Gas Integrated	4	6
Oil & Gas Midstream	4.7	36
Oil & Gas Refining & Marketing	0	17
Other Industrial Metals & Mining	-6.3	17
Other Precious Metals & Mining	-1.4	10
Packaged Foods	3.9	46
Packaging & Containers	3.3	20
Paper & Paper Products	2.7	4
Personal Services	8.8	10
Pollution & Treatment Controls	7.1	12
Publishing	-0.4	7
Railroads	5.5	8
Real Estate - Development	1.2	8
Real Estate Services	0.5	30
Recreational Vehicles	0.7	11
REIT - Diversified	1	16
REIT - Healthcare Facilities	1.4	16
REIT - Hotel & Motel	0.9	14
REIT - Industrial	3.4	17
REIT - Mortgage	0.4	40
REIT - Office	-0.4	22
REIT - Residential	1.7	20
REIT - Retail	2.5	26
REIT - Specialty	2.7	19
Rental & Leasing Services	2.3	18
Residential Construction	9.2	22
Resorts & Casinos	1.7	16
Restaurants	2.7	43
Scientific & Technical Instruments	3.3	24
Security & Protection Services	5.7	15
Semiconductor Equipment & Materials	3	27
Semiconductors	-1.1	60
Software - Application	-1.5	169
Software - Infrastructure	-0.6	119
Solar	-3.6	19
Specialty Business Services	3.5	31
Specialty Chemicals	1.6	50
Specialty Industrial Machinery	6.3	68
Specialty Retail	4.2	36
Staffing & Employment Services	3.5	21
Steel	1.7	11
Telecom Services	-0.1	33
Thermal Coal	6.9	6
Tobacco	12.5	8
Tools & Accessories	5.4	9
Travel Services	5.1	12
Trucking	2.8	13
Utilities - Diversified	2.1	10
Utilities - Regulated Electric	2.8	32
Utilities - Regulated Gas	2.6	16
Utilities - Regulated Water	2.9	13
Utilities - Renewable	1	15
Waste Management	-0.5	13""")

if debug_download_fullratio:
    print(df_per) 

    print(df_roe) 

    print(df_roa) 


def get_industry_per(ind):
    try:
        if ind is not None:
            ans = float(
                df_per.filter(pl.col("Industry") == ind).select("P/E Ratio").item()
            )
            return ans
        return per
    except Exception:
        spy = yf.Ticker("SPY")
        spy_info = spy.info
        per = spy_info.get("trailingPE")
        return per


def get_industry_roe(ind):
    try:
        if ind is not None:
            ans = float(df_roe.filter(pl.col("Industry") == ind).select("ROE").item())
            return ans / 100.0
        else:
            return 0.08
    except Exception:
        return 0.08


def get_industry_roa(ind):
    try:
        if ind is not None:
            ans = float(df_roa.filter(pl.col("Industry") == ind).select("ROA").item())
            return ans / 100.0
        return 0.06
    except Exception:
        return 0.06


######## LOAD TICKERS ###########
raw_tickers = get_tickers(country, limit, sp500)

prohibited = {
    "NVL",
    "SOJE",
    "RY-PT",
    "CEO",
    "DUKB",
    "ATVI",
    "SQ",
    "AED",
    "BACRP",
    "BDXA",
    "VZA",
    "AMOV",
    "ANTM",
    "SOJC",
    "ACH",
    "TBC",
    "PXD",
    "TBB",
    "SOJD",
    "AFA",
    "AEH",
}


# no data on yfinance or frequently cause errors
def keep_ticker(t):
    return t not in prohibited


tickers = list(filter(keep_ticker, raw_tickers))
################################################################################
from yf_cache_downloader import get_tickers_by_country_cache, update_cache


# ì˜ˆ) í‹°ì»¤ ë¦¬ìŠ¤íŠ¸ ë°›ì•„ì˜¤ê¸° (limit, api_keyëŠ” yf_cache_downloader.py ë‚´ë¶€ ë˜ëŠ” ì™¸ë¶€ì—ì„œ ì„¤ì • ê°€ëŠ¥)
tickers_for_cache = get_tickers_by_country_cache("US", limit=300, apikey=fmp_key)

# í•„ìš” ì—†ëŠ” í‹°ì»¤ ì œì™¸í•˜ê¸° (ì˜µì…˜)
tickers_to_remove = {
    "NVL",
    "SOJE",
    "RY-PT",
    "CEO",
    "DUKB",
    "ATVI",
    "SQ",
    "AED",
    "BACRP",
    "BDXA",
    "VZA",
    "AMOV",
    "ANTM",
    "SOJC",
    "ACH",
    "TBC",
    "PXD",
    "TBB",
    "SOJD",
    "AFA",
    "AEH",
}

tickers_for_cache = [t for t in tickers_for_cache if t not in tickers_to_remove]

# ìºì‹œ ì—…ë°ì´íŠ¸ (ëˆ„ë½ëœ ë°ì´í„°ë§Œ ë°›ì•„ì„œ yf_cache_multi.csv íŒŒì¼ ê°±ì‹ )
cache = update_cache(tickers_for_cache)

# ì´ì œ cacheì—ëŠ” ìµœì‹ ìœ¼ë¡œ ì±„ì›Œì§„ ë°ì´í„°ê°€ ë“¤ì–´ìˆìŒ
print(cache.head())
print("ìºì‹œ ë°ì´í„° ë²”ìœ„:", cache.index.min(), "~", cache.index.max())

if isinstance(cache.columns, pd.MultiIndex):
    successful_tickers = set([col[0] for col in cache.columns if col[1] == "Close"])
else:
    successful_tickers = set(cache.columns)

# âœ… ìµœì¢… í‹°ì»¤ ë¦¬ìŠ¤íŠ¸
tickers = [t for t in tickers if t in successful_tickers]

################################################################################

# Assume cache is already loaded and up-to-date (from yf_cache_downloader)

# Define the date range for last 1 year up to today
end_date = pd.Timestamp.today().normalize()
start_date = end_date - pd.Timedelta(days=365)

# Clip start_date and end_date to cache's available index range to avoid KeyErrors
start_date = max(start_date, cache.index.min())
end_date = min(end_date, cache.index.max())

# Slice cache by this date range
cache_slice = cache.loc[start_date:end_date]

# Extract Close prices only
if isinstance(cache_slice.columns, pd.MultiIndex):
    # Select columns where second level is 'Close'
    close_cols = [col for col in cache_slice.columns if col[1] == "Close"]
    df_close = cache_slice[close_cols].copy()
    # Rename columns to just ticker symbols
    df_close.columns = [col[0] for col in df_close.columns]
else:
    # Single-level columns (unlikely if your cache is multi-indexed)
    if "Close" in cache_slice.columns:
        df_close = cache_slice[["Close"]].copy()
        df_close.columns = [tickers[0]]  # Assuming one ticker
    else:
        raise ValueError("âŒ 'Close' column not found in cache.")

# Drop any columns (tickers) with all NaN Close values
df_close.dropna(axis=1, how="all", inplace=True)

print("Date range in df_close:", df_close.index.min(), "to", df_close.index.max())
print("Number of tickers with Close data:", len(df_close.columns))
print(df_close.head())
###################################################################################
# ì˜ˆ: cacheì—ì„œ Close ê°€ê²©ë§Œ ì¶”ì¶œí•œ í›„
df_momentum = df_close.copy()


def check_momentum_conditions(ticker: str) -> dict:
    result = {
        "ma_crossover": False,
        "ma_crossover_lt": False,
        "return_20d": False,
        "return_60d": False,
        "rsi_rebound": False,
        "macd_golden_cross": False,
    }

    try:
        # í‹°ì»¤ê°€ ë°ì´í„°ì— ì—†ìœ¼ë©´ ë°”ë¡œ ë°˜í™˜
        if ticker not in df_momentum.columns:
            print(f"[Error] {ticker} not in df_momentum.columns")
            return result

        # ê°œë³„ ì¢…ëª© ì‹œê³„ì—´ ì¶”ì¶œ í›„ 'Close'ë¡œ ì»¬ëŸ¼ëª… í†µì¼
        df_ticker = df_momentum[[ticker]].copy()
        df_ticker.columns = ["Close"]

        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        df_ticker["Close"] = df_ticker["Close"].ffill()

        if df_ticker["Close"].isna().all():
            print(f"[Error] All 'Close' values are NaN for {ticker}")
            return result

        if len(df_ticker) < 22:
            print(
                f"[Warning] Not enough data rows for 20-day return calculation for {ticker} (rows={len(df_ticker)})"
            )
            return result

        # ì´ë™í‰ê· ì„  ê³„ì‚°
        df_ticker["MA20"] = df_ticker["Close"].rolling(window=5).mean()
        df_ticker["MA60"] = df_ticker["Close"].rolling(window=20).mean()

        if pd.notna(df_ticker["MA20"].iloc[-1]) and pd.notna(
            df_ticker["MA60"].iloc[-1]
        ):
            if df_ticker["MA20"].iloc[-1] > df_ticker["MA60"].iloc[-1]:
                result["ma_crossover"] = True

        df_ticker["MA50"] = df_ticker["Close"].rolling(window=50).mean()
        df_ticker["MA200"] = df_ticker["Close"].rolling(window=200).mean()

        if pd.notna(df_ticker["MA50"].iloc[-1]) and pd.notna(
            df_ticker["MA200"].iloc[-1]
        ):
            if df_ticker["MA50"].iloc[-1] > df_ticker["MA200"].iloc[-1]:
                result["ma_crossover_lt"] = True

        # 20ì¼ ìˆ˜ìµë¥  ê³„ì‚°
        try:
            return_20d = (
                df_ticker["Close"].iloc[-1] / df_ticker["Close"].iloc[-21] - 1
            ) * 100
            if return_20d >= 10:
                result["return_20d"] = True
        except IndexError:
            print(
                f"[Warning] Not enough data for 20-day return calculation for {ticker}"
            )

        # 60ì¼ ìˆ˜ìµë¥  ê³„ì‚°
        try:
            return_60d = (
                df_ticker["Close"].iloc[-1] / df_ticker["Close"].iloc[-61] - 1
            ) * 100
            if return_60d >= 10:
                result["return_60d"] = True
        except IndexError:
            print(
                f"[Warning] Not enough data for 60-day return calculation for {ticker}"
            )

        # RSI
        try:
            rsi = ta.momentum.RSIIndicator(df_ticker["Close"], window=14).rsi()
            if len(rsi) >= 7:
                recent = rsi.iloc[-7:]
                if (
                    all(recent > 50)
                    and recent.iloc[-1] < 70
                    and recent.is_monotonic_increasing
                ):
                    result["rsi_rebound"] = True
        except Exception as e:
            print(f"[RSI Error] {ticker}: {e}")

        # MACD
        try:
            macd_obj = ta.trend.MACD(df_ticker["Close"])
            macd_line = macd_obj.macd()
            signal_line = macd_obj.macd_signal()

            if len(macd_line) >= 7:
                macd_recent = macd_line.iloc[-7:]
                signal_recent = signal_line.iloc[-7:]
                if (macd_recent > signal_recent).sum() >= 5 and macd_recent.iloc[
                    -1
                ] > signal_recent.iloc[-1]:
                    if macd_recent.iloc[-1] > macd_recent.iloc[0]:
                        result["macd_golden_cross"] = True
        except Exception as e:
            print(f"[MACD Error] {ticker}: {e}")

    except Exception as e:
        print(f"[Download Error] Ticker {ticker}: {e}")

    return result


def check_momentum_conditions_batch(tickers: list) -> pd.DataFrame:
    results = []
    for ticker in tickers:
        # print(f"Processing {ticker} ...")
        res = check_momentum_conditions(ticker)
        res["Ticker"] = ticker
        results.append(res)
    # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜ (Ticker ì»¬ëŸ¼ ì²« ì¹¼ëŸ¼ìœ¼ë¡œ ì´ë™)
    df_results = pd.DataFrame(results)
    cols = ["Ticker"] + [c for c in df_results.columns if c != "Ticker"]
    df_results = df_results[cols]
    return df_results


df_batch_result = check_momentum_conditions_batch(tickers)


def score_momentum(ma, ma_lt, ret_20d, ret_60d, rsi, macd):
    score = 0

    # ì´ë™í‰ê·  í¬ë¡œìŠ¤ì˜¤ë²„ (ë‹¨ê¸°, ì¥ê¸°)
    if ma:  # ë‹¨ê¸° MA í¬ë¡œìŠ¤ ì˜¤ë²„ ì‹ í˜¸ (True/False)
        score += 10
    if ma_lt:  # ì¥ê¸° MA í¬ë¡œìŠ¤ ì˜¤ë²„ ì‹ í˜¸ (True/False)
        score += 15

    # RSI ê³¼ë§¤ë„ ë°˜ë“± (True/False)
    if rsi:
        score += 20

    # MACD ê³¨ë“ í¬ë¡œìŠ¤ (True/False)
    if macd:
        score += 20

    # ë‹¨ê¸° ìˆ˜ìµë¥  ë°˜ì˜ (ì˜ˆ: 20ì¼ ìˆ˜ìµë¥ )
    if ret_20d is not None:
        if ret_20d > 0:
            score += min(ret_20d * 100, 10)  # 0~10ì , 1% ìƒìŠ¹ì‹œ 1ì 

    # ì¤‘ê¸° ìˆ˜ìµë¥  ë°˜ì˜ (ì˜ˆ: 60ì¼ ìˆ˜ìµë¥ )
    if ret_60d is not None:
        if ret_60d > 0:
            score += min(ret_60d * 100, 15)  # 0~15ì 
        # else:
        #     curr_score = score # í˜„ì¬ ì ìˆ˜
        #     curr_score = curr_score * 0.5
        #     score = max(score + ret_60d * 100, curr_score)

    return round(score, 2)


def get_operating_income_yoy(ticker_obj):
    try:
        financials = ticker_obj.financials

        if "Operating Income" not in financials.index:
            return None

        operating_income = financials.loc["Operating Income"].dropna()
        operating_income = operating_income.sort_index()  # ì˜¤ë˜ëœ ìˆœ ì •ë ¬

        if len(operating_income) < 2:
            return None

        # ìµœê·¼ 2ë…„ì¹˜ ì˜ì—…ì´ìµ
        latest = operating_income.iloc[-1]
        prev = operating_income.iloc[-2]

        if prev == 0:
            return None  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€

        yoy_growth = (latest - prev) / abs(prev)
        return yoy_growth

    except Exception:
        return None


def get_operating_income_qoq(ticker_obj):
    try:
        financials = ticker_obj.quarterly_financials

        if "Operating Income" not in financials.index:
            return None

        operating_income = financials.loc["Operating Income"].dropna()
        operating_income = operating_income.sort_index()  # ì˜¤ë˜ëœ ìˆœ ì •ë ¬

        if len(operating_income) < 2:
            return None

        latest = operating_income.iloc[-1]
        prev = operating_income.iloc[-2]

        if prev == 0:
            return None

        qoq_growth = (latest - prev) / abs(prev)
        return qoq_growth

    except Exception:
        return None


def score_intrinsic_value(
    conf_lower, conf_upper, current_price, fcf_yield, tenyr_treasury_yield, fcf_cagr
):
    score = 0

    if conf_lower is not None and conf_upper is not None and current_price is not None:
        if current_price < conf_upper:
            score += 2  # price is within fair value range
            if current_price <= conf_lower:
                score += 3  # price is at or below lower bound of fair value range
        else:
            score -= 3  # price outside fair value range

    if fcf_yield is not None:
        if fcf_yield > tenyr_treasury_yield:
            score += 2
        elif fcf_yield < tenyr_treasury_yield:
            score -= 1

    if fcf_cagr is not None:
        if fcf_cagr >= 0:
            score += 2
        else:
            score -= 1

    return score


def monte_carlo_dcf_valuation(
    info,
    initial_fcf,
    wacc,
    terminal_growth_rate,
    projection_years=5,
    num_simulations=10_000,
):
    if initial_fcf <= 0:
        return (None, None)
    if wacc <= terminal_growth_rate:
        return (None, None)

    if projection_years <= 0 or num_simulations <= 0:
        return (None, None)

    shares_outstanding = info.get("sharesOutstanding")
    if not shares_outstanding or shares_outstanding <= 0:
        return (None, None)

    total_debt = info.get("totalDebt") or 0
    cash = info.get("totalCash") or 0
    net_debt = total_debt - cash

    growth_mean = 0.08
    growth_std = 0.03

    equity_values = []

    for _ in range(num_simulations):
        fcf = initial_fcf
        total_value = 0

        for year in range(1, projection_years + 1):
            growth_rate = np.random.normal(growth_mean, growth_std)
            fcf *= 1 + growth_rate
            discounted_fcf = fcf / ((1 + wacc) ** year)
            total_value += discounted_fcf

        terminal_value = (
            fcf * (1 + terminal_growth_rate) / (wacc - terminal_growth_rate)
        )
        discounted_terminal_value = terminal_value / ((1 + wacc) ** projection_years)

        enterprise_value = total_value + discounted_terminal_value
        equity_value = enterprise_value - net_debt

        equity_values.append(equity_value)

    equity_values = np.array(equity_values)
    fair_value_per_share = equity_values / shares_outstanding

    # mean_val = np.mean(fair_value_per_share)
    # median_val = np.median(fair_value_per_share)
    # std_val = np.std(fair_value_per_share)
    conf_lower = np.percentile(fair_value_per_share, 2.5)
    conf_upper = np.percentile(fair_value_per_share, 97.5)

    return (float(conf_lower), float(conf_upper))


def analyze_moat(company_name: str, date_kr_ymd: str) -> str:
    prompt = f"""
ë‹¹ì‹ ì€ ê²½ì œì  í•´ì(Moat) ë¶„ì„ê³¼ ê°€ì¹˜ í•¨ì •(Value Trap) íƒì§€ì— íŠ¹í™”ëœ ì „ë¬¸ íˆ¬ì ë¶„ì„ê°€ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì‹­ì‹œì˜¤.

{date_kr_ymd} ê¸°ì¤€ "{company_name}"ì˜ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬, ì•„ë˜ ë„¤ ê°€ì§€ ê¸°ì¤€ì„ ë°”íƒ•ìœ¼ë¡œ í•´ë‹¹ ê¸°ì—…ì˜ **ì¤‘ì¥ê¸° í•µì‹¬ ê²½ìŸ ìš°ìœ„(Moat)** ë° **Value Trap ë¦¬ìŠ¤í¬**ë¥¼ ëª¨ë‘ ì •ì„±ì Â·ì •ëŸ‰ì ìœ¼ë¡œ ë¶„ì„í•´ ì£¼ì„¸ìš”.

---

ğŸ§± [ê¸°ì¤€ 1] {date_kr_ymd} ê¸°ì¤€ ê²½ì œì  í•´ì ë¶„ì„  
- ë¸Œëœë“œ, ë„¤íŠ¸ì›Œí¬ íš¨ê³¼, íŠ¹í—ˆ/IP, ì „í™˜ ë¹„ìš© ë“± í•´ìì˜ ìœ í˜•ê³¼ ê°•ë„  
- ì‚°ì—… ë‚´ ì§€ë°°ë ¥ ë˜ëŠ” êµ¬ì¡°ì  ì§„ì… ì¥ë²½ ì¡´ì¬ ì—¬ë¶€  
- ëª¨ë°© ë˜ëŠ” íŒŒê´´ì  í˜ì‹ ì˜ ìœ„í˜‘ ê°€ëŠ¥ì„±  

ğŸ“‰ [ê¸°ì¤€ 2] {date_kr_ymd} ê¸°ì¤€ ì‹¤ì  ë¯¼ê°ë„ ë° Value Trap ë¦¬ìŠ¤í¬  
- ìµœê·¼ ì‹¤ì  ë°œí‘œì—ì„œ ë§¤ì¶œ, ì´ìµ, ì„±ì¥ë¥  ì¶”ì„¸ì˜ ì•ˆì •ì„±  
- ROICê°€ WACCë¥¼ ì´ˆê³¼í•˜ë©° ìœ ì§€ë˜ëŠ”ì§€ ì—¬ë¶€  
- ì‹œì¥ì ìœ ìœ¨, ë§ˆì§„, FCF ë“± ì£¼ìš” ì§€í‘œì˜ í•˜ë½ ì¡°ì§  
- ì¼íšŒì„± ìˆ˜ìµ ë˜ëŠ” ë¹„ì˜ì—… í•­ëª© ì˜ì¡´ ì—¬ë¶€  

âš”ï¸ [ê¸°ì¤€ 3] {date_kr_ymd} ê¸°ì¤€ ê²½ìŸì‚¬ ëŒ€ë¹„ í•´ì ë°©ì–´ë ¥  
- ê²½ìŸì‚¬ ëŒ€ë¹„ ê¸°ìˆ ë ¥, ì œí’ˆë ¥, ê°€ê²© ê²½ìŸë ¥ ìš°ìœ„ ì—¬ë¶€  
- ì‹ ì œí’ˆ ì¶œì‹œ ì†ë„, ê·œì œ ëŒ€ì‘ë ¥, ìœ í†µë ¥, ê¸€ë¡œë²Œ ì§„ì¶œë ¥ ë¹„êµ  
- ì‚°ì—… ë‚´ ì‹œì¥ì ìœ ìœ¨ ë³€í™” ì¶”ì„¸  

â›³ [ê¸°ì¤€ 4] {date_kr_ymd} ê¸°ì¤€ ê²½ì˜ì§„ì˜ ì „ëµ ëŒ€ì‘ë ¥ ë° ìë³¸ ë°°ë¶„  
- ìì‚¬ì£¼ ë§¤ì…, ë°°ë‹¹, ì¸ìˆ˜í•©ë³‘, R&D ë“± ìë³¸ ë°°ë¶„ì˜ ì£¼ì£¼ ì¹œí™”ì„±  
- êµ¬ì¡°ì  ìœ„ê¸° ëŒ€ì‘ ì „ëµ ë³´ìœ  ì—¬ë¶€  
- CEO, CFO ë“± ê²½ì˜ì§„ ë¦¬ë”ì‹­ì˜ ì‹¤í–‰ë ¥  

---

âš ï¸ [ê°ì  ìš”ì¸: {date_kr_ymd} ê¸°ì¤€ Value Trap ì‹œê·¸ë„ í•˜ë‚˜ë¼ë„ ì¡´ì¬ ì‹œ ê°•í•œ ê°ì ]  
- ë³¸ì§ˆì  í€ë”ë©˜í„¸ ë¶•ê´´ ì§•í›„  
- ê²½ìŸì‚¬ì˜ ê¸°ìˆ  í˜ì‹ ì— ë°€ë ¤ ì‹œì¥ ì ìœ ìœ¨ í•˜ë½
- ì„±ì¥ ì‚°ì—… ë‚´ ìˆ˜ìµì„±Â·í˜„ê¸ˆíë¦„Â·ì ìœ ìœ¨ ë™ë°˜ í•˜ë½  

---

ğŸ“¤ **ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì˜ JSONìœ¼ë¡œë§Œ ê°„ê²°í•˜ê²Œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤. ì¶”ê°€ ì„¤ëª…ì´ë‚˜ ìƒì„¸ ë¶„ì„ì€ í¬í•¨í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.**

```json
{{
  "moat_analysis": "ê¸°ì—…ì˜ ì¤‘ì¥ê¸° í•µì‹¬ ê²½ìŸ ìš°ìœ„ ë° Value Trap ìœ„í—˜ì„± ìš”ì•½ (ë°˜ë“œì‹œ 2~3ì¤„ ì´ë‚´ ìš”ì•½)",
  "moat_score": 0,  // 0ì—ì„œ 10 ì‚¬ì´ ì •ìˆ˜ (ì•„ë˜ ê¸°ì¤€ ì°¸ê³ )
}}

Moat Score ê¸°ì¤€ (0~10):

0: ì™„ì „í•œ Commodity, ê°€ê²© ê²½ìŸ ì™¸ ê²½ìŸ ìš°ìœ„ ì—†ìŒ  
1-3: ê²½ìŸ ìš°ìœ„ ë¯¸ë¯¸~ë‚®ìŒ, ì°¨ë³„í™” ë¶€ì¡±, ì‹œì¥ ë‚´ ë°©ì–´ë ¥ ì•½í•¨  
4-5: ë¶€ë¶„ì  ê²½ìŸë ¥ ë³´ìœ , ì¼ì‹œì  ìš°ìœ„ í˜¹ì€ ìœ ì§€ ë¶ˆí™•ì‹¤  
6-7: ìƒë‹¹í•œ ê²½ìŸ ìš°ìœ„, êµ¬ì¡°ì  ìš°ìœ„ ìˆìœ¼ë‚˜ ëŒ€ì²´ ê°€ëŠ¥ì„± ì¡´ì¬  
8-9: ëšœë ·í•˜ê³  ì¥ê¸°ì  ê²½ìŸ ìš°ìœ„, ê°•ë ¥í•œ ì§„ì… ì¥ë²½ê³¼ ë„¤íŠ¸ì›Œí¬ íš¨ê³¼ ì¡´ì¬  
10: ì ˆëŒ€ì  ë…ì  ìš°ìœ„, ëŒ€ì²´ ë¶ˆê°€ëŠ¥í•˜ë©° ì§„ì… ë¶ˆê°€ ìˆ˜ì¤€  

â€» ê²½ìŸ ìš°ìœ„ê°€ ì•½í•˜ê±°ë‚˜ Value Trap ìœ„í—˜ì´ í•˜ë‚˜ë¼ë„ ê°ì§€ë˜ë©´ ì ìˆ˜ë¥¼ ê°•í•˜ê²Œ ê°ì í•˜ê³ , ë³´ìˆ˜ì ìœ¼ë¡œ ì‚°ì •í•˜ì‹­ì‹œì˜¤.  
"""
    return prompt.strip()


def parse_moat_response(response_text: str) -> dict:
    """
    LLM ì‘ë‹µì—ì„œ moat_analysisì™€ moat_scoreë¥¼ ì•ˆì „í•˜ê²Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    JSONì´ í˜¼í•©ë˜ì–´ ìˆê±°ë‚˜ í˜•ì‹ì´ ë¶ˆì™„ì „í•  ê²½ìš°ì—ë„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    # ê¸°ë³¸ê°’
    result = {"moat_analysis": response_text.strip(), "moat_score": None}

    # JSON í˜•ì‹ ì¶”ì¶œ ì‹œë„
    try:
        # ì¤‘ê´„í˜¸ë¡œ ëœ JSON ë¸”ëŸ­ ì¶”ì¶œ
        match = re.search(r"\{.*?\}", response_text, re.DOTALL)
        if match:
            json_block = match.group(0)
            parsed = json.loads(json_block)
            result["moat_analysis"] = parsed.get(
                "moat_analysis", result["moat_analysis"]
            ).strip()
            result["moat_score"] = (
                int(parsed.get("moat_score"))
                if parsed.get("moat_score") is not None
                else None
            )
            return result
    except (json.JSONDecodeError, ValueError, TypeError):
        pass  # continue to fallback logic

    # fallback ì ìˆ˜ ì¶”ì • ë¡œì§ (í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ë¡ )
    lower_text = response_text.lower()
    if any(
        kw in lower_text
        for kw in ["ì ˆëŒ€ì  ë…ì ", "ì™„ì „í•œ ë…ì ", "ëŒ€ì²´ ë¶ˆê°€", "ì§„ì… ë¶ˆê°€", "íŠ¹í—ˆ ë³´í˜¸"]
    ):
        result["moat_score"] = 10
    elif any(
        kw in lower_text
        for kw in ["ì§€ì†ì  ë…ì ", "ì§€ì†ì ì¸ ë…ì ", "ê°•ë ¥í•œ ì§„ì… ì¥ë²½", "ê·œì œ ë³´í˜¸"]
    ):
        result["moat_score"] = 9
    elif any(
        kw in lower_text
        for kw in ["ëšœë ·í•œ ê²½ìŸ ìš°ìœ„", "ë¸Œëœë“œ íŒŒì›Œ", "ê·œëª¨ì˜ ê²½ì œ", "ì „í™˜ ë¹„ìš©"]
    ):
        result["moat_score"] = 8
    elif any(
        kw in lower_text
        for kw in ["ê°•í•œ ê²½ìŸë ¥", "ê¸°ìˆ ë ¥", "ìœ í†µë§", "ê²½ìŸì‚¬ ëŒ€ë¹„ ìš°ìœ„"]
    ):
        result["moat_score"] = 7
    elif any(
        kw in lower_text for kw in ["ìƒë‹¹í•œ ê²½ìŸ ìš°ìœ„", "ìš°ìœ„ ìš”ì†Œ ì¡´ì¬", "ëŒ€ì²´ ê°€ëŠ¥ì„±"]
    ):
        result["moat_score"] = 6
    elif any(
        kw in lower_text for kw in ["í‰ê·  ì´ìƒì˜ ê²½ìŸë ¥", "ì°¨ë³„í™” ë¯¸ì•½", "ìœ ì§€ ë¶ˆí™•ì‹¤"]
    ):
        result["moat_score"] = 5
    elif any(
        kw in lower_text for kw in ["ë¶€ë¶„ì  ê²½ìŸë ¥", "ì¼ì‹œì  ìˆ˜ìµì„±", "ëŒ€ì²´ì¬ ì¡´ì¬"]
    ):
        result["moat_score"] = 4
    elif any(
        kw in lower_text for kw in ["ê²½ìŸ ìš°ìœ„ ë‚®ìŒ", "ì°¨ë³„í™” ê±°ì˜ ì—†ìŒ", "ë°©ì–´ë ¥ ë‚®ìŒ"]
    ):
        result["moat_score"] = 3
    elif any(
        kw in lower_text for kw in ["ë¯¸ë¯¸í•œ ê²½ìŸ ìš°ìœ„", "ë‹¨ê¸° ìœ í–‰", "êµ¬ì¡°ì  ìš°ìœ„ ì—†ìŒ"]
    ):
        result["moat_score"] = 2
    elif any(
        kw in lower_text
        for kw in [
            "ê²½ìŸ ìš°ìœ„ ì—†ìŒ",
            "ì§„ì… ì¥ë²½ ì—†ìŒ",
            "ë¸Œëœë“œ ì—†ìŒ",
            "ê¸°ìˆ ë ¥ ì—†ìŒ",
            "commoditized",
        ]
    ):
        result["moat_score"] = 1
    elif any(
        kw in lower_text for kw in ["commodity", "ì™„ì „í•œ commodity", "ì™„ì „ ê²½ìŸ ì‹œì¥"]
    ):
        result["moat_score"] = 0
    else:
        result["moat_score"] = -1  # íŒë‹¨ ë¶ˆê°€ (ì˜ˆì™¸ ì²˜ë¦¬ìš©)

    return result


def query_gemini(prompt: str) -> str:
    response = client.models.generate_content(
        model="gemini-2.5-flash",
        contents=prompt,
    )
    return response.text


retried_once = set()
q = Queue()
for ticker in tickers:
    q.put(ticker)


def process_ticker_quantitatives():
    while not q.empty():
        ticker = q.get()
        try:
            yf_ticker = yf.Ticker(ticker)
            info = yf_ticker.info
            beta = info.get("beta", None)
            name = info.get("shortName", ticker)
            industry = info.get("industry", None)
            currentPrice = info.get("currentPrice", None)
            percentage_change = get_percentage_change_ttm(ticker)

            # Valuation & Liquidity
            debtToEquity = info.get("debtToEquity", None)
            debtToEquity = debtToEquity / 100 if debtToEquity is not None else None
            currentRatio = info.get("currentRatio", None)
            pbr = info.get("priceToBook", None)
            per = info.get("trailingPE", None)

            # Industry
            industry_per = get_industry_per(industry)
            industry_per = round(industry_per) if industry_per is not None else None
            industry_roe = get_industry_roe(industry)
            industry_roa = get_industry_roa(industry)

            # Profitability
            roe = info.get("returnOnEquity", None)
            roa = info.get("returnOnAssets", None)
            icr = get_interest_coverage_ratio(yf_ticker)

            # Growth
            eps_cagr = compute_eps_growth_slope(yf_ticker)
            div_cagr = has_stable_dividend_growth_cagr(yf_ticker)
            opinc_yoy = get_operating_income_yoy(yf_ticker)
            opinc_qoq = get_operating_income_qoq(yf_ticker)

            # FCF & Valuation
            fcf_yield, fcf_cagr, fcf_list = get_fcf_yield_and_cagr(
                ticker, yf_ticker, api_key=fmp_key
            )
            tenyr_treasury_yield = get_10yr_treasury_yield()
            discount_rate = (
                (tenyr_treasury_yield + (beta * 5.0)) / 100.0
                if beta is not None
                else (tenyr_treasury_yield + 5.0) / 100.0
            )
            terminal_growth_rate = 0.02

            initial_fcf = (
                fcf_list[-1] if fcf_list and fcf_list[-1] is not None else None
            )

            intrinsic_value_range = (
                monte_carlo_dcf_valuation(
                    info,
                    initial_fcf,
                    discount_rate,
                    terminal_growth_rate,
                    projection_years=5,
                    num_simulations=10_000,
                )
                if initial_fcf is not None
                else (None, None)
            )

            result = {
                "ticker": ticker,
                "name": name,
                "price": currentPrice,
                "price_vs_fair_upper": (
                    ((intrinsic_value_range[1] - currentPrice) / currentPrice)
                    if intrinsic_value_range[1] and currentPrice
                    else None
                ),
                "price_vs_fair_lower": (
                    ((intrinsic_value_range[0] - currentPrice) / currentPrice)
                    if intrinsic_value_range[0] and currentPrice
                    else None
                ),
                "fcf_yield": fcf_yield,
                "per": per,
                "pbr": pbr,
                "de": debtToEquity,
                "cr": currentRatio,
                "roe": roe,
                "roa": roa,
                "icr": icr,
                "fcf_cagr": fcf_cagr,
                "eps_cagr": eps_cagr if isinstance(eps_cagr, float) else None,
                "div_cagr": div_cagr if isinstance(div_cagr, float) else None,
                "eps": eps_cagr if isinstance(eps_cagr, float) else None,
                "div_yield": (
                    info.get("dividendYield", div_cagr)
                    if div_cagr is not None
                    else None
                ),
                "opinc_yoy": opinc_yoy if isinstance(opinc_yoy, float) else None,
                "opinc_qoq": opinc_qoq if isinstance(opinc_qoq, float) else None,
                # Industry benchmarks
                "industry_per": industry_per,
                "industry_roe": industry_roe,
                "industry_roa": industry_roa,
                # NEW: fields needed for quant_style_score (will be filled later)
                "fcf_yield_rank": None,
                "per_rank": None,
                "pbr_rank": None,
                "fcf_cagr_rank": None,
                "eps_cagr_rank": None,
                "div_cagr_rank": None,
                "roe_z": None,
                "roa_z": None,
                # Misc.
                "industry": industry,
                "1M_Change": percentage_change,
            }

            with data_lock:
                data.append(result)

        except Exception as e:
            print(f"Error processing {ticker}: {e}")
            with data_lock:
                if ticker not in retried_once:
                    retried_once.add(ticker)
                    q.put(ticker)
            if "429" in str(e):
                print("Too many requests! Waiting 10 seconds...")
                time.sleep(10)

        finally:
            q.task_done()


threads = []

for _ in range(NUM_THREADS):
    t = threading.Thread(target=process_ticker_quantitatives)
    t.start()
    threads.append(t)

for t in threads:
    t.join()

q.join()

df = pd.DataFrame(data)

# Rank ê³„ì‚°
df["fcf_yield_rank"] = df["fcf_yield"].rank(pct=True)
df["per_rank"] = 1 - df["per"].rank(pct=True)
df["pbr_rank"] = 1 - df["pbr"].rank(pct=True)
df["fcf_cagr_rank"] = df["fcf_cagr"].rank(pct=True)
df["eps_cagr_rank"] = df["eps_cagr"].rank(pct=True)
df["div_cagr_rank"] = df["div_cagr"].rank(pct=True)
# ì—…ì¢…ë³„ í†µê³„ ê³„ì‚°
industry_stats = df.groupby("industry").agg(
    {"roe": ["mean", "std"], "roa": ["mean", "std"]}
)

industry_stats.columns = [
    "_".join(col).strip() for col in industry_stats.columns.values
]
industry_stats.index.name = "industry"

df = df.merge(industry_stats, left_on="industry", right_index=True, how="left")


def safe_z(x, mean, std):
    if pd.isna(x) or pd.isna(mean) or pd.isna(std) or std == 0:
        return 0
    return (x - mean) / std


df["roe_z"] = df.apply(
    lambda row: safe_z(row["roe"], row["roe_mean"], row["roe_std"]), axis=1
)
df["roa_z"] = df.apply(
    lambda row: safe_z(row["roa"], row["roa_mean"], row["roa_std"]), axis=1
)


def compute_quant_scores(df, tenyr_yield):
    scores = []
    for _, row in df.iterrows():
        valuation_score, momentum_score = quant_style_score(
            price_vs_fair_upper=row["price_vs_fair_upper"],
            price_vs_fair_lower=row["price_vs_fair_lower"],
            fcf_yield_rank=row["fcf_yield_rank"],
            fcf_vs_treasury_spread=(
                row["fcf_yield"] - tenyr_yield if row["fcf_yield"] is not None else None
            ),
            per=row["per"],
            per_rank=row["per_rank"],
            pbr_rank=row["pbr_rank"],
            de=row["de"],
            cr=row["cr"],
            industry_per=row["industry_per"],
            roe_z=row["roe_z"],
            roa_z=row["roa_z"],
            roe=row["roe"],  # ì¶”ê°€
            roa=row["roa"],  # ì¶”ê°€
            icr=row["icr"],
            fcf_cagr_rank=row["fcf_cagr_rank"],
            eps_cagr_rank=row["eps_cagr_rank"],
            div_cagr_rank=row["div_cagr_rank"],
            eps=row["eps"],
            div_yield=row["div_yield"],
            opinc_yoy=row["opinc_yoy"],
            opinc_qoq=row["opinc_qoq"],
            industry_roe=row["industry_roe"],
            industry_roa=row["industry_roa"],
        )
        scores.append(
            {
                "ticker": row["ticker"],
                "valuation_score": valuation_score,
                "momentum_score": momentum_score,
                "total_score": valuation_score + momentum_score,
            }
        )
    return pd.DataFrame(scores)


###############
def compute_price_flow_scores(df_main, df_batch_result):
    scores = []
    for ticker in df_main["ticker"]:
        row = df_batch_result.loc[df_batch_result["Ticker"] == ticker]
        if row.empty:
            scores.append(None)
            continue

        ma = bool(row["ma_crossover"].values[0])
        ma_lt = bool(row["ma_crossover_lt"].values[0])
        ret20 = row["return_20d"].values[0]
        ret60 = row["return_60d"].values[0]
        rsi = bool(row["rsi_rebound"].values[0])
        macd = bool(row["macd_golden_cross"].values[0])

        score = score_momentum(ma, ma_lt, ret20, ret60, rsi, macd)
        scores.append(score)
    return scores


# ë©”ì¸ dfì— price_flow_score ì»¬ëŸ¼ ì¶”ê°€
# Step 1: price_flow_score ë¨¼ì € ê³„ì‚°
df["price_flow_score"] = compute_price_flow_scores(df, df_batch_result)

# Step 2: í€€íŠ¸ ì ìˆ˜ ê³„ì‚° (valuation_score, momentum_score, total_score ë“±)
tenyr_yield = get_10yr_treasury_yield()
score_df = compute_quant_scores(df, tenyr_yield)

# Step 3: ë‘ ê²°ê³¼ merge (ì´ë•Œ total_scoreê°€ ìƒê¹€)
final_df = df.merge(score_df, on="ticker", how="left")

# Step 4: total_scoreì— price_flow_score ë”í•˜ê¸°
final_df["total_score"] = final_df["total_score"].fillna(0) + final_df[
    "price_flow_score"
].fillna(0)


def normalize_series(series):
    min_val = series.min()
    max_val = series.max()
    if pd.isna(min_val) or pd.isna(max_val) or min_val == max_val:
        return pd.Series([0.0] * len(series), index=series.index)
    return (series - min_val) / (max_val - min_val) * 100  # âœ… Scale to 0â€“100


# Normalize each category to 0â€“100
final_df["valuation_score_norm"] = normalize_series(final_df["valuation_score"])
final_df["momentum_score_norm"] = normalize_series(final_df["momentum_score"])
final_df["price_flow_score_norm"] = normalize_series(
    final_df["price_flow_score"].fillna(0)
)

# Buffett-style
valuation_weight = 0.4
momentum_weight = 0.3
price_flow_weight = 0.3

# quant fund
# valuation_weight = 0.4
# momentum_weight = 0.4
# price_flow_weight = 0.2

final_df["total_score"] = (
    final_df["valuation_score_norm"] * valuation_weight
    + final_df["momentum_score_norm"] * momentum_weight
    + final_df["price_flow_score_norm"] * price_flow_weight
)

# Round the normalized scores and total
score_cols = [
    "valuation_score_norm",
    "momentum_score_norm",
    "price_flow_score_norm",
    "total_score",
]

final_df[score_cols] = final_df[score_cols].round()

# 1) rename_dict ì •ì˜
rename_dict = {
    "ticker": "í‹°ì»¤",
    "name": "ì¢…ëª©",  # ì‹¤ì œ final_dfì— name ì»¬ëŸ¼ì´ ìˆìœ¼ë©´
    "industry": "ì—…ì¢…",
    "price": "í˜„ì¬ê°€",
    "1M_Change": "1ê°œì›”ëŒ€ë¹„",
    "valuation_score_norm": "ë°¸ë¥˜ì—ì´ì…˜",
    "momentum_score_norm": "ì‹¤ì ëª¨ë©˜í…€",
    "price_flow_score_norm": "ê°€ê²©/ìˆ˜ê¸‰",
    "total_score": "ì´ì ìˆ˜",
}

# 2) ì»¬ëŸ¼ëª… ë³€ê²½
final_df = final_df.rename(columns=rename_dict)

# 3) ë‚´ë³´ë‚¼ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ (ì›í•˜ëŠ” ìˆœì„œ ë° ì»¬ëŸ¼ë§Œ)
export_columns_kr = [
    "í‹°ì»¤",
    "ì¢…ëª©",
    "ì´ì ìˆ˜",
    "ì—…ì¢…",
    "í˜„ì¬ê°€",
    "1ê°œì›”ëŒ€ë¹„",
    "ë°¸ë¥˜ì—ì´ì…˜",
    "ì‹¤ì ëª¨ë©˜í…€",
    "ê°€ê²©/ìˆ˜ê¸‰",
]

# 4) ì •ë ¬
df = pd.DataFrame()
# ì»¬ëŸ¼ì„ í•„í„°ë§í•œ ìƒˆë¡œìš´ dfë¡œ overwrite
df = (
    final_df[export_columns_kr]
    .sort_values(by="ì´ì ìˆ˜", ascending=False)
    .reset_index(drop=True)
)
df = df.drop(columns=[col for col in df.columns if col not in export_columns_kr])

# 3ï¸âƒ£ ì»¬ëŸ¼ ìˆœì„œ ë§ì¶”ê¸° (í˜¹ì‹œ ìˆœì„œ í‹€ì–´ì¡Œì„ ìˆ˜ë„ ìˆìœ¼ë‹ˆ)
df = df[export_columns_kr]
# í‹°ì»¤ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±° (ì²« ë²ˆì§¸ í•­ëª©ë§Œ ë‚¨ê¹€)
df = df.drop_duplicates(subset="í‹°ì»¤", keep="first")


# ê·¸ë¦¬ê³  ê·¸ëŒ€ë¡œ ì €ì¥
df.to_excel(excel_path, index=False)


# 6) ìƒìœ„ í‹°ì»¤ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ

top_tickers_news = df["í‹°ì»¤"].head(news_lookup).tolist()


#################################################################
# Gemini 2.5 Flash API rate limits:
# rpm: 10 (requests per minute)
# tpm: 250,000 (tokens per minute)
# rpd: 250 (requests per day)

# ì ì ˆí•œ sleep_time ê³„ì‚°:
# - 1ë¶„ì— 10íšŒ ìš”ì²­ ê°€ëŠ¥ â†’ 1íšŒ ìš”ì²­ í›„ ìµœì†Œ 6ì´ˆ ëŒ€ê¸° í•„ìš” (60ì´ˆ / 10íšŒ = 6ì´ˆ)
# - í•˜ë£¨ 250íšŒ ì œí•œ â†’ 250ê°œ ì´ˆê³¼ ì‹œ ì¶”ê°€ ëŒ€ê¸° í•„ìš”
# - í† í° ì œí•œì€ ì¼ë°˜ ë‰´ìŠ¤/ëª¨íŠ¸ í”„ë¡¬í”„íŠ¸ì—ì„œëŠ” ê±°ì˜ ë„ë‹¬í•˜ì§€ ì•ŠìŒ

# ë”°ë¼ì„œ moat/news batch í•¨ìˆ˜ì—ì„œ sleep_time=6~7ì´ˆ ê¶Œì¥
# ì˜ˆì‹œ:
# moat_df = generate_moat_summary_batch(df, moat_limit, batch_size=10, sleep_time=7)
# (batch_size=10, sleep_time=7 â†’ 1ë¶„ì— ìµœëŒ€ 8~9íšŒ ìš”ì²­, ì•ˆì „)

# ë§Œì•½ ì—¬ëŸ¬ ìŠ¤ë ˆë“œ/í”„ë¡œì„¸ìŠ¤ì—ì„œ ë™ì‹œì— Gemini í˜¸ì¶œ ì‹œ, ë°˜ë“œì‹œ ì „ì²´ ìš”ì²­ í•©ì‚°ì´ rpm/rpdë¥¼ ë„˜ì§€ ì•Šë„ë¡ ì¡°ì • í•„ìš”


# ì°¸ê³ : ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œëŠ” 7ì´ˆ ì´ìƒ(ì˜ˆ: 8~10ì´ˆ)ë¡œ ì—¬ìœ  ìˆê²Œ ì„¤ì •í•˜ë©´ ë” ì•ˆì „í•¨


def generate_moat_summary_batch(
    df: pd.DataFrame, batch_size: int = 10, sleep_time: int = 8
) -> pd.DataFrame:
    top_tickers = df["ì¢…ëª©"].tolist()
    moat_data = []

    for i in range(0, len(top_tickers), batch_size):
        batch = top_tickers[i : i + batch_size]
        for ticker in batch:
            try:
                prompt = analyze_moat(ticker, date_kr_ymd)
                moat_text = query_gemini(prompt)
                parsed_response = parse_moat_response(moat_text)
                moat_data.append(
                    {
                        "ê¸°ì—…ëª…": ticker,
                        "ê²½ìŸ ìš°ìœ„ ë¶„ì„": parsed_response["moat_analysis"],
                        "Moat ì ìˆ˜": parsed_response["moat_score"],
                    }
                )
                time.sleep(1)
            except Exception as e:
                moat_data.append(
                    {
                        "ê¸°ì—…ëª…": f"âŒ ì˜¤ë¥˜: {str(e)}",
                        "ê²½ìŸ ìš°ìœ„ ë¶„ì„": "ë¶„ì„ ì‹¤íŒ¨",
                        "Moat ì ìˆ˜": "ë¶„ì„ ì‹¤íŒ¨",
                    }
                )
        if i + batch_size < len(top_tickers):
            print(
                f"Batch {i // batch_size + 1} completed. Sleeping {sleep_time} seconds to avoid rate limit..."
            )
            time.sleep(sleep_time)
    return pd.DataFrame(moat_data)


moat_df = generate_moat_summary_batch(df, batch_size=10, sleep_time=8)


#################################################################
# 1. ticker / ê¸°ì—…ëª… ê¸°ì¤€ìœ¼ë¡œ moat_dfë¥¼ dfì— merge
df = df.merge(
    moat_df[["ê¸°ì—…ëª…", "Moat ì ìˆ˜"]],
    left_on="ì¢…ëª©",  # final_df / dfì—ì„œ ê¸°ì—…ëª…ì„ ë‚˜íƒ€ë‚´ëŠ” ì»¬ëŸ¼
    right_on="ê¸°ì—…ëª…",  # moat_dfì—ì„œ ê¸°ì—…ëª… ì»¬ëŸ¼
    how="left",
)

# 2. Moat ì ìˆ˜ ê²°ì¸¡ê°’ì€ 0ìœ¼ë¡œ ì±„ì›€
df["Moat ì ìˆ˜"] = df["Moat ì ìˆ˜"].fillna(0).astype(float)


df["moat_score_norm"] = normalize_series(df["Moat ì ìˆ˜"])


# 4. ê¸°ì¡´ ê°€ì¤‘ì¹˜ ì„¤ì • (ì˜ˆ: Buffett ìŠ¤íƒ€ì¼ì— Moat í¬í•¨)
valuation_weight = 0.35
moat_weight = 0.35  # Moat ê°€ì¤‘ì¹˜ (ì¡°ì ˆ ê°€ëŠ¥)
momentum_weight = 0.2
price_flow_weight = 0.1

# 5. ìƒˆ total_score ê³„ì‚°
df["ì´ì ìˆ˜"] = (
    df["ë°¸ë¥˜ì—ì´ì…˜"] * valuation_weight
    + df["ì‹¤ì ëª¨ë©˜í…€"] * momentum_weight
    + df["ê°€ê²©/ìˆ˜ê¸‰"] * price_flow_weight
    + df["moat_score_norm"] * moat_weight
)


score_cols = ["ë°¸ë¥˜ì—ì´ì…˜", "ì‹¤ì ëª¨ë©˜í…€", "ê°€ê²©/ìˆ˜ê¸‰", "moat_score_norm", "ì´ì ìˆ˜"]
df[score_cols] = df[score_cols].round()

# 7. í•„ìš”í•˜ë©´ ì •ë ¬
df = df.sort_values(by="ì´ì ìˆ˜", ascending=False).reset_index(drop=True)
df = df.drop(columns=["ê¸°ì—…ëª…", "moat_score_norm"])


#################################################################
def get_news_for_tickers(tickers, api_token):
    all_news = []

    for ticker in tickers:
        try:
            company_info = yf.Ticker(ticker).info
            full_name = company_info.get("shortName", "")
        except Exception as e:
            print(f"[{ticker}] âš ï¸ Failed to retrieve company info: {e}")
            continue

        if not full_name:
            print(f"[{ticker}] âš ï¸ No company name found, skipping.")
            continue

        published_after = (datetime.now() - timedelta(days=2)).strftime("%Y-%m-%d")
        url = "https://api.marketaux.com/v1/news/all"
        params = {
            "api_token": api_token,
            "symbols": ticker.upper(),
            "language": "en",
            "published_after": published_after,
            "limit": 5,  # Fetch extra to allow filtering
            "sort": "relevance",
        }

        try:
            response = requests.get(url, params=params)
            response.raise_for_status()
            articles = response.json().get("data", [])
        except Exception as e:
            print(f"[{ticker}] âŒ API request failed: {e}")
            continue

        filtered_articles = []

        for article in articles:
            relevant = False
            sentiment_score = None

            for entity in article.get("entities", []):
                if entity.get("symbol", "").upper() == ticker.upper():
                    relevant = True
                    score = entity.get("sentiment_score")
                    try:
                        sentiment_score = round(float(score), 2)
                    except (TypeError, ValueError):
                        sentiment_score = None
                    break

            if not relevant:
                continue

            filtered_articles.append(
                {
                    "ê¸°ì—…ëª…": full_name,
                    "ê¸°ì‚¬ ì œëª©": article.get("title"),
                    "ê°ì •ì§€ìˆ˜": sentiment_score,
                    "ë‰´ìŠ¤ ìš”ì•½": article.get("description"),
                    "ë°œí–‰ì¼": article.get("published_at", "")[:10],
                    "URL": article.get("url"),
                }
            )

            if len(filtered_articles) >= 3:
                break

        if filtered_articles:
            all_news.extend(filtered_articles)
        else:
            print(f"[{ticker}] â„¹ï¸ No relevant news articles found.")

    return pd.DataFrame(all_news)


#################################################################
news_df = get_news_for_tickers(top_tickers_news, api_token=marketaux_api)
#################################################################

# Seleccionar Criterio de OptimizaciÃ³n
optimization_criterion = "sortino"  # Cambia a 'sharpe', 'cvar', 'sortino' o 'variance' para optimizar esos criterios
df = df.sort_values(by="ì´ì ìˆ˜", ascending=False).reset_index(drop=True)
top_tickers = df["í‹°ì»¤"].head(opt).tolist()
symbols = top_tickers

# ì˜¤ëŠ˜ ë‚ ì§œ
end_date = dt.datetime.today() - dt.timedelta(days=weekend)

# 1ë…„ ì „ ë‚ ì§œ (365ì¼ ì „)
start_date = end_date - timedelta(days=365)

# ë¬¸ìì—´ í¬ë§·ìœ¼ë¡œ ë³€í™˜ (yfinanceì— ë§ê²Œ)
start_str = start_date.strftime("%Y-%m-%d")
end_str = end_date.strftime("%Y-%m-%d")

# 1. 'Close' ì»¬ëŸ¼ë§Œ ì¶”ì¶œ (MultiIndex ì „ìš©)
if isinstance(cache.columns, pd.MultiIndex):
    # 'Close' ì»¬ëŸ¼ë§Œ ì„ íƒ
    close_columns = [col for col in cache.columns if col[1] == "Close"]
    close_df = cache[close_columns].copy()
    close_df.columns = [col[0] for col in close_columns]  # â†’ ('AAPL', 'Close') â†’ 'AAPL'
else:
    raise ValueError(
        "Expected MultiIndex columns in cache, but got single-index DataFrame."
    )

# 2. ìœ íš¨í•œ ì¢…ëª©(symbols)ë§Œ ì¶”ì¶œ
symbols_in_data = [s for s in symbols if s in close_df.columns]
if not symbols_in_data:
    raise ValueError("No valid symbols found in cached data.")

data = close_df[symbols_in_data]

# 3. ëª¨ë‘ NaNì¸ ì¢…ëª© ì œê±°
data = data.dropna(axis=1, how="all")

# 4. ì œê±°ëœ í‹°ì»¤ ë¡œê¹…
removed = [s for s in symbols if s not in data.columns]
for r in removed:
    print(f"âš ï¸  Removed due to all NaN: {r}")

# 5. ìµœì¢… ê²€ì¦
if data.empty or data.shape[1] == 0:
    raise ValueError("No valid data left after NaN filtering.")

returns = data.pct_change(fill_method="pad").dropna()


# Sharpe Ratio ìµœì í™” í•¨ìˆ˜
def objective_sharpe(weights):
    port_return = np.dot(weights, returns.mean()) * 252
    port_vol = np.sqrt(np.dot(weights.T, np.dot(returns.cov() * 252, weights)))
    return -port_return / port_vol  # ìµœëŒ€í™” ìœ„í•´ ìŒìˆ˜


# CVaR ìµœì í™” í•¨ìˆ˜ (5% VaR ê¸°ì¤€)
def objective_cvar(weights):
    portfolio_returns = returns.dot(
        weights
    )  # ìˆ˜ì •: np.dot(returns, weights)ë„ ê°€ëŠ¥í•˜ì§€ë§Œ DataFrameì´ë©´ .dotì´ ë” ì•ˆì „
    alpha = 0.05
    var = np.percentile(portfolio_returns, 100 * alpha)
    cvar = portfolio_returns[portfolio_returns <= var].mean()
    return cvar  # minimizeì—ì„œ ìµœì†Œí™”(ì†ì‹¤ ìµœëŒ€í™”) â†’ ë¶€í˜¸ ë°”ê¿”ì•¼ í•¨
    # return -cvar  # CVaR ìµœëŒ€í™”í•˜ë ¤ë©´ ìŒìˆ˜ë¡œ ë°˜í™˜


# Sortino Ratio ìµœì í™” í•¨ìˆ˜
def objective_sortino(weights):
    portfolio_returns = returns.dot(
        weights
    )  # ìˆ˜ì •: np.dot(weights) â†’ returns.dot(weights)
    mean_return = portfolio_returns.mean() * 252
    downside_returns = portfolio_returns[portfolio_returns < 0]
    downside_std = downside_returns.std() * np.sqrt(252)
    if downside_std == 0:
        return 0  # ë˜ëŠ” í° ê°’ ë°˜í™˜
    sortino_ratio = mean_return / downside_std
    return -sortino_ratio  # ìµœëŒ€í™” ìœ„í•´ ìŒìˆ˜


# ë¶„ì‚° ìµœì†Œí™” í•¨ìˆ˜
def objective_variance(weights):
    return np.dot(weights.T, np.dot(returns.cov() * 252, weights))


# Las restricciones
cons = {"type": "eq", "fun": lambda x: np.sum(x) - 1}

# Los lÃ­mites para los pesos
bounds = tuple((0, 1) for x in range(len(symbols)))


# OptimizaciÃ³n
init_guess = np.array(
    len(symbols)
    * [
        1.0 / len(symbols),
    ]
)
if optimization_criterion == "sharpe":
    opt_results = minimize(
        objective_sharpe, init_guess, method="SLSQP", bounds=bounds, constraints=cons
    )
elif optimization_criterion == "cvar":
    opt_results = minimize(
        objective_cvar, init_guess, method="SLSQP", bounds=bounds, constraints=cons
    )
elif optimization_criterion == "sortino":
    opt_results = minimize(
        objective_sortino, init_guess, method="SLSQP", bounds=bounds, constraints=cons
    )
elif optimization_criterion == "variance":
    opt_results = minimize(
        objective_variance, init_guess, method="SLSQP", bounds=bounds, constraints=cons
    )

# Los pesos Ã³ptimos
optimal_weights = opt_results.x


# Optimizar todos los criterios
opt_results_cvar = minimize(
    objective_cvar, init_guess, method="SLSQP", bounds=bounds, constraints=cons
)
opt_results_sortino = minimize(
    objective_sortino, init_guess, method="SLSQP", bounds=bounds, constraints=cons
)
opt_results_variance = minimize(
    objective_variance, init_guess, method="SLSQP", bounds=bounds, constraints=cons
)
opt_results_sharpe = minimize(
    objective_sharpe, init_guess, method="SLSQP", bounds=bounds, constraints=cons
)

# Pesos Ã³ptimos para cada criterio
optimal_weights_cvar = opt_results_cvar.x
optimal_weights_sortino = opt_results_sortino.x
optimal_weights_variance = opt_results_variance.x
optimal_weights_sharpe = opt_results_sharpe.x

# Graficar la frontera eficiente
port_returns = []
port_volatility = []
sharpe_ratio = []
all_weights = []  # almacena los pesos de todas las carteras simuladas

num_assets = len(symbols)
num_portfolios = 50000

np.random.seed(101)

for single_portfolio in range(num_portfolios):
    weights = np.random.random(num_assets)
    weights /= np.sum(weights)
    returns_portfolio = np.dot(weights, returns.mean()) * 252
    volatility = np.sqrt(np.dot(weights.T, np.dot(returns.cov() * 252, weights)))
    sr = returns_portfolio / volatility
    sharpe_ratio.append(sr)
    port_returns.append(returns_portfolio)
    port_volatility.append(volatility)
    all_weights.append(weights)  # registra los pesos para esta cartera

plt.figure(figsize=(12, 8))
plt.scatter(port_volatility, port_returns, c=sharpe_ratio, cmap="viridis")
plt.colorbar(label="Sharpe Ratio")
plt.xlabel("Volatility")
plt.ylabel("Return")

# Calcular y graficar los retornos y la volatilidad del portafolio Ã³ptimo para cada criterio
opt_returns_cvar = np.dot(optimal_weights_cvar, returns.mean()) * 252
opt_volatility_cvar = np.sqrt(
    np.dot(optimal_weights_cvar.T, np.dot(returns.cov() * 252, optimal_weights_cvar))
)
opt_portfolio_cvar = plt.scatter(
    opt_volatility_cvar, opt_returns_cvar, color="hotpink", s=50, label="CVaR"
)

opt_returns_sortino = np.dot(optimal_weights_sortino, returns.mean()) * 252
opt_volatility_sortino = np.sqrt(
    np.dot(
        optimal_weights_sortino.T, np.dot(returns.cov() * 252, optimal_weights_sortino)
    )
)
opt_portfolio_sortino = plt.scatter(
    opt_volatility_sortino, opt_returns_sortino, color="g", s=50, label="Sortino"
)

opt_returns_variance = np.dot(optimal_weights_variance, returns.mean()) * 252
opt_volatility_variance = np.sqrt(
    np.dot(
        optimal_weights_variance.T,
        np.dot(returns.cov() * 252, optimal_weights_variance),
    )
)
opt_portfolio_variance = plt.scatter(
    opt_volatility_variance, opt_returns_variance, color="b", s=50, label="Variance"
)

opt_returns_sharpe = np.dot(optimal_weights_sharpe, returns.mean()) * 252
opt_volatility_sharpe = np.sqrt(
    np.dot(
        optimal_weights_sharpe.T, np.dot(returns.cov() * 252, optimal_weights_sharpe)
    )
)
opt_portfolio_sharpe = plt.scatter(
    opt_volatility_sharpe, opt_returns_sharpe, color="r", s=50, label="Sharpe"
)

plt.legend(loc="upper right")

plt.show()


# FunciÃ³n para calcular el drawdown mÃ¡ximo
def max_drawdown(return_series):
    comp_ret = (1 + return_series).cumprod()
    peak = comp_ret.expanding(min_periods=1).max()
    dd = (comp_ret / peak) - 1
    return dd.min()


def detailed_portfolio_statistics(weights):
    portfolio_returns = returns.dot(weights)
    mean_return_annualized = gmean(portfolio_returns + 1) ** 252 - 1
    std_dev_annualized = portfolio_returns.std() * np.sqrt(252)
    skewness = skew(portfolio_returns)
    kurt = kurtosis(portfolio_returns)
    max_dd = max_drawdown(portfolio_returns)
    count = len(portfolio_returns)

    # âœ… Safe TNX fetch with fallback
    try:
        tnx = yf.Ticker("^TNX")
        tnx_data = tnx.history(period="1d")
        latest_yield = tnx_data["Close"].iloc[-1]
        risk_free_rate = round(latest_yield / 100.0, 2)
    except Exception as e:
        print(f"âš ï¸ Failed to fetch TNX: {e}")
        risk_free_rate = 0.04  # default 4% fallback

    sharpe_ratio = (mean_return_annualized - risk_free_rate) / std_dev_annualized

    # CVaR ê³„ì‚° (5% ìˆ˜ì¤€)
    alpha = 0.05
    sorted_returns = np.sort(portfolio_returns)
    var_index = int(np.floor(alpha * len(sorted_returns)))
    var = sorted_returns[var_index]
    cvar = sorted_returns[:var_index].mean()
    cvar_annualized = (1 + cvar) ** 252 - 1  # ì—°ìœ¨í™”

    downside_returns = portfolio_returns[portfolio_returns < 0]
    downside_std_dev = downside_returns.std() * np.sqrt(252)
    sortino_ratio = (
        mean_return_annualized / downside_std_dev if downside_std_dev != 0 else np.nan
    )
    variance = std_dev_annualized**2

    return (
        mean_return_annualized,
        std_dev_annualized,
        skewness,
        kurt,
        max_dd,
        count,
        sharpe_ratio,
        cvar_annualized,
        sortino_ratio,
        variance,
    )


# Calcular estadÃ­sticas para cada portafolio
statistics_cvar = detailed_portfolio_statistics(optimal_weights_cvar)
statistics_sortino = detailed_portfolio_statistics(optimal_weights_sortino)
statistics_variance = detailed_portfolio_statistics(optimal_weights_variance)
statistics_sharpe = detailed_portfolio_statistics(optimal_weights_sharpe)

# Nombres de las estadÃ­sticas
statistics_names = [
    "Retorno anualizado",
    "Volatilidad anualizada",
    "Skewness",
    "Kurtosis",
    "Max Drawdown",
    "Conteo de datos",
    "Sharpe Ratio",
    "CVaR",
    "Ratio Sortino",
    "Varianza",
]

# Diccionario que asocia los nombres de los mÃ©todos de optimizaciÃ³n con los pesos Ã³ptimos y las estadÃ­sticas
portfolio_data = {
    "CVaR": {
        "weights": optimal_weights_cvar,
        "statistics": detailed_portfolio_statistics(optimal_weights_cvar),
    },
    "Sortino": {
        "weights": optimal_weights_sortino,
        "statistics": detailed_portfolio_statistics(optimal_weights_sortino),
    },
    "Variance": {
        "weights": optimal_weights_variance,
        "statistics": detailed_portfolio_statistics(optimal_weights_variance),
    },
    "Sharpe": {
        "weights": optimal_weights_sharpe,
        "statistics": detailed_portfolio_statistics(optimal_weights_sharpe),
    },
}

# 1. í¬íŠ¸í´ë¦¬ì˜¤ ë¹„ì¤‘ í‘œ (ê° ë°©ë²•ë³„, í‹°ì»¤ë³„ ë¹„ì¤‘)
weight_rows = []
for method, data in portfolio_data.items():
    for symbol, weight in zip(symbols, data["weights"]):
        weight_rows.append(
            {"ìµœì í™” ê¸°ì¤€": method, "í‹°ì»¤": symbol, "ë¹„ì¤‘(%)": round(weight * 100, 2)}
        )
df_weights = pd.DataFrame(weight_rows)

# 2. í¬íŠ¸í´ë¦¬ì˜¤ í†µê³„ í‘œ (ê° ë°©ë²•ë³„ í†µê³„ í•œ ì¤„)
statistics_names_kr = [
    "ì—°í™˜ì‚° ìˆ˜ìµë¥ ",
    "ì—°í™˜ì‚° ë³€ë™ì„±",
    "ì™œë„",
    "ì²¨ë„",
    "ìµœëŒ€ ë‚™í­",
    "ë°ì´í„° ê°œìˆ˜",
    "ìƒ¤í”„ ë¹„ìœ¨",
    "CVaR",
    "ì†Œë¥´í‹°ë…¸ ë¹„ìœ¨",
    "ë¶„ì‚°",
]
stats_rows = []
for method, data in portfolio_data.items():
    stats_dict = {"ìµœì í™” ê¸°ì¤€": method}
    for name_kr, stat in zip(statistics_names_kr, data["statistics"]):
        # ìˆ«ìëŠ” ëª¨ë‘ ì†Œìˆ˜ì  ë‘˜ì§¸ìë¦¬ë¡œ ë°˜ì˜¬ë¦¼, ë°ì´í„° ê°œìˆ˜ëŠ” ì •ìˆ˜ë¡œ
        if name_kr == "ë°ì´í„° ê°œìˆ˜":
            stats_dict[name_kr] = int(stat)
        else:
            stats_dict[name_kr] = np.round(stat, 2)
    stats_rows.append(stats_dict)
df_stats = pd.DataFrame(stats_rows)


def autofit_columns_and_wrap(ws, df: pd.DataFrame, workbook):
    # í”½ì…€ -> ë¬¸ì ìˆ˜ í™˜ì‚° (0.1428 ë°°ìœ¨ ê¸°ì¤€)
    pixel_widths = [92, 200, 50, 500, 85, 150]
    char_widths = [round(p * 0.1428) for p in pixel_widths]

    # wrap + top-align í¬ë§·
    wrap_format = workbook.add_format({"text_wrap": True, "valign": "top"})

    # í—¤ë” ì‘ì„± ë° ì—´ ë„ˆë¹„ ì„¤ì •
    for i, col in enumerate(df.columns):
        width = char_widths[i] if i < len(char_widths) else 20
        ws.set_column(i, i, width)
        ws.write(0, i, str(col), wrap_format)

    # ë°ì´í„° ì…€ ì‘ì„±
    for row in range(1, len(df) + 1):
        for col in range(len(df.columns)):
            val = df.iat[row - 1, col]

            # NaN / inf / None -> ë¬¸ìì—´ ë³€í™˜
            if isinstance(val, float):
                if math.isnan(val) or math.isinf(val):
                    val = str(val)
            elif val is None:
                val = ""

            # Excel ì“°ê¸° ì‹¤íŒ¨ ëŒ€ë¹„ ì•ˆì „ write
            try:
                ws.write(row, col, val, wrap_format)
            except Exception:
                ws.write(row, col, str(val), wrap_format)


def autofit_columns_and_wrap_moat(ws, df: pd.DataFrame, workbook):

    # ì—´ ë„ˆë¹„ ì„¤ì • (í”½ì…€ ê¸°ì¤€ â†’ ë¬¸ì ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜)
    pixel_widths = [92, 500]
    char_widths = [round(p * 0.1428) for p in pixel_widths]  # = [13, 71]

    # wrap + top-align í¬ë§·
    wrap_format = workbook.add_format({"text_wrap": True, "valign": "top"})

    # ì—´ ë„ˆë¹„ ë° í—¤ë” ì„¤ì •
    for i, col in enumerate(df.columns):
        width = char_widths[i] if i < len(char_widths) else 20
        ws.set_column(i, i, width)
        ws.write(0, i, str(col), wrap_format)

    # ë°ì´í„° ì…€ ì‘ì„±
    for row in range(1, len(df) + 1):
        for col in range(len(df.columns)):
            val = df.iat[row - 1, col]

            # NaN / inf / None ì²˜ë¦¬
            if isinstance(val, float):
                if math.isnan(val) or math.isinf(val):
                    val = str(val)
            elif val is None:
                val = ""

            try:
                ws.write(row, col, val, wrap_format)
            except Exception:
                ws.write(row, col, str(val), wrap_format)


with pd.ExcelWriter(excel_path, engine="xlsxwriter") as writer:

    # ì¢…ëª©ë¶„ì„ ì‹œíŠ¸ ë¨¼ì € ìƒì„±í•´ì•¼ í•¨
    df.to_excel(
        writer, index=False, sheet_name="ì¢…ëª©ë¶„ì„"
    )  # df_analysisëŠ” ì¢…ëª©ë¶„ì„ ë°ì´í„°í”„ë ˆì„

    # ê²½ìŸìš°ìœ„(Moat) ì‹œíŠ¸ ì €ì¥ ë° í‘œ ì ìš©
    moat_df.to_excel(writer, index=False, sheet_name="ê²½ìŸìš°ìœ„ë¶„ì„")
    ws_moat = writer.sheets["ê²½ìŸìš°ìœ„ë¶„ì„"]
    (mr_moat, mc_moat) = moat_df.shape
    ws_moat.add_table(
        0,
        0,
        mr_moat,
        mc_moat - 1,
        {
            "columns": [{"header": col} for col in moat_df.columns],
            "style": "Table Style Medium 9",
        },
    )
    autofit_columns_and_wrap_moat(ws_moat, moat_df, writer.book)

    # ê¸°ì¡´ í¬íŠ¸í´ë¦¬ì˜¤ë¹„ì¤‘ ì‹œíŠ¸ ëŒ€ì‹  ê° ê¸°ì¤€ë³„ë¡œ ë‚˜ëˆ ì„œ ì €ì¥ (ì—‘ì…€ í‘œë¡œ)
    for method in ["CVaR", "Sortino", "Variance", "Sharpe"]:
        df_method = df_weights[df_weights["ìµœì í™” ê¸°ì¤€"] == method]

        df_method = df_method[df_method["ë¹„ì¤‘(%)"] != 0]

        df_method.to_excel(writer, index=False, sheet_name=f"í¬íŠ¸ë¹„ì¤‘_{method}")
        ws = writer.sheets[f"í¬íŠ¸ë¹„ì¤‘_{method}"]
        (mr, mc) = df_method.shape
        ws.add_table(
            0,
            0,
            mr,
            mc - 1,
            {
                "columns": [{"header": col} for col in df_method.columns],
                "style": "Table Style Medium 9",
            },
        )

    # í¬íŠ¸í´ë¦¬ì˜¤í†µê³„ ì‹œíŠ¸ë„ ì—‘ì…€ í‘œë¡œ
    df_stats.to_excel(writer, index=False, sheet_name="í¬íŠ¸í´ë¦¬ì˜¤í†µê³„")
    ws_stats = writer.sheets["í¬íŠ¸í´ë¦¬ì˜¤í†µê³„"]
    (mr_stats, mc_stats) = df_stats.shape
    ws_stats.add_table(
        0,
        0,
        mr_stats,
        mc_stats - 1,
        {
            "columns": [{"header": col} for col in df_stats.columns],
            "style": "Table Style Medium 9",
        },
    )

    # ë‰´ìŠ¤ ë°ì´í„°í”„ë ˆì„ ì‹œíŠ¸ ìƒì„± ë° í‘œ ì ìš©
    news_df.to_excel(writer, index=False, sheet_name="ì¢…ëª©ë‰´ìŠ¤")
    ws_news = writer.sheets["ì¢…ëª©ë‰´ìŠ¤"]
    (nr, nc) = news_df.shape
    ws_news.add_table(
        0,
        0,
        nr,
        nc - 1,
        {
            "columns": [{"header": col} for col in news_df.columns],
            "style": "Table Style Medium 9",
        },
    )
    autofit_columns_and_wrap(ws_news, news_df, writer.book)

    workbook = writer.book
    # 1) dfë¡œ í†µì¼
    worksheet = writer.sheets["ì¢…ëª©ë¶„ì„"]

    currency_format = workbook.add_format({"num_format": "$#,##.00"})

    # 4ï¸âƒ£ "í˜„ì¬ê°€" ì»¬ëŸ¼ ìœ„ì¹˜ êµ¬í•´ì„œ ì„œì‹ ì ìš©
    price_col_idx = df.columns.get_loc("í˜„ì¬ê°€")  # 0ë¶€í„° ì‹œì‘í•˜ëŠ” ì¸ë±ìŠ¤
    for row in range(1, len(df) + 1):  # í—¤ë” ì œì™¸, 1ë¶€í„° ì‹œì‘
        value = df.at[row - 1, "í˜„ì¬ê°€"]
        if isinstance(value, float) and (math.isnan(value) or math.isinf(value)):
            value = 0
        worksheet.write_number(row, price_col_idx, value, currency_format)

    start_row = 0  # data starts after header row 1
    end_row = len(df)
    start_col = 0
    end_col = len(df.columns) - 1

    def xl_col(col_idx):
        div = col_idx + 1
        string = ""
        while div > 0:
            div, mod = div // 26, div % 26
            if mod == 0:
                mod = 26
                div -= 1
            string = chr(64 + mod) + string
        return string

    first_cell = f"{xl_col(start_col)}{start_row + 1}"
    last_cell = f"{xl_col(end_col)}{end_row + 1}"
    data_range = f"{first_cell}:{last_cell}"

    # 1) Add Excel table for the data
    worksheet.add_table(
        data_range,
        {
            "columns": [{"header": col} for col in df.columns],
            "style": "Table Style Medium 9",
        },
    )

    # 5) ì»¬ëŸ¼ë³„ ë„ˆë¹„ ì§€ì •
    col_widths = {
        "í‹°ì»¤": 6,
        "ì¢…ëª©": 25,
        "ì—…ì¢…": 25,
        "í˜„ì¬ê°€": 10,
        "1ê°œì›”ëŒ€ë¹„": 10,
    }
    for col_name, width in col_widths.items():
        if col_name in df.columns:
            col_idx = df.columns.get_loc(col_name)
            worksheet.set_column(col_idx, col_idx, width)

    # 6) ê·¸ë¼ë°ì´ì…˜ í¬ë§·íŒ… ì ìš© (ì´ì ìˆ˜ ì»¬ëŸ¼)
    total_score_col_idx = df.columns.get_loc("ì´ì ìˆ˜")
    total_score_col_letter = xl_col(total_score_col_idx)
    total_score_range = (
        f"{total_score_col_letter}{start_row + 1}:{total_score_col_letter}{end_row + 1}"
    )

    worksheet.conditional_format(
        total_score_range,
        {
            "type": "3_color_scale",
            "min_type": "min",
            "mid_type": "percentile",
            "mid_value": 50,
            "max_type": "max",
            "min_color": "#FF0000",
            "mid_color": "#FFFF00",
            "max_color": "#00FF00",
        },
    )


def generate_prompt(df_news: pd.DataFrame) -> str:

    news_summary = []
    if {"ê¸°ì—…ëª…", "ê°ì •ì§€ìˆ˜", "ë‰´ìŠ¤ ìš”ì•½"}.issubset(df_news.columns):
        grouped = df_news.groupby("ê¸°ì—…ëª…")
        for comp, group in grouped:
            avg_sent = group["ê°ì •ì§€ìˆ˜"].mean()
            recent_summaries = (
                group.sort_values(by="ë°œí–‰ì¼", ascending=False)["ë‰´ìŠ¤ ìš”ì•½"]
                .head(3)
                .tolist()
            )
            summaries_text = " / ".join([s for s in recent_summaries if s])
            news_summary.append(
                f"{comp}: í‰ê·  ê°ì •ì§€ìˆ˜ {avg_sent:.2f}, ìµœê·¼ ë‰´ìŠ¤ ìš”ì•½: {summaries_text}"
            )

    prompt = f"""
ë‹¹ì‹ ì€ ê¸°ì—… ë¶„ì„ê³¼ ê±°ì‹œê²½ì œ ë¶„ì„ì— ëŠ¥ìˆ™í•œ ì „ë¬¸ ì£¼ì‹ ë¶„ì„ê°€ì…ë‹ˆë‹¤.
í•­ìƒ í•œêµ­ì–´ë¡œ ì‘ë‹µí•´ ì£¼ì„¸ìš”.

ë‹¤ìŒì€ {date_kr_ymd} ê¸°ì¤€ìœ¼ë¡œ ìˆ˜ì§‘ëœ {limit}ê°œ ê¸°ì—…ì˜ ë‰´ìŠ¤ ìš”ì•½ê³¼ ê°ì • ë¶„ì„ ì§€ìˆ˜ì…ë‹ˆë‹¤.  
---

ğŸ“Œ ë‰´ìŠ¤ ìš”ì•½ ë° ê°ì • ì§€ìˆ˜:
{chr(10).join(news_summary)}

---

### ë¶„ì„ ìš”ì²­:

1. {date_kr_ymd} ê¸°ì¤€ ì´ë²ˆ ì£¼ ì£¼ëª©í•  ë§Œí•œ ê¸°ì—… ë‰´ìŠ¤ (3~5ê°œ)  
- ë°˜ë“œì‹œ **ìœ„ ë‰´ìŠ¤ ìš”ì•½ì—ì„œ ì–¸ê¸‰ëœ ê¸°ì—… ë° ë‚´ìš©ë§Œ ì‚¬ìš©**í•´ ì£¼ì„¸ìš”.  
- ê¸°ì—…ëª…ê³¼ í•µì‹¬ ë‰´ìŠ¤, ê·¸ë¦¬ê³  **íˆ¬ì ê´€ì ì—ì„œì˜ ì˜ë¯¸**ë¥¼ ê°„ê²°íˆ ìš”ì•½í•´ ì£¼ì„¸ìš”.  

**ì˜ˆì‹œ í˜•ì‹:**  
- ì—”ë¹„ë””ì•„: 2ë¶„ê¸° ì‹¤ì  ì˜ˆìƒ ìƒíšŒ. ë°˜ë„ì²´ ì—…í™© íšŒë³µ ê¸°ëŒ€ê° ë°˜ì˜.

2. {date_kr_ymd} ê¸°ì¤€ ê±°ì‹œê²½ì œ í™˜ê²½ ìš”ì•½  
- ê´€ì„¸, ê¸ˆë¦¬, ì¸í”Œë ˆì´ì…˜, ê³ ìš©, ì†Œë¹„, ì›-ë‹¬ëŸ¬ í™˜ìœ¨ ë“± ì£¼ìš” ì§€í‘œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°„ê²°íˆ ì •ë¦¬í•´ ì£¼ì„¸ìš”.  
- ìˆ«ìë‚˜ ë°©í–¥ì„± ìœ„ì£¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
- ë°˜ë“œì‹œ ê²€ìƒ‰ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.

3. ë¯¸êµ­ ì¦ì‹œì— ëŒ€í•œ ì˜í–¥ ë¶„ì„  
- ìœ„ ê±°ì‹œê²½ì œ í™˜ê²½ì´ **ë¯¸êµ­ ì¦ì‹œì— ë¯¸ì¹˜ëŠ” ì˜í–¥**ì„ ê°„ê²°íˆ ì„¤ëª…í•´ ì£¼ì„¸ìš”.  
- ê¸ˆë¦¬ ë°©í–¥ì„±, ê¸°ìˆ ì£¼/ê°€ì¹˜ì£¼ ì„ í˜¸, íˆ¬ìì ì‹¬ë¦¬ ë³€í™” ë“±ì„ ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.
- ë°˜ë“œì‹œ ê²€ìƒ‰ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
"""

    return prompt.strip()


##########################################################################################################


def main(df_news):
    prompt = generate_prompt(df_news)
    print("Prompt sent to Gemini:\n", prompt)

    answer = query_gemini(prompt)
    return answer


answer = main(news_df)

#########################################################################################################

msg = EmailMessage()
msg["Subject"] = f"DeepFund Weekly Insights | {date_kr}"
msg["From"] = Address(display_name="Hyungsuk Choi", addr_spec=EMAIL)
msg["To"] = ""  # or '' or a single address to satisfy the 'To' header requirement

content = (
    f"ê·€í•˜ì˜ ì¤‘ì¥ê¸° íˆ¬ì ì°¸ê³ ë¥¼ ìœ„í•´ {date_kr} ê¸°ì¤€, "
    f"ì‹œê°€ì´ì•¡ ìƒìœ„ {limit}ê°œ ìƒì¥ê¸°ì—…ì— ëŒ€í•œ ìµœì‹  í€€íŠ¸ ë¶„ì„ ìë£Œë¥¼ ì „ë‹¬ë“œë¦½ë‹ˆë‹¤. "
    "ê° ê¸°ì—…ì˜ ì´ì ìˆ˜ëŠ” ë°¸ë¥˜ì—ì´ì…˜ ì ìˆ˜, ì‹¤ì ëª¨ë©˜í…€ ì ìˆ˜, ê·¸ë¦¬ê³  ê°€ê²©/ìˆ˜ê¸‰ ì ìˆ˜ë¥¼ ë°˜ì˜í•˜ì˜€ìŠµë‹ˆë‹¤.\n\n"
    "ë³¸ ìë£ŒëŠ” ì›ŒëŸ° ë²„í•ì˜ íˆ¬ì ì² í•™ì„ ê¸°ë°˜ìœ¼ë¡œ, "
    "ê¸°ì—…ì˜ ì¬ë¬´ ê±´ì „ì„± ë° ì‹¤ì ì„ ìˆ˜ì¹˜í™”í•˜ì—¬ í‰ê°€í•œ ê²°ê³¼ì…ë‹ˆë‹¤. "
    "íˆ¬ì íŒë‹¨ ì‹œì—ëŠ” ì •ì„±ì  ìš”ì†Œì— ëŒ€í•œ ë³„ë„ì˜ ë©´ë°€í•œ ê²€í† ë„ "
    "í•¨ê»˜ ë³‘í–‰í•˜ì‹œê¸°ë¥¼ ê¶Œì¥ë“œë¦½ë‹ˆë‹¤.\n\n"
    "ğŸ“Œì£¼ìš” ì¬ë¬´ì§€í‘œ í•´ì„¤\n"
    "D/E ë¶€ì±„ë¹„ìœ¨ (Debt to Equity): ìë³¸ ëŒ€ë¹„ ë¶€ì±„ì˜ ë¹„ìœ¨ë¡œ, ì¬ë¬´ ê±´ì „ì„±ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë‚®ì„ìˆ˜ë¡ ì•ˆì •ì ì…ë‹ˆë‹¤.\n"
    "CR ìœ ë™ë¹„ìœ¨ (Current Ratio): ìœ ë™ìì‚°ì´ ìœ ë™ë¶€ì±„ë¥¼ ì–¼ë§ˆë‚˜ ì»¤ë²„í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\n"
    "PBR ì£¼ê°€ìˆœìì‚°ë¹„ìœ¨ (Price to Book Ratio): ì£¼ê°€ê°€ ì¥ë¶€ê°€ì¹˜ ëŒ€ë¹„ ì–¼ë§ˆë‚˜ ë†’ì€ì§€ë¥¼ ë‚˜íƒ€ë‚´ë©°, 1ë³´ë‹¤ ë‚®ìœ¼ë©´ ì €í‰ê°€ë¡œ í•´ì„ë˜ê¸°ë„ í•©ë‹ˆë‹¤.\n"
    "PER ì£¼ê°€ìˆ˜ìµë¹„ìœ¨ (Price to Earnings Ratio): ì´ìµ ëŒ€ë¹„ ì£¼ê°€ ìˆ˜ì¤€ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë‚®ì„ìˆ˜ë¡ ì´ìµ ëŒ€ë¹„ ì €ë ´í•œ ê¸°ì—…ì…ë‹ˆë‹¤. ë†’ì„ìˆ˜ë¡ ì‹œì¥ì˜ ê¸°ëŒ€ì¹˜ê°€ ë†’ìŠµë‹ˆë‹¤.\n"
    "ROE ìê¸°ìë³¸ì´ìµë¥  (Return on Equity): ìë³¸ì„(ë¶€ì±„ ë¯¸í¬í•¨) ì–¼ë§ˆë‚˜ íš¨ìœ¨ì ìœ¼ë¡œ ìš´ìš©í•´ ì´ìµì„ ëƒˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n"
    "ROA ì´ìì‚°ì´ìµë¥  (Return on Assets): ì´ìì‚°(ë¶€ì±„ í¬í•¨) ëŒ€ë¹„ ìˆ˜ìµë¥ ë¡œ, ë³´ìˆ˜ì ì¸ ìˆ˜ìµì„± ì§€í‘œì…ë‹ˆë‹¤.\n"
    "ICR ì´ìë³´ìƒë¹„ìœ¨ (Interest Coverage Ratio): ì˜ì—…ì´ìµìœ¼ë¡œ ì´ìë¹„ìš©ì„ ì–¼ë§ˆë‚˜ ê°ë‹¹í•  ìˆ˜ ìˆëŠ”ì§€ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n"
    "EPS ì£¼ë‹¹ìˆœì´ìµ (Earnings Per Share): ìµœê·¼ 5ë…„ê°„ 1ì£¼ë‹¹ ê¸°ì—…ì´ ì°½ì¶œí•œ ìˆœì´ìµì˜ ì„±ì¥ë¥ ë¡œ, ìˆ˜ìµì„±ê³¼ ì„±ì¥ì„± íŒë‹¨ì— ìœ ìš©í•©ë‹ˆë‹¤.\n"
    "ë°°ë‹¹ì„±ì¥ë¥ : ìµœê·¼ 10ë…„ê°„ ë°°ë‹¹ê¸ˆì˜ ì„±ì¥ë¥ ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì…ë‹ˆë‹¤.\n"
    "ì˜ì—…ì´ìµë¥ : ìµœê·¼ 5ê°œ ì˜ì—…ë…„ë„/ë¶„ê¸°ì˜ í‰ê·  ì˜ì—…ì´ìµë¥  ì„±ì¥ë¥ ë¡œ, ê¸°ì—…ì˜ ìˆ˜ìµì„± ìˆ˜ì¤€ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n"
    "ëª¨ë©˜í…€: ì£¼ê°€ì˜ ì¤‘ì¥ê¸° ìƒìŠ¹ íë¦„ì„ ë°˜ì˜í•œ ì§€í‘œë¡œ, ì£¼ê°€ì˜ íƒ„ë ¥ê³¼ ì¶”ì„¸ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.\n\n"
    "í•´ë‹¹ ë©”ì¼ì€ ë§¤ì£¼ í‰ì¼ ì˜¤í›„ 5ì‹œì— ìë™ ë°œì†¡ë˜ë©°, ì•ˆì •ì ì´ê³  í˜„ëª…í•œ íˆ¬ìë¥¼ ìœ„í•œ ì°¸ê³  ìë£Œë¡œ ì œê³µë©ë‹ˆë‹¤.\n\n"
    "ê·€í•˜ì˜ ì„±ê³µì ì¸ íˆ¬ìë¥¼ ì‘ì›í•©ë‹ˆë‹¤."
)

msg.set_content(content)
html_content = f"""
<html>
  <body>

    <p><strong>ì§€ê¸ˆ ë¬´ë£Œ êµ¬ë…í•˜ê³  AI íˆ¬ì ì¸ì‚¬ì´íŠ¸ë¥¼ ë§¤ì£¼ ë°›ì•„ë³´ì„¸ìš”:</strong> <a href="https://portfolio-production-54cf.up.railway.app/" target="_blank">êµ¬ë…í•˜ëŸ¬ ê°€ê¸°</a></p>
    
    <p>ê·€í•˜ì˜ ì¤‘ì¥ê¸° íˆ¬ì ì°¸ê³ ë¥¼ ìœ„í•´ <b>{date_kr}</b> ê¸°ì¤€, 
    ì‹œê°€ì´ì•¡ ìƒìœ„ <b>{limit}</b>ê°œ, ë‰´ìš•ì¦ê¶Œê±°ë˜ì†Œ(NYSE), ë‚˜ìŠ¤ë‹¥(NASDAQ), ì•„ë©•ìŠ¤(AMEX)ì— ìƒì¥ëœ ê¸°ì—…ë“¤ì˜ ìµœì‹  í€€íŠ¸ ë°ì´í„°ë¥¼ ì „ë‹¬ë“œë¦½ë‹ˆë‹¤.</p>

    <p>ê° ê¸°ì—…ì˜ ì´ì ìˆ˜ëŠ” ë°¸ë¥˜ì—ì´ì…˜ ì ìˆ˜, ì‹¤ì ëª¨ë©˜í…€ ì ìˆ˜, ê°€ê²©/ìˆ˜ê¸‰ ì ìˆ˜, ê·¸ë¦¬ê³  ê²½ìŸ ìš°ìœ„ì˜ ì§€ì† ê°€ëŠ¥ì„±ì„ ë°˜ì˜í•˜ì˜€ìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ì•„ë˜ í•´ì„¤ì„ ì°¸ê³ í•´ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.</p>

    <h3 style="margin-top: 30px;"><strong>{date_kr} AI ì„ ì • ì£¼ìš” ë‰´ìŠ¤ ë° ê±°ì‹œê²½ì œ ë¶„ì„</strong></h3>

    {markdown.markdown(answer)}

    <h3>ğŸ“Œ ì£¼ìš” ì¬ë¬´ì§€í‘œ í•´ì„¤</h3>
    <table border="1" cellpadding="8" cellspacing="0" style="border-collapse: collapse; font-family: sans-serif;">
      <thead style="background-color: #f2f2f2;">
        <tr>
          <th>ì§€í‘œ</th>
          <th>í•œê¸€ëª…</th>
          <th>ì„¤ëª…</th>
        </tr>
      </thead>
      <tbody>
        <tr><td><b>FCF</b></td><td>ììœ í˜„ê¸ˆíë¦„</td><td>ê¸°ì—…ì´ ì˜ì—…í™œë™ì„ í†µí•´ ë²Œì–´ë“¤ì¸ í˜„ê¸ˆì—ì„œ ì„¤ë¹„ íˆ¬ì ë“± ì‚¬ì—… ìœ ì§€ë¥¼ ìœ„í•´ ì§€ì¶œí•œ ìê¸ˆì„ ì œì™¸í•œ í›„, ì‹¤ì œë¡œ ê¸°ì—…ì´ ììœ ë¡­ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í˜„ê¸ˆì…ë‹ˆë‹¤. ì´ í˜„ê¸ˆì€ ì‹ ê·œ íˆ¬ì ë“± ë‹¤ì–‘í•œ ìš©ë„ë¡œ í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</td></tr>
        <tr><td><b>ì¶”ì •DCFë²”ìœ„</b></td><td>í• ì¸ëœ í˜„ê¸ˆíë¦„</td><td>ë¯¸ë˜ ì˜ˆìƒ ììœ í˜„ê¸ˆíë¦„(FCF)ì„ ë³´ìˆ˜ì ì¸ í• ì¸ìœ¨ë¡œ í˜„ì¬ ê°€ì¹˜ë¡œ í™˜ì‚°í•˜ì—¬ ì‚°ì¶œí•œ ê¸°ì—…ì˜ ë‚´ì¬ê°€ì¹˜ì…ë‹ˆë‹¤. ë³¸ ë‚´ì¬ê°€ì¹˜ëŠ” ëª¬í…Œì¹´ë¥¼ë¡œ ì‹œë®¬ë ˆì´ì…˜ì„ í†µí•´ ì—¬ëŸ¬ ì„±ì¥ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ê³ ë ¤í•˜ë©°, 95% ì‹ ë¢°êµ¬ê°„ ë²”ìœ„ ë‚´ì—ì„œ ë‚´ì¬ê°€ì¹˜ ë³€ë™ì„±ì„ í‰ê°€í•˜ì—¬ ê¸°ì—…ì˜ ì €í‰ê°€ ì—¬ë¶€ë¥¼ ë³´ë‹¤ ì •ë°€í•˜ê²Œ íŒë‹¨í•©ë‹ˆë‹¤.</td></tr>
        <tr><td><b>D/E</b></td><td>ë¶€ì±„ë¹„ìœ¨</td><td>ìë³¸ ëŒ€ë¹„ ë¶€ì±„ì˜ ë¹„ìœ¨ë¡œ, ì¬ë¬´ ê±´ì „ì„±ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë‚®ì„ìˆ˜ë¡ ì•ˆì •ì ì…ë‹ˆë‹¤.</td></tr>
        <tr><td><b>CR</b></td><td>ìœ ë™ë¹„ìœ¨</td><td>ìœ ë™ìì‚°ì´ ìœ ë™ë¶€ì±„ë¥¼ ì–¼ë§ˆë‚˜ ì»¤ë²„í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.</td></tr>
        <tr><td><b>PBR</b></td><td>ì£¼ê°€ìˆœìì‚°ë¹„ìœ¨</td><td>ì£¼ê°€ê°€ ì¥ë¶€ê°€ì¹˜ ëŒ€ë¹„ ì–¼ë§ˆë‚˜ ë†’ì€ì§€ë¥¼ ë‚˜íƒ€ë‚´ë©°, 1ë³´ë‹¤ ë‚®ìœ¼ë©´ ì €í‰ê°€ë¡œ í•´ì„ë˜ê¸°ë„ í•©ë‹ˆë‹¤.</td></tr>
        <tr><td><b>PER</b></td><td>ì£¼ê°€ìˆ˜ìµë¹„ìœ¨</td><td>ì´ìµ ëŒ€ë¹„ ì£¼ê°€ ìˆ˜ì¤€ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë‚®ì„ìˆ˜ë¡ ì´ìµ ëŒ€ë¹„ ì €ë ´í•œ ê¸°ì—…ì…ë‹ˆë‹¤. ë†’ì„ìˆ˜ë¡ ì‹œì¥ì˜ ê¸°ëŒ€ì¹˜ê°€ ë†’ìŠµë‹ˆë‹¤.</td></tr>
        <tr><td><b>ROE</b></td><td>ìê¸°ìë³¸ì´ìµë¥ </td><td>ìë³¸ì„(ë¶€ì±„ ë¯¸í¬í•¨) ì–¼ë§ˆë‚˜ íš¨ìœ¨ì ìœ¼ë¡œ ìš´ìš©í•´ ì´ìµì„ ëƒˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.</td></tr>
        <tr><td><b>ROA</b></td><td>ì´ìì‚°ì´ìµë¥ </td><td>ì´ìì‚°(ë¶€ì±„ í¬í•¨) ëŒ€ë¹„ ìˆ˜ìµë¥ ë¡œ, ë³´ìˆ˜ì ì¸ ìˆ˜ìµì„± ì§€í‘œì…ë‹ˆë‹¤.</td></tr>
        <tr><td><b>ICR</b></td><td>ì´ìë³´ìƒë¹„ìœ¨</td><td>ì˜ì—…ì´ìµìœ¼ë¡œ ì´ìë¹„ìš©ì„ ì–¼ë§ˆë‚˜ ê°ë‹¹í•  ìˆ˜ ìˆëŠ”ì§€ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.</td></tr>
        <tr><td><b>FCFìˆ˜ìµë¥ </b></td><td>-</td><td>ììœ í˜„ê¸ˆíë¦„(FCF)ì„ ì‹œê°€ì´ì•¡ìœ¼ë¡œ ë‚˜ëˆˆ ë¹„ìœ¨ë¡œ, ì´ ë¹„ìœ¨ì´ ë†’ì„ìˆ˜ë¡ ê¸°ì—…ì´ ì°½ì¶œí•˜ëŠ” í˜„ê¸ˆ ëŒ€ë¹„ ì£¼ê°€ê°€ ì €í‰ê°€ë˜ì—ˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.</td></tr>
        <tr><td><b>FCFì„±ì¥ë¥ </b></td><td>-</td><td>ìµœê·¼ 5ë…„ê°„ ììœ í˜„ê¸ˆíë¦„ì˜ ì„±ì¥ë¥ ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì…ë‹ˆë‹¤.</td></tr>
        <tr><td><b>EPS</b></td><td>ì£¼ë‹¹ìˆœì´ìµ</td><td>ìµœê·¼ 5ë…„ê°„ 1ì£¼ë‹¹ ê¸°ì—…ì´ ì°½ì¶œí•œ ìˆœì´ìµì˜ ì„±ì¥ë¥ ë¡œ, ìˆ˜ìµì„±ê³¼ ì„±ì¥ì„± íŒë‹¨ì— ìœ ìš©í•©ë‹ˆë‹¤.</td></tr>
        <tr><td><b>ë°°ë‹¹ì„±ì¥ë¥ </b></td><td>-</td><td>ìµœê·¼ 10ë…„ê°„ ë°°ë‹¹ê¸ˆì˜ ì„±ì¥ë¥ ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì…ë‹ˆë‹¤.</td></tr>
        <tr><td><b>ì˜ì—…ì´ìµë¥ </b></td><td>-</td><td>ìµœê·¼ 4ê°œ ì˜ì—…ë…„ë„/ë¶„ê¸°ì˜ í‰ê·  ì˜ì—…ì´ìµë¥  ì„±ì¥ë¥ ë¡œ, ê¸°ì—…ì˜ ìˆ˜ìµì„± ìˆ˜ì¤€ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.</td></tr>
        <tr><td><b>ëª¨ë©˜í…€</b></td><td>-</td><td>ì£¼ê°€ì˜ ì¤‘ì¥ê¸° ìƒìŠ¹ íë¦„ì„ ë°˜ì˜í•œ ì§€í‘œë¡œ, ì£¼ê°€ì˜ íƒ„ë ¥ê³¼ ì¶”ì„¸ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.</td></tr>
        <tr><td><b>ESG</b></td><td>-</td><td>ê¸°ì—…ì˜ ì§€ì†ê°€ëŠ¥ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œë¡œ, ë™ì¢…ì—…ê³„ ëŒ€ë¹„ ìˆ˜ì¤€ê³¼ í•¨ê»˜ í‰ê°€í•©ë‹ˆë‹¤.</td></tr>
        <tr><td><b>CVaR</b></td><td>ì¡°ê±´ë¶€ ìœ„í—˜ê°€ì¹˜</td><td>í¬íŠ¸í´ë¦¬ì˜¤ê°€ ê·¹ë‹¨ì ì¸ ì†ì‹¤ì„ ê²ªì„ ê²½ìš°, ì†ì‹¤ì´ ë°œìƒí•˜ëŠ” ìµœì•… 5% êµ¬ê°„ ë‚´ì—ì„œ í‰ê· ì ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ì†ì‹¤ì´ ë°œìƒí•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì…ë‹ˆë‹¤.</td></tr>
        <tr><td><b>Sortino Ratio</b></td><td>ì†Œí‹°ë…¸ ì§€ìˆ˜</td><td>ìˆ˜ìµë¥  ëŒ€ë¹„ í•˜ë°© ìœ„í—˜(ì†ì‹¤ ë³€ë™ì„±)ì„ ê³ ë ¤í•œ ìœ„í—˜ ì¡°ì • ìˆ˜ìµë¥ ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ê°’ì´ ë†’ì„ìˆ˜ë¡ í•˜ë°© ìœ„í—˜ ëŒ€ë¹„ ìˆ˜ìµë¥ ì´ ìš°ìˆ˜í•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.</td></tr>
        <tr><td><b>Variance</b></td><td>ë¶„ì‚°</td><td>í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥ ì˜ ë³€ë™ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œë¡œ, ìœ„í—˜ ìˆ˜ì¤€ í‰ê°€ì— ì‚¬ìš©ë©ë‹ˆë‹¤. ê°’ì´ ë‚®ì„ìˆ˜ë¡ ì•ˆì •ì ì¸ í¬íŠ¸í´ë¦¬ì˜¤ì„ì„ ëœ»í•©ë‹ˆë‹¤.</td></tr>
        <tr><td><b>Sharpe Ratio</b></td><td>ìƒ¤í”„ ì§€ìˆ˜</td><td>í¬íŠ¸í´ë¦¬ì˜¤ì˜ ì´ˆê³¼ ìˆ˜ìµë¥ ì„ í‘œì¤€í¸ì°¨ë¡œ ë‚˜ëˆˆ ì§€í‘œë¡œ, ìœ„í—˜ ëŒ€ë¹„ ìˆ˜ìµë¥ ì„ í‰ê°€í•©ë‹ˆë‹¤. ê°’ì´ í´ìˆ˜ë¡ íš¨ìœ¨ì ì¸ íˆ¬ìì„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.</td></tr>
        <tr><td><b>Sentiment Score</b></td><td>ê°ì„± ì ìˆ˜</td><td>í…ìŠ¤íŠ¸ì˜ ê¸ì • ë˜ëŠ” ë¶€ì • ì •ë„ë¥¼ ìˆ˜ì¹˜í™”í•œ ì§€í‘œë¡œ, íˆ¬ì ì‹¬ë¦¬ë‚˜ ë‰´ìŠ¤ ë°˜ì‘ì„ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤. ê°’ì´ ë†’ì„ìˆ˜ë¡ ê¸ì •ì ì¸ ì •ì„œì„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.</td></tr>
      </tbody>
    </table>

    <p style="margin-top: 20px; font-size: 14px; color: #444;">
    ë³¸ ìë£ŒëŠ” <strong>ì›ŒëŸ° ë²„í•ì˜ 'ê°€ì¹˜íˆ¬ì'</strong> ì² í•™ì„ ê¸°ë°˜ìœ¼ë¡œ,<br>
    ê¸°ì—…ì˜ ì¬ë¬´ ê±´ì „ì„±ê³¼ ì£¼ê°€ì˜ ì¶”ì„¸ë¥¼ ìˆ˜ì¹˜í™”í•˜ì—¬ í‰ê°€í•œ ê²°ê³¼ì…ë‹ˆë‹¤.<br>
    ë³¸ ìë£ŒëŠ” ì •ë³´ ì œê³µ ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš©ë˜ë©°, íˆ¬ì ì†ì‹¤ì— ëŒ€í•œ ë²•ì  ì±…ì„ì€ ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.
    </p>

    <p><em>í•´ë‹¹ ë©”ì¼ì€ ë§¤ì£¼ í™”, í†  ì˜¤ì „ 8ì‹œì— ìë™ ë°œì†¡ë©ë‹ˆë‹¤.</em></p>
  </body>
</html>
"""

msg.add_alternative(html_content, subtype="html")

with open(excel_path, "rb") as f:
    msg.add_attachment(
        f.read(),
        maintype="application",
        subtype="vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        filename="DeepFund_Weekly_Insights.xlsx",
    )

with smtplib.SMTP_SSL("smtp.gmail.com", 465) as smtp:
    smtp.login(EMAIL, PASSWORD)
    smtp.send_message(
        msg, to_addrs=recipients
    )  # send_message's to_addrs param controls actual recipients

# SPDX-FileCopyrightText: ¬© 2025 Hyungsuk Choi <chs_3411@naver[dot]com>, University of Maryland
# SPDX-License-Identifier: MIT

import yfinance as yf
import pandas as pd
import numpy as np
import requests
import string
import datetime as dt
import openpyxl
import math
from queue import Queue
import threading
import time
import polars as pl

# import shelve
from bs4 import BeautifulSoup
from urllib.request import urlopen
import smtplib
from email.message import EmailMessage
from email.headerregistry import Address
import os
import ta  # Í∏∞Ïà†Ï†Å ÏßÄÌëú Í≥ÑÏÇ∞ ÎùºÏù¥Î∏åÎü¨Î¶¨
import re
import matplotlib.pyplot as plt
from scipy.optimize import minimize
from scipy.stats import norm
from scipy.stats import skew, kurtosis
from scipy.stats.mstats import gmean
from scipy.stats import linregress
from datetime import datetime, timedelta, date
from google import genai
from google.genai import types
import json
import markdown

# ÌòÑÏû¨ ÌååÏùº (src/buffett_us.py) Í∏∞Ï§ÄÏúºÎ°ú ÌîÑÎ°úÏ†ùÌä∏ Î£®Ìä∏ Í≤ΩÎ°ú
project_root = os.path.dirname(os.path.dirname(__file__))
# backend Í≤ΩÎ°úÎ°ú ÏóëÏÖÄ Ï†ÄÏû•
excel_path = os.path.join(project_root, "backend", "deep_fund.xlsx")

################ DEPENDENCIES ###########################

# pip install -r requirements.txt

#########################################################
recipients = ["chs_3411@naver.com", "eljm2080@gmail.com", "hyungsukchoi3411@gmail.com"]

# JSONÏóêÏÑú Ïù¥Î©îÏùº Î∂àÎü¨Ïò§Í∏∞

try:
    recipients_json_path = os.path.join(project_root, "backend", "recipients.json")
    with open(recipients_json_path, "r") as f:
        loaded_emails = json.load(f)
        for email in loaded_emails:
            if email not in recipients:
                recipients.append(email)  # append ÌòïÌÉúÎ°ú Ï∂îÍ∞Ä
except (FileNotFoundError, json.JSONDecodeError):
    print("‚ö†Ô∏è recipients.json ÌååÏùºÏù¥ ÏóÜÍ±∞ÎÇò ÏûòÎ™ªÎêòÏóàÏäµÎãàÎã§.")

recipients = list(set(recipients))

################ PREDETERMINED FIELDS ###################

EMAIL = os.environ["EMAIL_ADDRESS"]
PASSWORD = os.environ["EMAIL_PASSWORD"]
fmp_key = os.environ["FMP_API_KEY"]
marketaux_api = os.environ["MARKETAUX_API"]
NUM_THREADS = 2  # multithreading

country = "US"
limit = 200  # max 250 requests/day #
sp500 = True

# top X tickers to optimize
opt = 20

# for news
news_lookup = 100

# for moat
moat_limit = 200
#########################################################


##########################################################################################################
# Initialize the client (picks up your API key automatically from env vars, or pass api_key explicitly)
client = genai.Client(api_key=os.environ["GEMINI_API_KEY"])


# Define the grounding tool
grounding_tool = types.Tool(google_search=types.GoogleSearch())

# Configure generation settings
config = types.GenerateContentConfig(tools=[grounding_tool])

##########################################################################################################


# print('May take up to few minutes...')

today = dt.datetime.today().weekday()
weekend = today - 4  # returns 1 for saturday, 2 for sunday
formattedDate = (
    (dt.datetime.today() - dt.timedelta(days=weekend)).strftime("%Y%m%d")
    if today >= 5
    else dt.datetime.today().strftime("%Y%m%d")
)

three_months_approx = dt.datetime.today() - dt.timedelta(days=90)
formattedDate_3m_ago = three_months_approx.strftime("%Y%m%d")

date_kr = dt.datetime.strptime(formattedDate, "%Y%m%d").strftime("%-mÏõî %-dÏùº")
date_kr_month = dt.datetime.strptime(formattedDate, "%Y%m%d").strftime("%-mÏõî")
date_kr_ymd = dt.datetime.strptime(formattedDate, "%Y%m%d").strftime(
    "%YÎÖÑ %-mÏõî %-dÏùº"
)  # Unix

esg_dict = {
    "LAG_PERF": "ÎØ∏Ìù°",
    "AVG_PERF": "Î≥¥ÌÜµ",
    "LEAD_PERF": "Ïö∞Ïàò",
}

data = []
data_lock = threading.Lock()


def get_tickers(country: str, limit: int, sp500: bool):
    if country is not None:
        return get_tickers_by_country(country, limit, fmp_key)  # US, JP, KR
    elif sp500:
        return pl.read_csv(
            "https://datahub.io/core/s-and-p-500-companies/r/constituents.csv"
        )["Symbol"].to_list()
    elif not sp500:
        nasdaq100_url = "https://en.wikipedia.org/wiki/NASDAQ-100"
        nasdaq100 = pd.read_html(nasdaq100_url, header=0)[
            4
        ]  # Might need to adjust index (5th table on the page)
        return nasdaq100["Ticker"].tolist()
    else:
        raise Exception("No tickers list satisfies the given parameter")


def get_tickers_by_country(country: str, limit: int, apikey: str):
    url = "https://financialmodelingprep.com/api/v3/stock-screener"
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    }

    exchanges = ["nyse", "nasdaq", "amex"]
    all_stocks = []

    try:
        for exchange in exchanges:
            params = {
                "exchange": exchange,
                "limit": 500,  # ÎÑâÎÑâÌûà Í∞ÄÏ†∏Ïò§Í∏∞
                "type": "stock",
                "isEtf": False,
                "isFund": False,
                "apikey": apikey,
            }
            r = requests.get(url, headers=headers, params=params)
            r.raise_for_status()
            data = r.json()
            all_stocks.extend(data)

        # ÏãúÍ∞ÄÏ¥ùÏï° Í∏∞Ï§Ä Ï†ïÎ†¨
        sorted_stocks = sorted(
            all_stocks, key=lambda x: x.get("marketCap", 0), reverse=True
        )

        # Ï§ëÎ≥µ Ï†úÍ±∞ Î∞è ÏÉÅÏúÑ limitÎßå Ï∂îÏ∂ú
        seen = set()
        unique_sorted = []
        for stock in sorted_stocks:
            symbol = stock.get("symbol")
            if symbol and symbol not in seen:
                seen.add(symbol)
                unique_sorted.append(symbol)
            if len(unique_sorted) >= limit:
                break

        return unique_sorted

    except Exception as e:
        print(f"Error: {e}")
        return []


def safe_check(val):
    return val is not None and not (isinstance(val, float) and np.isnan(val))


def quant_style_score(
    price_vs_fair_upper=None,
    price_vs_fair_lower=None,
    fcf_yield_rank=None,
    fcf_vs_treasury_spread=None,
    per=None,
    per_rank=None,
    pbr_rank=None,
    de=None,
    cr=None,
    industry_per=None,
    roe_z=None,
    roa_z=None,
    roe=None,
    roa=None,
    icr=None,
    fcf_cagr_rank=None,
    eps_cagr_rank=None,
    div_cagr_rank=None,
    eps=None,
    div_yield=None,
    opinc_yoy=None,
    opinc_qoq=None,
    industry_roe=None,
    industry_roa=None,
):
    valuation_score = 0
    earnings_momentum_score = 0

    # === Valuation (30Ï†ê) ===
    if safe_check(price_vs_fair_upper) and price_vs_fair_upper > 0:
        valuation_score += min(price_vs_fair_upper * 5, 1.5)  # DCF 5%
    if safe_check(price_vs_fair_lower) and price_vs_fair_lower > 0:
        valuation_score += min(price_vs_fair_lower * 6, 1.5)  # Î≥¥ÏàòÏ†Å DCF
    if safe_check(fcf_vs_treasury_spread):
        if fcf_vs_treasury_spread > 0:
            valuation_score += min(fcf_vs_treasury_spread * 10, 1.5)  # FCF spread
        else:
            valuation_score -= 0.5
    if safe_check(fcf_yield_rank):
        valuation_score += fcf_yield_rank * 1.5  # FCF ÏàòÏùµÎ•† 5%
    if safe_check(per_rank):
        valuation_score += (1 - per_rank) * 3.0  # PER/EY 10%
    if safe_check(per) and safe_check(industry_per):
        if per < industry_per * 0.7:
            valuation_score += 0.5
        elif per > industry_per * 1.3:
            valuation_score -= 0.5
    if safe_check(pbr_rank):
        valuation_score += (1 - pbr_rank) * 1.5  # PBR 5%
    if safe_check(de):
        if 0 < de <= 0.5:
            valuation_score += 0.75
        elif de > 1.0:
            valuation_score -= 0.5
    if safe_check(cr):
        if 1.5 <= cr <= 2.5:
            valuation_score += 0.75
        elif cr < 1.0:
            valuation_score -= 0.5

    # === Fundamental Momentum (20Ï†ê) ===
    if safe_check(roe_z):
        earnings_momentum_score += min(max(roe_z, -2), 2) * 1.25  # ROE Í∞úÏÑ†
    if safe_check(roa_z):
        earnings_momentum_score += min(max(roa_z, -2), 2) * 1.25  # ROA Í∞úÏÑ†
    if not safe_check(roe_z) and safe_check(roe) and safe_check(industry_roe):
        if roe > industry_roe:
            earnings_momentum_score += 0.5
    if not safe_check(roa_z) and safe_check(roa) and safe_check(industry_roa):
        if roa > industry_roa:
            earnings_momentum_score += 0.5
    if safe_check(icr):
        if icr >= 10:
            earnings_momentum_score += 1
        elif icr >= 5:
            earnings_momentum_score += 0.5
        elif icr < 1:
            earnings_momentum_score -= 0.5
    if safe_check(fcf_cagr_rank):
        earnings_momentum_score += fcf_cagr_rank * 1.5
    if safe_check(eps_cagr_rank):
        earnings_momentum_score += eps_cagr_rank * 1.5
    if safe_check(div_cagr_rank):
        earnings_momentum_score += div_cagr_rank * 1.0
    if safe_check(eps):
        if eps >= 0.3:
            earnings_momentum_score += 1.5
        elif eps >= 0.1:
            earnings_momentum_score += 0.75
        elif eps < 0:
            earnings_momentum_score -= 1
    if safe_check(div_yield):
        if div_yield >= 0.1:
            earnings_momentum_score += 0.75
        elif div_yield >= 0.08:
            earnings_momentum_score += 0.5
        elif div_yield >= 0.06:
            earnings_momentum_score += 0.25
        elif div_yield < 0.02:
            earnings_momentum_score -= 0.5
    if safe_check(opinc_yoy):
        if opinc_yoy > 0.2:
            earnings_momentum_score += 1.5
        elif opinc_yoy > 0.05:
            earnings_momentum_score += 1
        elif opinc_yoy < 0:
            earnings_momentum_score -= 1
    if safe_check(opinc_qoq):
        if opinc_qoq > 0.1:
            earnings_momentum_score += 1
        elif opinc_qoq > 0:
            earnings_momentum_score += 0.5
        elif opinc_qoq < 0:
            earnings_momentum_score -= 0.5

    return round(valuation_score, 2), round(earnings_momentum_score, 2)


def get_fcf_yield_and_cagr(ticker, yf_ticker, api_key="YOUR_API_KEY"):
    def try_fmp(ticker, api_key):
        try:
            # 1. Market cap
            profile_url = f"https://financialmodelingprep.com/api/v3/profile/{ticker}?apikey={api_key}"
            profile_resp = requests.get(profile_url)
            if profile_resp.status_code != 200 or not profile_resp.json():
                return (None, None, [])

            market_cap = profile_resp.json()[0].get("mktCap")
            if market_cap is None or market_cap == 0:
                return (None, None, [])

            # 2. FCF list
            url = f"https://financialmodelingprep.com/api/v3/cash-flow-statement/{ticker}?limit=10&apikey={api_key}"
            response = requests.get(url)
            if response.status_code != 200 or not response.json():
                return (None, None, [])

            data = response.json()
            fcf_list = [
                item["freeCashFlow"]
                for item in data
                if item.get("freeCashFlow") is not None
            ]
            fcf_list = fcf_list[::-1]

            if len(fcf_list) < 2:
                return (None, None, fcf_list)

            # 3. FCF Yield
            latest_fcf = fcf_list[-1]
            fcf_yield = round((latest_fcf / market_cap) * 100, 2) if latest_fcf else 0.0

            # 4. FCF CAGR
            initial_fcf, final_fcf = fcf_list[0], fcf_list[-1]
            n_years = len(fcf_list) - 1
            if initial_fcf <= 0 or final_fcf <= 0:
                fcf_cagr = None
            else:
                fcf_cagr = round(
                    ((final_fcf / initial_fcf) ** (1 / n_years) - 1) * 100, 2
                )

            return (fcf_yield, fcf_cagr, fcf_list)

        except Exception:
            return (None, None, [])

    def try_yf(ticker):
        try:

            market_cap = ticker.info.get("marketCap")
            if market_cap is None or market_cap == 0:
                return (None, None, [])

            cashflow_df = ticker.cashflow
            if cashflow_df.empty or "Free Cash Flow" not in cashflow_df.index:
                return (None, None, [])

            fcf_series = cashflow_df.loc["Free Cash Flow"].dropna()[::-1]
            if len(fcf_series) < 2:
                return (None, None, fcf_series.tolist())

            fcf_list = fcf_series.tolist()
            latest_fcf = fcf_series.iloc[-1]
            fcf_yield = (
                round((latest_fcf / market_cap) * 100, 2) if latest_fcf else None
            )

            initial_fcf, final_fcf = fcf_series.iloc[0], fcf_series.iloc[-1]
            n_years = len(fcf_series) - 1
            if initial_fcf <= 0 or final_fcf <= 0:
                fcf_cagr = None
            else:
                fcf_cagr = round(
                    ((final_fcf / initial_fcf) ** (1 / n_years) - 1) * 100, 2
                )

            return (fcf_yield, fcf_cagr, fcf_list)

        except Exception:
            return (None, None, [])

    result = try_yf(yf_ticker)
    if result is not None and (
        result[0] is not None
        or result[1] is not None
        or (result[2] and len(result[2]) > 0)
    ):
        return result

    result = try_fmp(ticker, api_key)
    # FMP Í≤∞Í≥ºÍ∞Ä Î™®Îëê None/Îπà Î¶¨Ïä§Ìä∏Î©¥ yfinance ÏãúÎèÑ
    if result is not None and (
        result[0] is not None
        or result[1] is not None
        or (result[2] and len(result[2]) > 0)
    ):
        return result

    return (None, None, [])  # Always return a tuple


def get_10yr_treasury_yield():
    try:
        tnx = yf.Ticker("^TNX")
        tnx_data = tnx.history(period="1d")
        latest_yield = tnx_data["Close"].iloc[-1]
        return round(latest_yield, 2)  # Already in percent
    except Exception as e:
        return f"Error fetching yield: {e}"


def dcf_valuation(
    fcf_history,
    discount_rate,
    long_term_growth,
    years=10,
    shares_outstanding=None,
    cagr=0.05,
):

    if not fcf_history or len(fcf_history) < 2:
        return (None, None)

    start_fcf = fcf_history[0]  # oldest
    end_fcf = fcf_history[-1]  # most recent

    if start_fcf <= 0 or end_fcf <= 0:
        return (None, None)

    if discount_rate <= long_term_growth:
        return (None, None)  # Invalid terminal growth assumption

    if cagr is None or cagr <= 0:
        return (None, None)  # Invalid CAGR assumption
    est_cagr = cagr / 100.0 * 0.6  # conservative
    growth_rate = min(est_cagr, discount_rate)  # Ensure growth rate is reasonable

    # Project FCFs
    projected_fcfs = [end_fcf * (1 + growth_rate) ** i for i in range(1, years + 1)]

    # Terminal Value
    terminal_value = (
        projected_fcfs[-1] * (1 + long_term_growth) / (discount_rate - long_term_growth)
    )

    # Discounting
    discounted_fcfs = [
        fcf / ((1 + discount_rate) ** i) for i, fcf in enumerate(projected_fcfs, 1)
    ]
    discounted_terminal_value = terminal_value / ((1 + discount_rate) ** years)

    enterprise_value = sum(discounted_fcfs) + discounted_terminal_value

    if shares_outstanding is None or shares_outstanding <= 0:
        return (None, None)

    intrinsic_value = enterprise_value / shares_outstanding

    return (
        float(intrinsic_value),
        growth_rate * 100,
    )  # Return intrinsic value and growth rate in percentage


def get_trading_volume_vs_avg20(ticker_symbol: str) -> float:
    try:
        # Fetch 21 days of data
        ticker = yf.Ticker(ticker_symbol)
        hist = ticker.history(period="21d")

        if len(hist) < 21:
            return None  # Not enough data

        today_volume = hist["Volume"].iloc[-1]
        avg_volume_20 = hist["Volume"].iloc[:-1].mean()

        if avg_volume_20 == 0 or pd.isna(today_volume) or pd.isna(avg_volume_20):
            return None  # Invalid data

        # Return ratio (e.g., 1.5 means 150% of avg volume)
        return round(today_volume / avg_volume_20, 1)

    except Exception as e:
        print(f"[Error] {ticker_symbol}: {e}")
        return None


def has_stable_dividend_growth_cagr(ticker):
    try:
        stock = ticker
        divs = stock.dividends

        if divs.empty:
            return None

        # Sum dividends annually
        annual_divs = divs.groupby(divs.index.year).sum()

        # Need at least 10 full years
        if len(annual_divs) < 10:
            return None

        # Get last full year (last year completed)
        last_year = dt.datetime.today().year - 1

        # Ensure we have dividends data up to last_year
        if last_year not in annual_divs.index:
            return None

        # Select exactly 10 years ending at last_year
        recent_years = sorted(annual_divs.index)
        recent_years = [
            year for year in recent_years if last_year - 9 <= year <= last_year
        ]

        if len(recent_years) < 10:
            return None

        last_10_divs = [annual_divs[year] for year in recent_years]

        div_start = last_10_divs[0]
        div_end = last_10_divs[-1]

        # Validate data
        if div_start <= 0 or div_end <= 0:
            return None

        periods = len(last_10_divs) - 1  # 9 periods for 10 years

        cagr = (div_end / div_start) ** (1 / periods) - 1

        return cagr  # returns float (e.g., 0.05 for 5%)

    except Exception:
        return None


def compute_eps_growth_slope(ticker):
    try:
        income_stmt = ticker.financials  # Annual financials DataFrame

        if "Diluted EPS" not in income_stmt.index:
            return None

        eps_series = income_stmt.loc["Diluted EPS"].dropna()
        eps_series = eps_series.sort_index()  # Oldest to newest

        # Keep last 5 years
        current_year = dt.datetime.today().year
        eps_series = eps_series[
            [col for col in eps_series.index if col.year >= current_year - 5]
        ]

        if len(eps_series) < 2:
            return None

        eps_list = eps_series.tolist()

        x = list(range(len(eps_list)))
        slope, _, _, _, _ = linregress(x, eps_list)
        return slope

    except Exception:
        return None


# gets the most recent interest coverage ratio available
def get_interest_coverage_ratio(ticker):
    financials = (
        ticker.financials
    )  # Annual financials, columns = dates (most recent first)
    ratio = None
    if not financials.columns.empty:
        for date in financials.columns:
            if date.year < dt.datetime.today().year - 5:  # sift out old data
                return None

            try:
                ebit = financials.loc["Operating Income", date]
                interest_expense = financials.loc["Interest Expense", date]
                if (
                    math.isnan(interest_expense)
                    or math.isnan(ebit)
                    or not interest_expense
                    or ebit is None
                    or interest_expense is None
                ):
                    continue  # Avoid division by zero
                else:
                    ratio = round((ebit / abs(interest_expense)), 2)
                    break
            except KeyError:
                continue
        return ratio
    else:
        return None


### 1mo ver.
def get_percentage_change(ticker):
    ticker_obj = yf.Ticker(ticker)

    # Get last 1 month of price data
    data = ticker_obj.history(period="1mo")

    if len(data) >= 2:
        start_close = data["Close"].iloc[0]
        end_close = data["Close"].iloc[-1]

        if start_close != 0:
            percent_change = ((end_close - start_close) / start_close) * 100
            sign = "+" if percent_change >= 0 else ""
            return f" ({sign}{percent_change:.2f}%)"
        else:
            return " ()"
    else:
        return " ()"


def get_percentage_change_ttm(ticker):
    ticker_obj = yf.Ticker(ticker)
    today = date.today()
    start_of_month = today.replace(day=1)

    # Fetch price data from start of month to today

    data = ticker_obj.history(start=start_of_month, end=today + timedelta(days=1))
    print(data)

    if len(data) >= 2:
        # Use the first available trading day as the start
        start_close = data["Close"].iloc[0]
        end_close = data["Close"].iloc[-1]

        if start_close and not pd.isna(start_close):
            percent_change = ((end_close - start_close) / start_close) * 100
            sign = "+" if percent_change >= 0 else ""
            return f" ({sign}{percent_change:.2f}%)"
        else:
            return " ()"
    else:
        return " ()"


def download_industry_per():
    # FullRatioÏùò ÏÇ∞ÏóÖÎ≥Ñ PER ÌéòÏù¥ÏßÄ URL
    url = "https://fullratio.com/pe-ratio-by-industry"
    headers = {"User-Agent": "Mozilla/5.0"}

    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, "html.parser")

    # ÌÖåÏù¥Î∏î Ï∞æÍ∏∞ (Ïù¥Îïå tableÏù¥ NoneÏù∏ÏßÄ Ï≤¥ÌÅ¨)
    table = soup.find("table")
    if table is None:
        raise Exception(
            "ÌÖåÏù¥Î∏îÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. Íµ¨Ï°∞Í∞Ä Î∞îÎÄåÏóàÍ±∞ÎÇò JSÎ°ú Î°úÎî©Îê† Ïàò ÏûàÏäµÎãàÎã§."
        )

    # tbodyÍ∞Ä ÏûàÎäî Í≤ΩÏö∞
    tbody = table.find("tbody")
    if tbody:
        rows = tbody.find_all("tr")
    else:
        rows = table.find_all("tr")[1:]  # Ìó§Îçî Ï†úÏô∏

    # Í∞Å ÌñâÏóêÏÑú Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú
    per_data = []
    for row in rows:
        cols = row.find_all("td")
        if len(cols) >= 2:
            industry = cols[0].text.strip()
            pe_ratio = cols[1].text.strip()
            per_data.append({"Industry": industry, "P/E Ratio": pe_ratio})

    # Í≤∞Í≥º Ï∂úÎ†•
    return pl.DataFrame(per_data)


def download_industry_roe():
    url_roe = "https://fullratio.com/roe-by-industry"
    headers_roe = {"User-Agent": "Mozilla/5.0"}

    response_roe = requests.get(url_roe, headers=headers_roe)
    soup_roe = BeautifulSoup(response_roe.text, "html.parser")

    # ÌÖåÏù¥Î∏î Ï∞æÍ∏∞ (Ïù¥Îïå tableÏù¥ NoneÏù∏ÏßÄ Ï≤¥ÌÅ¨)
    table_roe = soup_roe.find("table")
    if table_roe is None:
        raise Exception(
            "ÌÖåÏù¥Î∏îÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. Íµ¨Ï°∞Í∞Ä Î∞îÎÄåÏóàÍ±∞ÎÇò JSÎ°ú Î°úÎî©Îê† Ïàò ÏûàÏäµÎãàÎã§."
        )

    # tbodyÍ∞Ä ÏûàÎäî Í≤ΩÏö∞
    tbody_roe = table_roe.find("tbody")
    if tbody_roe:
        rows_roe = tbody_roe.find_all("tr")
    else:
        rows_roe = table_roe.find_all("tr")[1:]  # Ìó§Îçî Ï†úÏô∏

    # Í∞Å ÌñâÏóêÏÑú Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú
    roe_data = []
    for row in rows_roe:
        cols_roe = row.find_all("td")
        if len(cols_roe) >= 2:
            industry_roe = cols_roe[0].text.strip()
            roe_num = cols_roe[1].text.strip()
            roe_data.append({"Industry": industry_roe, "ROE": roe_num})

    # Í≤∞Í≥º Ï∂úÎ†•
    return pl.DataFrame(roe_data)


def download_industry_roa():
    url_roa = "https://fullratio.com/roa-by-industry"
    headers_roa = {"User-Agent": "Mozilla/5.0"}

    response_roa = requests.get(url_roa, headers=headers_roa)
    soup_roa = BeautifulSoup(response_roa.text, "html.parser")

    # ÌÖåÏù¥Î∏î Ï∞æÍ∏∞ (Ïù¥Îïå tableÏù¥ NoneÏù∏ÏßÄ Ï≤¥ÌÅ¨)
    table_roa = soup_roa.find("table")
    if table_roa is None:
        raise Exception(
            "ÌÖåÏù¥Î∏îÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. Íµ¨Ï°∞Í∞Ä Î∞îÎÄåÏóàÍ±∞ÎÇò JSÎ°ú Î°úÎî©Îê† Ïàò ÏûàÏäµÎãàÎã§."
        )

    # tbodyÍ∞Ä ÏûàÎäî Í≤ΩÏö∞
    tbody_roa = table_roa.find("tbody")
    if tbody_roa:
        rows_roa = tbody_roa.find_all("tr")
    else:
        rows_roa = table_roa.find_all("tr")[1:]  # Ìó§Îçî Ï†úÏô∏

    # Í∞Å ÌñâÏóêÏÑú Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú
    roa_data = []
    for row in rows_roa:
        cols_roa = row.find_all("td")
        if len(cols_roa) >= 2:
            industry_roa = cols_roa[0].text.strip()
            roa_num = cols_roa[1].text.strip()
            roa_data.append({"Industry": industry_roa, "ROA": roa_num})

    df_roa = pl.DataFrame(roa_data)
    return pl.DataFrame(roa_data)


#
df_per = download_industry_per()
df_roe = download_industry_roe()
df_roa = download_industry_roa()


def get_industry_per(ind):
    try:
        if ind is not None:
            ans = float(
                df_per.filter(pl.col("Industry") == ind).select("P/E Ratio").item()
            )
            return ans
        return per
    except Exception:
        spy = yf.Ticker("SPY")
        spy_info = spy.info
        per = spy_info.get("trailingPE")
        return per


def get_industry_roe(ind):
    try:
        if ind is not None:
            ans = float(df_roe.filter(pl.col("Industry") == ind).select("ROE").item())
            return ans / 100.0
        else:
            return 0.08
    except Exception:
        return 0.08


def get_industry_roa(ind):
    try:
        if ind is not None:
            ans = float(df_roa.filter(pl.col("Industry") == ind).select("ROA").item())
            return ans / 100.0
        return 0.06
    except Exception:
        return 0.06


######## LOAD TICKERS ###########
raw_tickers = get_tickers(country, limit, sp500)

prohibited = {
    "NVL",
    "SOJE",
    "RY-PT",
    "CEO",
    "DUKB",
    "ATVI",
    "SQ",
    "AED",
    "BACRP",
    "BDXA",
    "VZA",
    "AMOV",
    "ANTM",
    "SOJC",
    "ACH",
    "TBC",
    "PXD",
    "TBB",
    "SOJD",
    "AFA",
    "AEH",
}


# no data on yfinance or frequently cause errors
def keep_ticker(t):
    return t not in prohibited


tickers = list(filter(keep_ticker, raw_tickers))
################################################################################
from yf_cache_downloader import get_tickers_by_country_cache, update_cache

# Ïòà) Ìã∞Ïª§ Î¶¨Ïä§Ìä∏ Î∞õÏïÑÏò§Í∏∞ (limit, api_keyÎäî yf_cache_downloader.py ÎÇ¥Î∂Ä ÎòêÎäî Ïô∏Î∂ÄÏóêÏÑú ÏÑ§Ï†ï Í∞ÄÎä•)
tickers_for_cache = get_tickers_by_country_cache("US", limit=300, apikey=fmp_key)

# ÌïÑÏöî ÏóÜÎäî Ìã∞Ïª§ Ï†úÏô∏ÌïòÍ∏∞ (ÏòµÏÖò)
tickers_to_remove = {
    "NVL",
    "SOJE",
    "RY-PT",
    "CEO",
    "DUKB",
    "ATVI",
    "SQ",
    "AED",
    "BACRP",
    "BDXA",
    "VZA",
    "AMOV",
    "ANTM",
    "SOJC",
    "ACH",
    "TBC",
    "PXD",
    "TBB",
    "SOJD",
    "AFA",
    "AEH",
}

tickers_for_cache = [t for t in tickers_for_cache if t not in tickers_to_remove]

# Ï∫êÏãú ÏóÖÎç∞Ïù¥Ìä∏ (ÎàÑÎùΩÎêú Îç∞Ïù¥ÌÑ∞Îßå Î∞õÏïÑÏÑú yf_cache_multi.csv ÌååÏùº Í∞±Ïã†)
cache = update_cache(tickers_for_cache)

# Ïù¥Ï†ú cacheÏóêÎäî ÏµúÏã†ÏúºÎ°ú Ï±ÑÏõåÏßÑ Îç∞Ïù¥ÌÑ∞Í∞Ä Îì§Ïñ¥ÏûàÏùå
print(cache.head())
print("Ï∫êÏãú Îç∞Ïù¥ÌÑ∞ Î≤îÏúÑ:", cache.index.min(), "~", cache.index.max())

if isinstance(cache.columns, pd.MultiIndex):
    successful_tickers = set([col[0] for col in cache.columns if col[1] == "Close"])
else:
    successful_tickers = set(cache.columns)

# ‚úÖ ÏµúÏ¢Ö Ìã∞Ïª§ Î¶¨Ïä§Ìä∏
tickers = [t for t in tickers if t in successful_tickers]

################################################################################

# Assume cache is already loaded and up-to-date (from yf_cache_downloader)

# Define the date range for last 1 year up to today
end_date = pd.Timestamp.today().normalize()
start_date = end_date - pd.Timedelta(days=365)

# Clip start_date and end_date to cache's available index range to avoid KeyErrors
start_date = max(start_date, cache.index.min())
end_date = min(end_date, cache.index.max())

# Slice cache by this date range
cache_slice = cache.loc[start_date:end_date]

# Extract Close prices only
if isinstance(cache_slice.columns, pd.MultiIndex):
    # Select columns where second level is 'Close'
    close_cols = [col for col in cache_slice.columns if col[1] == "Close"]
    df_close = cache_slice[close_cols].copy()
    # Rename columns to just ticker symbols
    df_close.columns = [col[0] for col in df_close.columns]
else:
    # Single-level columns (unlikely if your cache is multi-indexed)
    if "Close" in cache_slice.columns:
        df_close = cache_slice[["Close"]].copy()
        df_close.columns = [tickers[0]]  # Assuming one ticker
    else:
        raise ValueError("‚ùå 'Close' column not found in cache.")

# Drop any columns (tickers) with all NaN Close values
df_close.dropna(axis=1, how="all", inplace=True)

print("Date range in df_close:", df_close.index.min(), "to", df_close.index.max())
print("Number of tickers with Close data:", len(df_close.columns))
print(df_close.head())
###################################################################################
# Ïòà: cacheÏóêÏÑú Close Í∞ÄÍ≤©Îßå Ï∂îÏ∂úÌïú ÌõÑ
df_momentum = df_close.copy()


def check_momentum_conditions(ticker: str) -> dict:
    result = {
        "ma_crossover": False,
        "ma_crossover_lt": False,
        "return_20d": False,
        "return_60d": False,
        "rsi_rebound": False,
        "macd_golden_cross": False,
    }

    try:
        # Ìã∞Ïª§Í∞Ä Îç∞Ïù¥ÌÑ∞Ïóê ÏóÜÏúºÎ©¥ Î∞îÎ°ú Î∞òÌôò
        if ticker not in df_momentum.columns:
            print(f"[Error] {ticker} not in df_momentum.columns")
            return result

        # Í∞úÎ≥Ñ Ï¢ÖÎ™© ÏãúÍ≥ÑÏó¥ Ï∂îÏ∂ú ÌõÑ 'Close'Î°ú Ïª¨ÎüºÎ™Ö ÌÜµÏùº
        df_ticker = df_momentum[[ticker]].copy()
        df_ticker.columns = ["Close"]

        # Í≤∞Ï∏°Ïπò Ï≤òÎ¶¨
        df_ticker["Close"] = df_ticker["Close"].ffill()

        if df_ticker["Close"].isna().all():
            print(f"[Error] All 'Close' values are NaN for {ticker}")
            return result

        if len(df_ticker) < 22:
            print(
                f"[Warning] Not enough data rows for 20-day return calculation for {ticker} (rows={len(df_ticker)})"
            )
            return result

        # Ïù¥ÎèôÌèâÍ∑†ÏÑ† Í≥ÑÏÇ∞
        df_ticker["MA20"] = df_ticker["Close"].rolling(window=5).mean()
        df_ticker["MA60"] = df_ticker["Close"].rolling(window=20).mean()

        if pd.notna(df_ticker["MA20"].iloc[-1]) and pd.notna(
            df_ticker["MA60"].iloc[-1]
        ):
            if df_ticker["MA20"].iloc[-1] > df_ticker["MA60"].iloc[-1]:
                result["ma_crossover"] = True

        df_ticker["MA50"] = df_ticker["Close"].rolling(window=50).mean()
        df_ticker["MA200"] = df_ticker["Close"].rolling(window=200).mean()

        if pd.notna(df_ticker["MA50"].iloc[-1]) and pd.notna(
            df_ticker["MA200"].iloc[-1]
        ):
            if df_ticker["MA50"].iloc[-1] > df_ticker["MA200"].iloc[-1]:
                result["ma_crossover_lt"] = True

        # 20Ïùº ÏàòÏùµÎ•† Í≥ÑÏÇ∞
        try:
            return_20d = (
                df_ticker["Close"].iloc[-1] / df_ticker["Close"].iloc[-21] - 1
            ) * 100
            if return_20d >= 10:
                result["return_20d"] = True
        except IndexError:
            print(
                f"[Warning] Not enough data for 20-day return calculation for {ticker}"
            )

        # 60Ïùº ÏàòÏùµÎ•† Í≥ÑÏÇ∞
        try:
            return_60d = (
                df_ticker["Close"].iloc[-1] / df_ticker["Close"].iloc[-61] - 1
            ) * 100
            if return_60d >= 10:
                result["return_60d"] = True
        except IndexError:
            print(
                f"[Warning] Not enough data for 60-day return calculation for {ticker}"
            )

        # RSI
        try:
            rsi = ta.momentum.RSIIndicator(df_ticker["Close"], window=14).rsi()
            if len(rsi) >= 7:
                recent = rsi.iloc[-7:]
                if (
                    all(recent > 50)
                    and recent.iloc[-1] < 70
                    and recent.is_monotonic_increasing
                ):
                    result["rsi_rebound"] = True
        except Exception as e:
            print(f"[RSI Error] {ticker}: {e}")

        # MACD
        try:
            macd_obj = ta.trend.MACD(df_ticker["Close"])
            macd_line = macd_obj.macd()
            signal_line = macd_obj.macd_signal()

            if len(macd_line) >= 7:
                macd_recent = macd_line.iloc[-7:]
                signal_recent = signal_line.iloc[-7:]
                if (macd_recent > signal_recent).sum() >= 5 and macd_recent.iloc[
                    -1
                ] > signal_recent.iloc[-1]:
                    if macd_recent.iloc[-1] > macd_recent.iloc[0]:
                        result["macd_golden_cross"] = True
        except Exception as e:
            print(f"[MACD Error] {ticker}: {e}")

    except Exception as e:
        print(f"[Download Error] Ticker {ticker}: {e}")

    return result


def check_momentum_conditions_batch(tickers: list) -> pd.DataFrame:
    results = []
    for ticker in tickers:
        # print(f"Processing {ticker} ...")
        res = check_momentum_conditions(ticker)
        res["Ticker"] = ticker
        results.append(res)
    # Í≤∞Í≥º Î¶¨Ïä§Ìä∏Î•º DataFrameÏúºÎ°ú Î≥ÄÌôò (Ticker Ïª¨Îüº Ï≤´ ÏπºÎüºÏúºÎ°ú Ïù¥Îèô)
    df_results = pd.DataFrame(results)
    cols = ["Ticker"] + [c for c in df_results.columns if c != "Ticker"]
    df_results = df_results[cols]
    return df_results


df_batch_result = check_momentum_conditions_batch(tickers)


def score_momentum(ma, ma_lt, ret_20d, ret_60d, rsi, macd):
    score = 0

    # Ïù¥ÎèôÌèâÍ∑† ÌÅ¨Î°úÏä§Ïò§Î≤Ñ (Îã®Í∏∞, Ïû•Í∏∞)
    if ma:  # Îã®Í∏∞ MA ÌÅ¨Î°úÏä§ Ïò§Î≤Ñ Ïã†Ìò∏ (True/False)
        score += 10
    if ma_lt:  # Ïû•Í∏∞ MA ÌÅ¨Î°úÏä§ Ïò§Î≤Ñ Ïã†Ìò∏ (True/False)
        score += 15

    # RSI Í≥ºÎß§ÎèÑ Î∞òÎì± (True/False)
    if rsi:
        score += 20

    # MACD Í≥®Îì†ÌÅ¨Î°úÏä§ (True/False)
    if macd:
        score += 20

    # Îã®Í∏∞ ÏàòÏùµÎ•† Î∞òÏòÅ (Ïòà: 20Ïùº ÏàòÏùµÎ•†)
    if ret_20d is not None:
        if ret_20d > 0:
            score += min(ret_20d * 100, 10)  # 0~10Ï†ê, 1% ÏÉÅÏäπÏãú 1Ï†ê

    # Ï§ëÍ∏∞ ÏàòÏùµÎ•† Î∞òÏòÅ (Ïòà: 60Ïùº ÏàòÏùµÎ•†)
    if ret_60d is not None:
        if ret_60d > 0:
            score += min(ret_60d * 100, 15)  # 0~15Ï†ê
        # else:
        #     curr_score = score # ÌòÑÏû¨ Ï†êÏàò
        #     curr_score = curr_score * 0.5
        #     score = max(score + ret_60d * 100, curr_score)

    return round(score, 2)


def get_operating_income_yoy(ticker_obj):
    try:
        financials = ticker_obj.financials

        if "Operating Income" not in financials.index:
            return None

        operating_income = financials.loc["Operating Income"].dropna()
        operating_income = operating_income.sort_index()  # Ïò§ÎûòÎêú Ïàú Ï†ïÎ†¨

        if len(operating_income) < 2:
            return None

        # ÏµúÍ∑º 2ÎÖÑÏπò ÏòÅÏóÖÏù¥Ïùµ
        latest = operating_income.iloc[-1]
        prev = operating_income.iloc[-2]

        if prev == 0:
            return None  # 0ÏúºÎ°ú ÎÇòÎàÑÍ∏∞ Î∞©ÏßÄ

        yoy_growth = (latest - prev) / abs(prev)
        return yoy_growth

    except Exception:
        return None


def get_operating_income_qoq(ticker_obj):
    try:
        financials = ticker_obj.quarterly_financials

        if "Operating Income" not in financials.index:
            return None

        operating_income = financials.loc["Operating Income"].dropna()
        operating_income = operating_income.sort_index()  # Ïò§ÎûòÎêú Ïàú Ï†ïÎ†¨

        if len(operating_income) < 2:
            return None

        latest = operating_income.iloc[-1]
        prev = operating_income.iloc[-2]

        if prev == 0:
            return None

        qoq_growth = (latest - prev) / abs(prev)
        return qoq_growth

    except Exception:
        return None


def score_intrinsic_value(
    conf_lower, conf_upper, current_price, fcf_yield, tenyr_treasury_yield, fcf_cagr
):
    score = 0

    if conf_lower is not None and conf_upper is not None and current_price is not None:
        if current_price < conf_upper:
            score += 2  # price is within fair value range
            if current_price <= conf_lower:
                score += 3  # price is at or below lower bound of fair value range
        else:
            score -= 3  # price outside fair value range

    if fcf_yield is not None:
        if fcf_yield > tenyr_treasury_yield:
            score += 2
        elif fcf_yield < tenyr_treasury_yield:
            score -= 1

    if fcf_cagr is not None:
        if fcf_cagr >= 0:
            score += 2
        else:
            score -= 1

    return score


def monte_carlo_dcf_valuation(
    info,
    initial_fcf,
    wacc,
    terminal_growth_rate,
    projection_years=5,
    num_simulations=10_000,
):
    if initial_fcf <= 0:
        return (None, None)
    if wacc <= terminal_growth_rate:
        return (None, None)

    if projection_years <= 0 or num_simulations <= 0:
        return (None, None)

    shares_outstanding = info.get("sharesOutstanding")
    if not shares_outstanding or shares_outstanding <= 0:
        return (None, None)

    total_debt = info.get("totalDebt") or 0
    cash = info.get("totalCash") or 0
    net_debt = total_debt - cash

    growth_mean = 0.08
    growth_std = 0.03

    equity_values = []

    for _ in range(num_simulations):
        fcf = initial_fcf
        total_value = 0

        for year in range(1, projection_years + 1):
            growth_rate = np.random.normal(growth_mean, growth_std)
            fcf *= 1 + growth_rate
            discounted_fcf = fcf / ((1 + wacc) ** year)
            total_value += discounted_fcf

        terminal_value = (
            fcf * (1 + terminal_growth_rate) / (wacc - terminal_growth_rate)
        )
        discounted_terminal_value = terminal_value / ((1 + wacc) ** projection_years)

        enterprise_value = total_value + discounted_terminal_value
        equity_value = enterprise_value - net_debt

        equity_values.append(equity_value)

    equity_values = np.array(equity_values)
    fair_value_per_share = equity_values / shares_outstanding

    # mean_val = np.mean(fair_value_per_share)
    # median_val = np.median(fair_value_per_share)
    # std_val = np.std(fair_value_per_share)
    conf_lower = np.percentile(fair_value_per_share, 2.5)
    conf_upper = np.percentile(fair_value_per_share, 97.5)

    return (float(conf_lower), float(conf_upper))


def analyze_moat(company_name: str, date_kr_ymd: str) -> str:
    prompt = f"""
ÎãπÏã†ÏùÄ Í≤ΩÏ†úÏ†Å Ìï¥Ïûê(Moat) Î∂ÑÏÑùÍ≥º Í∞ÄÏπò Ìï®Ï†ï(Value Trap) ÌÉêÏßÄÏóê ÌäπÌôîÎêú Ï†ÑÎ¨∏ Ìà¨Ïûê Î∂ÑÏÑùÍ∞ÄÏûÖÎãàÎã§. Î∞òÎìúÏãú ÌïúÍµ≠Ïñ¥Î°ú ÎãµÎ≥ÄÌïòÏã≠ÏãúÏò§.

{date_kr_ymd} Í∏∞Ï§Ä "{company_name}"Ïùò Ï†ïÎ≥¥Î•º Í≤ÄÏÉâÌïòÏó¨, ÏïÑÎûò ÎÑ§ Í∞ÄÏßÄ Í∏∞Ï§ÄÏùÑ Î∞îÌÉïÏúºÎ°ú Ìï¥Îãπ Í∏∞ÏóÖÏùò **Ï§ëÏû•Í∏∞ ÌïµÏã¨ Í≤ΩÏüÅ Ïö∞ÏúÑ(Moat)** Î∞è **Value Trap Î¶¨Ïä§ÌÅ¨**Î•º Î™®Îëê Ï†ïÏÑ±Ï†Å¬∑Ï†ïÎüâÏ†ÅÏúºÎ°ú Î∂ÑÏÑùÌï¥ Ï£ºÏÑ∏Ïöî.

---

üß± [Í∏∞Ï§Ä 1] {date_kr_ymd} Í∏∞Ï§Ä Í≤ΩÏ†úÏ†Å Ìï¥Ïûê Î∂ÑÏÑù  
- Î∏åÎûúÎìú, ÎÑ§Ìä∏ÏõåÌÅ¨ Ìö®Í≥º, ÌäπÌóà/IP, Ï†ÑÌôò ÎπÑÏö© Îì± Ìï¥ÏûêÏùò Ïú†ÌòïÍ≥º Í∞ïÎèÑ  
- ÏÇ∞ÏóÖ ÎÇ¥ ÏßÄÎ∞∞Î†• ÎòêÎäî Íµ¨Ï°∞Ï†Å ÏßÑÏûÖ Ïû•Î≤Ω Ï°¥Ïû¨ Ïó¨Î∂Ä  
- Î™®Î∞© ÎòêÎäî ÌååÍ¥¥Ï†Å ÌòÅÏã†Ïùò ÏúÑÌòë Í∞ÄÎä•ÏÑ±  

üìâ [Í∏∞Ï§Ä 2] {date_kr_ymd} Í∏∞Ï§Ä Ïã§Ï†Å ÎØºÍ∞êÎèÑ Î∞è Value Trap Î¶¨Ïä§ÌÅ¨  
- ÏµúÍ∑º Ïã§Ï†Å Î∞úÌëúÏóêÏÑú Îß§Ï∂ú, Ïù¥Ïùµ, ÏÑ±Ïû•Î•† Ï∂îÏÑ∏Ïùò ÏïàÏ†ïÏÑ±  
- ROICÍ∞Ä WACCÎ•º Ï¥àÍ≥ºÌïòÎ©∞ Ïú†ÏßÄÎêòÎäîÏßÄ Ïó¨Î∂Ä  
- ÏãúÏû•Ï†êÏú†Ïú®, ÎßàÏßÑ, FCF Îì± Ï£ºÏöî ÏßÄÌëúÏùò ÌïòÎùΩ Ï°∞Ïßê  
- ÏùºÌöåÏÑ± ÏàòÏùµ ÎòêÎäî ÎπÑÏòÅÏóÖ Ìï≠Î™© ÏùòÏ°¥ Ïó¨Î∂Ä  

‚öîÔ∏è [Í∏∞Ï§Ä 3] {date_kr_ymd} Í∏∞Ï§Ä Í≤ΩÏüÅÏÇ¨ ÎåÄÎπÑ Ìï¥Ïûê Î∞©Ïñ¥Î†•  
- Í≤ΩÏüÅÏÇ¨ ÎåÄÎπÑ Í∏∞Ïà†Î†•, Ï†úÌíàÎ†•, Í∞ÄÍ≤© Í≤ΩÏüÅÎ†• Ïö∞ÏúÑ Ïó¨Î∂Ä  
- Ïã†Ï†úÌíà Ï∂úÏãú ÏÜçÎèÑ, Í∑úÏ†ú ÎåÄÏùëÎ†•, Ïú†ÌÜµÎ†•, Í∏ÄÎ°úÎ≤å ÏßÑÏ∂úÎ†• ÎπÑÍµê  
- ÏÇ∞ÏóÖ ÎÇ¥ ÏãúÏû•Ï†êÏú†Ïú® Î≥ÄÌôî Ï∂îÏÑ∏  

‚õ≥ [Í∏∞Ï§Ä 4] {date_kr_ymd} Í∏∞Ï§Ä Í≤ΩÏòÅÏßÑÏùò Ï†ÑÎûµ ÎåÄÏùëÎ†• Î∞è ÏûêÎ≥∏ Î∞∞Î∂Ñ  
- ÏûêÏÇ¨Ï£º Îß§ÏûÖ, Î∞∞Îãπ, Ïù∏ÏàòÌï©Î≥ë, R&D Îì± ÏûêÎ≥∏ Î∞∞Î∂ÑÏùò Ï£ºÏ£º ÏπúÌôîÏÑ±  
- Íµ¨Ï°∞Ï†Å ÏúÑÍ∏∞ ÎåÄÏùë Ï†ÑÎûµ Î≥¥Ïú† Ïó¨Î∂Ä  
- CEO, CFO Îì± Í≤ΩÏòÅÏßÑ Î¶¨ÎçîÏã≠Ïùò Ïã§ÌñâÎ†•  

---

‚ö†Ô∏è [Í∞êÏ†ê ÏöîÏù∏: {date_kr_ymd} Í∏∞Ï§Ä Value Trap ÏãúÍ∑∏ÎÑê ÌïòÎÇòÎùºÎèÑ Ï°¥Ïû¨ Ïãú Í∞ïÌïú Í∞êÏ†ê]  
- Î≥∏ÏßàÏ†Å ÌéÄÎçîÎ©òÌÑ∏ Î∂ïÍ¥¥ ÏßïÌõÑ  
- Í≤ΩÏüÅÏÇ¨Ïùò Í∏∞Ïà† ÌòÅÏã†Ïóê Î∞ÄÎ†§ ÏãúÏû• Ï†êÏú†Ïú® ÌïòÎùΩ
- ÏÑ±Ïû• ÏÇ∞ÏóÖ ÎÇ¥ ÏàòÏùµÏÑ±¬∑ÌòÑÍ∏àÌùêÎ¶Ñ¬∑Ï†êÏú†Ïú® ÎèôÎ∞ò ÌïòÎùΩ  

---

üì§ **Î∞òÎìúÏãú ÏïÑÎûò ÌòïÏãùÏùò JSONÏúºÎ°úÎßå Í∞ÑÍ≤∞ÌïòÍ≤å Ï∂úÎ†•ÌïòÏã≠ÏãúÏò§. Ï∂îÍ∞Ä ÏÑ§Î™ÖÏù¥ÎÇò ÏÉÅÏÑ∏ Î∂ÑÏÑùÏùÄ Ìè¨Ìï®ÌïòÏßÄ ÎßàÏã≠ÏãúÏò§.**

```json
{{
  "moat_analysis": "Í∏∞ÏóÖÏùò Ï§ëÏû•Í∏∞ ÌïµÏã¨ Í≤ΩÏüÅ Ïö∞ÏúÑ Î∞è Value Trap ÏúÑÌóòÏÑ± ÏöîÏïΩ (Î∞òÎìúÏãú 2~3Ï§Ñ Ïù¥ÎÇ¥ ÏöîÏïΩ)",
  "moat_score": 0,  // 0ÏóêÏÑú 10 ÏÇ¨Ïù¥ Ï†ïÏàò (ÏïÑÎûò Í∏∞Ï§Ä Ï∞∏Í≥†)
}}

Moat Score Í∏∞Ï§Ä (0~10):

0: ÏôÑÏ†ÑÌïú Commodity, Í∞ÄÍ≤© Í≤ΩÏüÅ Ïô∏ Í≤ΩÏüÅ Ïö∞ÏúÑ ÏóÜÏùå  
1-3: Í≤ΩÏüÅ Ïö∞ÏúÑ ÎØ∏ÎØ∏~ÎÇÆÏùå, Ï∞®Î≥ÑÌôî Î∂ÄÏ°±, ÏãúÏû• ÎÇ¥ Î∞©Ïñ¥Î†• ÏïΩÌï®  
4-5: Î∂ÄÎ∂ÑÏ†Å Í≤ΩÏüÅÎ†• Î≥¥Ïú†, ÏùºÏãúÏ†Å Ïö∞ÏúÑ ÌòπÏùÄ Ïú†ÏßÄ Î∂àÌôïÏã§  
6-7: ÏÉÅÎãπÌïú Í≤ΩÏüÅ Ïö∞ÏúÑ, Íµ¨Ï°∞Ï†Å Ïö∞ÏúÑ ÏûàÏúºÎÇò ÎåÄÏ≤¥ Í∞ÄÎä•ÏÑ± Ï°¥Ïû¨  
8-9: ÎöúÎ†∑ÌïòÍ≥† Ïû•Í∏∞Ï†Å Í≤ΩÏüÅ Ïö∞ÏúÑ, Í∞ïÎ†•Ìïú ÏßÑÏûÖ Ïû•Î≤ΩÍ≥º ÎÑ§Ìä∏ÏõåÌÅ¨ Ìö®Í≥º Ï°¥Ïû¨  
10: Ï†àÎåÄÏ†Å ÎèÖÏ†ê Ïö∞ÏúÑ, ÎåÄÏ≤¥ Î∂àÍ∞ÄÎä•ÌïòÎ©∞ ÏßÑÏûÖ Î∂àÍ∞Ä ÏàòÏ§Ä  

‚Äª Í≤ΩÏüÅ Ïö∞ÏúÑÍ∞Ä ÏïΩÌïòÍ±∞ÎÇò Value Trap ÏúÑÌóòÏù¥ ÌïòÎÇòÎùºÎèÑ Í∞êÏßÄÎêòÎ©¥ Ï†êÏàòÎ•º Í∞ïÌïòÍ≤å Í∞êÏ†êÌïòÍ≥†, Î≥¥ÏàòÏ†ÅÏúºÎ°ú ÏÇ∞Ï†ïÌïòÏã≠ÏãúÏò§.  
"""
    return prompt.strip()


def parse_moat_response(response_text: str) -> dict:
    """
    LLM ÏùëÎãµÏóêÏÑú moat_analysisÏôÄ moat_scoreÎ•º ÏïàÏ†ÑÌïòÍ≤å Ï∂îÏ∂úÌï©ÎãàÎã§.
    JSONÏù¥ ÌòºÌï©ÎêòÏñ¥ ÏûàÍ±∞ÎÇò ÌòïÏãùÏù¥ Î∂àÏôÑÏ†ÑÌï† Í≤ΩÏö∞ÏóêÎèÑ Ï≤òÎ¶¨Ìï©ÎãàÎã§.
    """
    # Í∏∞Î≥∏Í∞í
    result = {"moat_analysis": response_text.strip(), "moat_score": None}

    # JSON ÌòïÏãù Ï∂îÏ∂ú ÏãúÎèÑ
    try:
        # Ï§ëÍ¥ÑÌò∏Î°ú Îêú JSON Î∏îÎü≠ Ï∂îÏ∂ú
        match = re.search(r"\{.*?\}", response_text, re.DOTALL)
        if match:
            json_block = match.group(0)
            parsed = json.loads(json_block)
            result["moat_analysis"] = parsed.get(
                "moat_analysis", result["moat_analysis"]
            ).strip()
            result["moat_score"] = (
                int(parsed.get("moat_score"))
                if parsed.get("moat_score") is not None
                else None
            )
            return result
    except (json.JSONDecodeError, ValueError, TypeError):
        pass  # continue to fallback logic

    # fallback Ï†êÏàò Ï∂îÏ†ï Î°úÏßÅ (ÌÖçÏä§Ìä∏ Í∏∞Î∞ò Ï∂îÎ°†)
    lower_text = response_text.lower()
    if any(
        kw in lower_text
        for kw in ["Ï†àÎåÄÏ†Å ÎèÖÏ†ê", "ÏôÑÏ†ÑÌïú ÎèÖÏ†ê", "ÎåÄÏ≤¥ Î∂àÍ∞Ä", "ÏßÑÏûÖ Î∂àÍ∞Ä", "ÌäπÌóà Î≥¥Ìò∏"]
    ):
        result["moat_score"] = 10
    elif any(
        kw in lower_text
        for kw in ["ÏßÄÏÜçÏ†Å ÎèÖÏ†ê", "ÏßÄÏÜçÏ†ÅÏù∏ ÎèÖÏ†ê", "Í∞ïÎ†•Ìïú ÏßÑÏûÖ Ïû•Î≤Ω", "Í∑úÏ†ú Î≥¥Ìò∏"]
    ):
        result["moat_score"] = 9
    elif any(
        kw in lower_text
        for kw in ["ÎöúÎ†∑Ìïú Í≤ΩÏüÅ Ïö∞ÏúÑ", "Î∏åÎûúÎìú ÌååÏõå", "Í∑úÎ™®Ïùò Í≤ΩÏ†ú", "Ï†ÑÌôò ÎπÑÏö©"]
    ):
        result["moat_score"] = 8
    elif any(
        kw in lower_text
        for kw in ["Í∞ïÌïú Í≤ΩÏüÅÎ†•", "Í∏∞Ïà†Î†•", "Ïú†ÌÜµÎßù", "Í≤ΩÏüÅÏÇ¨ ÎåÄÎπÑ Ïö∞ÏúÑ"]
    ):
        result["moat_score"] = 7
    elif any(
        kw in lower_text for kw in ["ÏÉÅÎãπÌïú Í≤ΩÏüÅ Ïö∞ÏúÑ", "Ïö∞ÏúÑ ÏöîÏÜå Ï°¥Ïû¨", "ÎåÄÏ≤¥ Í∞ÄÎä•ÏÑ±"]
    ):
        result["moat_score"] = 6
    elif any(
        kw in lower_text for kw in ["ÌèâÍ∑† Ïù¥ÏÉÅÏùò Í≤ΩÏüÅÎ†•", "Ï∞®Î≥ÑÌôî ÎØ∏ÏïΩ", "Ïú†ÏßÄ Î∂àÌôïÏã§"]
    ):
        result["moat_score"] = 5
    elif any(
        kw in lower_text for kw in ["Î∂ÄÎ∂ÑÏ†Å Í≤ΩÏüÅÎ†•", "ÏùºÏãúÏ†Å ÏàòÏùµÏÑ±", "ÎåÄÏ≤¥Ïû¨ Ï°¥Ïû¨"]
    ):
        result["moat_score"] = 4
    elif any(
        kw in lower_text for kw in ["Í≤ΩÏüÅ Ïö∞ÏúÑ ÎÇÆÏùå", "Ï∞®Î≥ÑÌôî Í±∞Ïùò ÏóÜÏùå", "Î∞©Ïñ¥Î†• ÎÇÆÏùå"]
    ):
        result["moat_score"] = 3
    elif any(
        kw in lower_text for kw in ["ÎØ∏ÎØ∏Ìïú Í≤ΩÏüÅ Ïö∞ÏúÑ", "Îã®Í∏∞ Ïú†Ìñâ", "Íµ¨Ï°∞Ï†Å Ïö∞ÏúÑ ÏóÜÏùå"]
    ):
        result["moat_score"] = 2
    elif any(
        kw in lower_text
        for kw in [
            "Í≤ΩÏüÅ Ïö∞ÏúÑ ÏóÜÏùå",
            "ÏßÑÏûÖ Ïû•Î≤Ω ÏóÜÏùå",
            "Î∏åÎûúÎìú ÏóÜÏùå",
            "Í∏∞Ïà†Î†• ÏóÜÏùå",
            "commoditized",
        ]
    ):
        result["moat_score"] = 1
    elif any(
        kw in lower_text for kw in ["commodity", "ÏôÑÏ†ÑÌïú commodity", "ÏôÑÏ†Ñ Í≤ΩÏüÅ ÏãúÏû•"]
    ):
        result["moat_score"] = 0
    else:
        result["moat_score"] = -1  # ÌåêÎã® Î∂àÍ∞Ä (ÏòàÏô∏ Ï≤òÎ¶¨Ïö©)

    return result


def query_gemini(prompt: str) -> str:
    response = client.models.generate_content(
        model="gemini-2.5-flash",
        contents=prompt,
    )
    return response.text


retried_once = set()
q = Queue()
for ticker in tickers:
    q.put(ticker)


def process_ticker_quantitatives():
    while not q.empty():
        ticker = q.get()
        try:
            yf_ticker = yf.Ticker(ticker)
            info = yf_ticker.info
            beta = info.get("beta", None)
            name = info.get("shortName", ticker)
            industry = info.get("industry", None)
            currentPrice = info.get("currentPrice", None)
            percentage_change = get_percentage_change_ttm(ticker)

            # Valuation & Liquidity
            debtToEquity = info.get("debtToEquity", None)
            debtToEquity = debtToEquity / 100 if debtToEquity is not None else None
            currentRatio = info.get("currentRatio", None)
            pbr = info.get("priceToBook", None)
            per = info.get("trailingPE", None)

            # Industry
            industry_per = get_industry_per(industry)
            industry_per = round(industry_per) if industry_per is not None else None
            industry_roe = get_industry_roe(industry)
            industry_roa = get_industry_roa(industry)

            # Profitability
            roe = info.get("returnOnEquity", None)
            roa = info.get("returnOnAssets", None)
            icr = get_interest_coverage_ratio(yf_ticker)

            # Growth
            eps_cagr = compute_eps_growth_slope(yf_ticker)
            div_cagr = has_stable_dividend_growth_cagr(yf_ticker)
            opinc_yoy = get_operating_income_yoy(yf_ticker)
            opinc_qoq = get_operating_income_qoq(yf_ticker)

            # FCF & Valuation
            fcf_yield, fcf_cagr, fcf_list = get_fcf_yield_and_cagr(
                ticker, yf_ticker, api_key=fmp_key
            )
            tenyr_treasury_yield = get_10yr_treasury_yield()
            discount_rate = (
                (tenyr_treasury_yield + (beta * 5.0)) / 100.0
                if beta is not None
                else (tenyr_treasury_yield + 5.0) / 100.0
            )
            terminal_growth_rate = 0.02

            initial_fcf = (
                fcf_list[-1] if fcf_list and fcf_list[-1] is not None else None
            )

            intrinsic_value_range = (
                monte_carlo_dcf_valuation(
                    info,
                    initial_fcf,
                    discount_rate,
                    terminal_growth_rate,
                    projection_years=5,
                    num_simulations=10_000,
                )
                if initial_fcf is not None
                else (None, None)
            )

            result = {
                "ticker": ticker,
                "name": name,
                "price": currentPrice,
                "price_vs_fair_upper": (
                    ((intrinsic_value_range[1] - currentPrice) / currentPrice)
                    if intrinsic_value_range[1] and currentPrice
                    else None
                ),
                "price_vs_fair_lower": (
                    ((intrinsic_value_range[0] - currentPrice) / currentPrice)
                    if intrinsic_value_range[0] and currentPrice
                    else None
                ),
                "fcf_yield": fcf_yield,
                "per": per,
                "pbr": pbr,
                "de": debtToEquity,
                "cr": currentRatio,
                "roe": roe,
                "roa": roa,
                "icr": icr,
                "fcf_cagr": fcf_cagr,
                "eps_cagr": eps_cagr if isinstance(eps_cagr, float) else None,
                "div_cagr": div_cagr if isinstance(div_cagr, float) else None,
                "eps": eps_cagr if isinstance(eps_cagr, float) else None,
                "div_yield": (
                    info.get("dividendYield", div_cagr)
                    if div_cagr is not None
                    else None
                ),
                "opinc_yoy": opinc_yoy if isinstance(opinc_yoy, float) else None,
                "opinc_qoq": opinc_qoq if isinstance(opinc_qoq, float) else None,
                # Industry benchmarks
                "industry_per": industry_per,
                "industry_roe": industry_roe,
                "industry_roa": industry_roa,
                # NEW: fields needed for quant_style_score (will be filled later)
                "fcf_yield_rank": None,
                "per_rank": None,
                "pbr_rank": None,
                "fcf_cagr_rank": None,
                "eps_cagr_rank": None,
                "div_cagr_rank": None,
                "roe_z": None,
                "roa_z": None,
                # Misc.
                "industry": industry,
                "1M_Change": percentage_change,
            }

            with data_lock:
                data.append(result)

        except Exception as e:
            print(f"Error processing {ticker}: {e}")
            with data_lock:
                if ticker not in retried_once:
                    retried_once.add(ticker)
                    q.put(ticker)
            if "429" in str(e):
                print("Too many requests! Waiting 10 seconds...")
                time.sleep(10)

        finally:
            q.task_done()


threads = []

for _ in range(NUM_THREADS):
    t = threading.Thread(target=process_ticker_quantitatives)
    t.start()
    threads.append(t)

for t in threads:
    t.join()

q.join()

df = pd.DataFrame(data)

# Rank Í≥ÑÏÇ∞
df["fcf_yield_rank"] = df["fcf_yield"].rank(pct=True)
df["per_rank"] = 1 - df["per"].rank(pct=True)
df["pbr_rank"] = 1 - df["pbr"].rank(pct=True)
df["fcf_cagr_rank"] = df["fcf_cagr"].rank(pct=True)
df["eps_cagr_rank"] = df["eps_cagr"].rank(pct=True)
df["div_cagr_rank"] = df["div_cagr"].rank(pct=True)
# ÏóÖÏ¢ÖÎ≥Ñ ÌÜµÍ≥Ñ Í≥ÑÏÇ∞
industry_stats = df.groupby("industry").agg(
    {"roe": ["mean", "std"], "roa": ["mean", "std"]}
)

industry_stats.columns = [
    "_".join(col).strip() for col in industry_stats.columns.values
]
industry_stats.index.name = "industry"

df = df.merge(industry_stats, left_on="industry", right_index=True, how="left")


def safe_z(x, mean, std):
    if pd.isna(x) or pd.isna(mean) or pd.isna(std) or std == 0:
        return 0
    return (x - mean) / std


df["roe_z"] = df.apply(
    lambda row: safe_z(row["roe"], row["roe_mean"], row["roe_std"]), axis=1
)
df["roa_z"] = df.apply(
    lambda row: safe_z(row["roa"], row["roa_mean"], row["roa_std"]), axis=1
)


def compute_quant_scores(df, tenyr_yield):
    scores = []
    for _, row in df.iterrows():
        valuation_score, momentum_score = quant_style_score(
            price_vs_fair_upper=row["price_vs_fair_upper"],
            price_vs_fair_lower=row["price_vs_fair_lower"],
            fcf_yield_rank=row["fcf_yield_rank"],
            fcf_vs_treasury_spread=(
                row["fcf_yield"] - tenyr_yield if row["fcf_yield"] is not None else None
            ),
            per=row["per"],
            per_rank=row["per_rank"],
            pbr_rank=row["pbr_rank"],
            de=row["de"],
            cr=row["cr"],
            industry_per=row["industry_per"],
            roe_z=row["roe_z"],
            roa_z=row["roa_z"],
            roe=row["roe"],  # Ï∂îÍ∞Ä
            roa=row["roa"],  # Ï∂îÍ∞Ä
            icr=row["icr"],
            fcf_cagr_rank=row["fcf_cagr_rank"],
            eps_cagr_rank=row["eps_cagr_rank"],
            div_cagr_rank=row["div_cagr_rank"],
            eps=row["eps"],
            div_yield=row["div_yield"],
            opinc_yoy=row["opinc_yoy"],
            opinc_qoq=row["opinc_qoq"],
            industry_roe=row["industry_roe"],
            industry_roa=row["industry_roa"],
        )
        scores.append(
            {
                "ticker": row["ticker"],
                "valuation_score": valuation_score,
                "momentum_score": momentum_score,
                "total_score": valuation_score + momentum_score,
            }
        )
    return pd.DataFrame(scores)


###############
def compute_price_flow_scores(df_main, df_batch_result):
    scores = []
    for ticker in df_main["ticker"]:
        row = df_batch_result.loc[df_batch_result["Ticker"] == ticker]
        if row.empty:
            scores.append(None)
            continue

        ma = bool(row["ma_crossover"].values[0])
        ma_lt = bool(row["ma_crossover_lt"].values[0])
        ret20 = row["return_20d"].values[0]
        ret60 = row["return_60d"].values[0]
        rsi = bool(row["rsi_rebound"].values[0])
        macd = bool(row["macd_golden_cross"].values[0])

        score = score_momentum(ma, ma_lt, ret20, ret60, rsi, macd)
        scores.append(score)
    return scores


# Î©îÏù∏ dfÏóê price_flow_score Ïª¨Îüº Ï∂îÍ∞Ä
# Step 1: price_flow_score Î®ºÏ†Ä Í≥ÑÏÇ∞
df["price_flow_score"] = compute_price_flow_scores(df, df_batch_result)

# Step 2: ÌÄÄÌä∏ Ï†êÏàò Í≥ÑÏÇ∞ (valuation_score, momentum_score, total_score Îì±)
tenyr_yield = get_10yr_treasury_yield()
score_df = compute_quant_scores(df, tenyr_yield)

# Step 3: Îëê Í≤∞Í≥º merge (Ïù¥Îïå total_scoreÍ∞Ä ÏÉùÍπÄ)
final_df = df.merge(score_df, on="ticker", how="left")

# Step 4: total_scoreÏóê price_flow_score ÎçîÌïòÍ∏∞
final_df["total_score"] = final_df["total_score"].fillna(0) + final_df[
    "price_flow_score"
].fillna(0)


def normalize_series(series):
    min_val = series.min()
    max_val = series.max()
    if pd.isna(min_val) or pd.isna(max_val) or min_val == max_val:
        return pd.Series([0.0] * len(series), index=series.index)
    return (series - min_val) / (max_val - min_val) * 100  # ‚úÖ Scale to 0‚Äì100


# Normalize each category to 0‚Äì100
final_df["valuation_score_norm"] = normalize_series(final_df["valuation_score"])
final_df["momentum_score_norm"] = normalize_series(final_df["momentum_score"])
final_df["price_flow_score_norm"] = normalize_series(
    final_df["price_flow_score"].fillna(0)
)

# Buffett-style
valuation_weight = 0.4
momentum_weight = 0.3
price_flow_weight = 0.3

# quant fund
# valuation_weight = 0.4
# momentum_weight = 0.4
# price_flow_weight = 0.2

final_df["total_score"] = (
    final_df["valuation_score_norm"] * valuation_weight
    + final_df["momentum_score_norm"] * momentum_weight
    + final_df["price_flow_score_norm"] * price_flow_weight
)

# Round the normalized scores and total
score_cols = [
    "valuation_score_norm",
    "momentum_score_norm",
    "price_flow_score_norm",
    "total_score",
]

final_df[score_cols] = final_df[score_cols].round()

# 1) rename_dict Ï†ïÏùò
rename_dict = {
    "ticker": "Ìã∞Ïª§",
    "name": "Ï¢ÖÎ™©",  # Ïã§Ï†ú final_dfÏóê name Ïª¨ÎüºÏù¥ ÏûàÏúºÎ©¥
    "industry": "ÏóÖÏ¢Ö",
    "price": "ÌòÑÏû¨Í∞Ä",
    "1M_Change": "1Í∞úÏõîÎåÄÎπÑ",
    "valuation_score_norm": "Î∞∏Î•òÏóêÏù¥ÏÖò",
    "momentum_score_norm": "Ïã§Ï†ÅÎ™®Î©òÌÖÄ",
    "price_flow_score_norm": "Í∞ÄÍ≤©/ÏàòÍ∏â",
    "total_score": "Ï¥ùÏ†êÏàò",
}

# 2) Ïª¨ÎüºÎ™Ö Î≥ÄÍ≤Ω
final_df = final_df.rename(columns=rename_dict)

# 3) ÎÇ¥Î≥¥ÎÇº Ïª¨Îüº Î¶¨Ïä§Ìä∏ (ÏõêÌïòÎäî ÏàúÏÑú Î∞è Ïª¨ÎüºÎßå)
export_columns_kr = [
    "Ìã∞Ïª§",
    "Ï¢ÖÎ™©",
    "Ï¥ùÏ†êÏàò",
    "ÏóÖÏ¢Ö",
    "ÌòÑÏû¨Í∞Ä",
    "1Í∞úÏõîÎåÄÎπÑ",
    "Î∞∏Î•òÏóêÏù¥ÏÖò",
    "Ïã§Ï†ÅÎ™®Î©òÌÖÄ",
    "Í∞ÄÍ≤©/ÏàòÍ∏â",
]

# 4) Ï†ïÎ†¨
df = pd.DataFrame()
# Ïª¨ÎüºÏùÑ ÌïÑÌÑ∞ÎßÅÌïú ÏÉàÎ°úÏö¥ dfÎ°ú overwrite
df = (
    final_df[export_columns_kr]
    .sort_values(by="Ï¥ùÏ†êÏàò", ascending=False)
    .reset_index(drop=True)
)
df = df.drop(columns=[col for col in df.columns if col not in export_columns_kr])

# 3Ô∏è‚É£ Ïª¨Îüº ÏàúÏÑú ÎßûÏ∂îÍ∏∞ (ÌòπÏãú ÏàúÏÑú ÌãÄÏñ¥Ï°åÏùÑ ÏàòÎèÑ ÏûàÏúºÎãà)
df = df[export_columns_kr]
# Ìã∞Ïª§ Í∏∞Ï§ÄÏúºÎ°ú Ï§ëÎ≥µ Ï†úÍ±∞ (Ï≤´ Î≤àÏß∏ Ìï≠Î™©Îßå ÎÇ®ÍπÄ)
df = df.drop_duplicates(subset="Ìã∞Ïª§", keep="first")


# Í∑∏Î¶¨Í≥† Í∑∏ÎåÄÎ°ú Ï†ÄÏû•
df.to_excel(excel_path, index=False)


# 6) ÏÉÅÏúÑ Ìã∞Ïª§ Î¶¨Ïä§Ìä∏ Ï∂îÏ∂ú

top_tickers_news = df["Ìã∞Ïª§"].head(news_lookup).tolist()


#################################################################
# Gemini 2.5 Flash API rate limits:
# rpm: 10 (requests per minute)
# tpm: 250,000 (tokens per minute)
# rpd: 250 (requests per day)

# Ï†ÅÏ†àÌïú sleep_time Í≥ÑÏÇ∞:
# - 1Î∂ÑÏóê 10Ìöå ÏöîÏ≤≠ Í∞ÄÎä• ‚Üí 1Ìöå ÏöîÏ≤≠ ÌõÑ ÏµúÏÜå 6Ï¥à ÎåÄÍ∏∞ ÌïÑÏöî (60Ï¥à / 10Ìöå = 6Ï¥à)
# - ÌïòÎ£® 250Ìöå Ï†úÌïú ‚Üí 250Í∞ú Ï¥àÍ≥º Ïãú Ï∂îÍ∞Ä ÎåÄÍ∏∞ ÌïÑÏöî
# - ÌÜ†ÌÅ∞ Ï†úÌïúÏùÄ ÏùºÎ∞ò Îâ¥Ïä§/Î™®Ìä∏ ÌîÑÎ°¨ÌîÑÌä∏ÏóêÏÑúÎäî Í±∞Ïùò ÎèÑÎã¨ÌïòÏßÄ ÏïäÏùå

# Îî∞ÎùºÏÑú moat/news batch Ìï®ÏàòÏóêÏÑú sleep_time=6~7Ï¥à Í∂åÏû•
# ÏòàÏãú:
# moat_df = generate_moat_summary_batch(df, moat_limit, batch_size=10, sleep_time=7)
# (batch_size=10, sleep_time=7 ‚Üí 1Î∂ÑÏóê ÏµúÎåÄ 8~9Ìöå ÏöîÏ≤≠, ÏïàÏ†Ñ)

# ÎßåÏïΩ Ïó¨Îü¨ Ïä§Î†àÎìú/ÌîÑÎ°úÏÑ∏Ïä§ÏóêÏÑú ÎèôÏãúÏóê Gemini Ìò∏Ï∂ú Ïãú, Î∞òÎìúÏãú Ï†ÑÏ≤¥ ÏöîÏ≤≠ Ìï©ÏÇ∞Ïù¥ rpm/rpdÎ•º ÎÑòÏßÄ ÏïäÎèÑÎ°ù Ï°∞Ï†ï ÌïÑÏöî


# Ï∞∏Í≥†: Ïã§Ï†ú Ïö¥ÏòÅ ÌôòÍ≤ΩÏóêÏÑúÎäî 7Ï¥à Ïù¥ÏÉÅ(Ïòà: 8~10Ï¥à)Î°ú Ïó¨Ïú† ÏûàÍ≤å ÏÑ§Ï†ïÌïòÎ©¥ Îçî ÏïàÏ†ÑÌï®


def generate_moat_summary_batch(
    df: pd.DataFrame, batch_size: int = 10, sleep_time: int = 8
) -> pd.DataFrame:
    top_tickers = df["Ï¢ÖÎ™©"].tolist()
    moat_data = []

    for i in range(0, len(top_tickers), batch_size):
        batch = top_tickers[i : i + batch_size]
        for ticker in batch:
            try:
                prompt = analyze_moat(ticker, date_kr_ymd)
                moat_text = query_gemini(prompt)
                parsed_response = parse_moat_response(moat_text)
                moat_data.append(
                    {
                        "Í∏∞ÏóÖÎ™Ö": ticker,
                        "Í≤ΩÏüÅ Ïö∞ÏúÑ Î∂ÑÏÑù": parsed_response["moat_analysis"],
                        "Moat Ï†êÏàò": parsed_response["moat_score"],
                    }
                )
                time.sleep(1)
            except Exception as e:
                moat_data.append(
                    {
                        "Í∏∞ÏóÖÎ™Ö": f"‚ùå Ïò§Î•ò: {str(e)}",
                        "Í≤ΩÏüÅ Ïö∞ÏúÑ Î∂ÑÏÑù": "Î∂ÑÏÑù Ïã§Ìå®",
                        "Moat Ï†êÏàò": "Î∂ÑÏÑù Ïã§Ìå®",
                    }
                )
        if i + batch_size < len(top_tickers):
            print(
                f"Batch {i // batch_size + 1} completed. Sleeping {sleep_time} seconds to avoid rate limit..."
            )
            time.sleep(sleep_time)
    return pd.DataFrame(moat_data)


moat_df = generate_moat_summary_batch(df, batch_size=10, sleep_time=8)


#################################################################
# 1. ticker / Í∏∞ÏóÖÎ™Ö Í∏∞Ï§ÄÏúºÎ°ú moat_dfÎ•º dfÏóê merge
df = df.merge(
    moat_df[["Í∏∞ÏóÖÎ™Ö", "Moat Ï†êÏàò"]],
    left_on="Ï¢ÖÎ™©",  # final_df / dfÏóêÏÑú Í∏∞ÏóÖÎ™ÖÏùÑ ÎÇòÌÉÄÎÇ¥Îäî Ïª¨Îüº
    right_on="Í∏∞ÏóÖÎ™Ö",  # moat_dfÏóêÏÑú Í∏∞ÏóÖÎ™Ö Ïª¨Îüº
    how="left",
)

# 2. Moat Ï†êÏàò Í≤∞Ï∏°Í∞íÏùÄ 0ÏúºÎ°ú Ï±ÑÏõÄ
df["Moat Ï†êÏàò"] = df["Moat Ï†êÏàò"].fillna(0).astype(float)


df["moat_score_norm"] = normalize_series(df["Moat Ï†êÏàò"])


# 4. Í∏∞Ï°¥ Í∞ÄÏ§ëÏπò ÏÑ§Ï†ï (Ïòà: Buffett Ïä§ÌÉÄÏùºÏóê Moat Ìè¨Ìï®)
valuation_weight = 0.35
moat_weight = 0.35  # Moat Í∞ÄÏ§ëÏπò (Ï°∞Ï†à Í∞ÄÎä•)
momentum_weight = 0.2
price_flow_weight = 0.1

# 5. ÏÉà total_score Í≥ÑÏÇ∞
df["Ï¥ùÏ†êÏàò"] = (
    df["Î∞∏Î•òÏóêÏù¥ÏÖò"] * valuation_weight
    + df["Ïã§Ï†ÅÎ™®Î©òÌÖÄ"] * momentum_weight
    + df["Í∞ÄÍ≤©/ÏàòÍ∏â"] * price_flow_weight
    + df["moat_score_norm"] * moat_weight
)


score_cols = ["Î∞∏Î•òÏóêÏù¥ÏÖò", "Ïã§Ï†ÅÎ™®Î©òÌÖÄ", "Í∞ÄÍ≤©/ÏàòÍ∏â", "moat_score_norm", "Ï¥ùÏ†êÏàò"]
df[score_cols] = df[score_cols].round()

# 7. ÌïÑÏöîÌïòÎ©¥ Ï†ïÎ†¨
df = df.sort_values(by="Ï¥ùÏ†êÏàò", ascending=False).reset_index(drop=True)
df = df.drop(columns=["Í∏∞ÏóÖÎ™Ö", "moat_score_norm"])


#################################################################
def get_news_for_tickers(tickers, api_token):
    all_news = []

    for ticker in tickers:
        try:
            company_info = yf.Ticker(ticker).info
            full_name = company_info.get("shortName", "")
        except Exception as e:
            print(f"[{ticker}] ‚ö†Ô∏è Failed to retrieve company info: {e}")
            continue

        if not full_name:
            print(f"[{ticker}] ‚ö†Ô∏è No company name found, skipping.")
            continue

        published_after = (datetime.now() - timedelta(days=2)).strftime("%Y-%m-%d")
        url = "https://api.marketaux.com/v1/news/all"
        params = {
            "api_token": api_token,
            "symbols": ticker.upper(),
            "language": "en",
            "published_after": published_after,
            "limit": 5,  # Fetch extra to allow filtering
            "sort": "relevance",
        }

        try:
            response = requests.get(url, params=params)
            response.raise_for_status()
            articles = response.json().get("data", [])
        except Exception as e:
            print(f"[{ticker}] ‚ùå API request failed: {e}")
            continue

        filtered_articles = []

        for article in articles:
            relevant = False
            sentiment_score = None

            for entity in article.get("entities", []):
                if entity.get("symbol", "").upper() == ticker.upper():
                    relevant = True
                    score = entity.get("sentiment_score")
                    try:
                        sentiment_score = round(float(score), 2)
                    except (TypeError, ValueError):
                        sentiment_score = None
                    break

            if not relevant:
                continue

            filtered_articles.append(
                {
                    "Í∏∞ÏóÖÎ™Ö": full_name,
                    "Í∏∞ÏÇ¨ Ï†úÎ™©": article.get("title"),
                    "Í∞êÏ†ïÏßÄÏàò": sentiment_score,
                    "Îâ¥Ïä§ ÏöîÏïΩ": article.get("description"),
                    "Î∞úÌñâÏùº": article.get("published_at", "")[:10],
                    "URL": article.get("url"),
                }
            )

            if len(filtered_articles) >= 3:
                break

        if filtered_articles:
            all_news.extend(filtered_articles)
        else:
            print(f"[{ticker}] ‚ÑπÔ∏è No relevant news articles found.")

    return pd.DataFrame(all_news)


#################################################################
news_df = get_news_for_tickers(top_tickers_news, api_token=marketaux_api)
#################################################################

# Seleccionar Criterio de Optimizaci√≥n
optimization_criterion = "sortino"  # Cambia a 'sharpe', 'cvar', 'sortino' o 'variance' para optimizar esos criterios
df = df.sort_values(by="Ï¥ùÏ†êÏàò", ascending=False).reset_index(drop=True)
top_tickers = df["Ìã∞Ïª§"].head(opt).tolist()
symbols = top_tickers

# Ïò§Îäò ÎÇ†Ïßú
end_date = dt.datetime.today() - dt.timedelta(days=weekend)

# 1ÎÖÑ Ï†Ñ ÎÇ†Ïßú (365Ïùº Ï†Ñ)
start_date = end_date - timedelta(days=365)

# Î¨∏ÏûêÏó¥ Ìè¨Îß∑ÏúºÎ°ú Î≥ÄÌôò (yfinanceÏóê ÎßûÍ≤å)
start_str = start_date.strftime("%Y-%m-%d")
end_str = end_date.strftime("%Y-%m-%d")

# 1. 'Close' Ïª¨ÎüºÎßå Ï∂îÏ∂ú (MultiIndex Ï†ÑÏö©)
if isinstance(cache.columns, pd.MultiIndex):
    # 'Close' Ïª¨ÎüºÎßå ÏÑ†ÌÉù
    close_columns = [col for col in cache.columns if col[1] == "Close"]
    close_df = cache[close_columns].copy()
    close_df.columns = [col[0] for col in close_columns]  # ‚Üí ('AAPL', 'Close') ‚Üí 'AAPL'
else:
    raise ValueError(
        "Expected MultiIndex columns in cache, but got single-index DataFrame."
    )

# 2. Ïú†Ìö®Ìïú Ï¢ÖÎ™©(symbols)Îßå Ï∂îÏ∂ú
symbols_in_data = [s for s in symbols if s in close_df.columns]
if not symbols_in_data:
    raise ValueError("No valid symbols found in cached data.")

data = close_df[symbols_in_data]

# 3. Î™®Îëê NaNÏù∏ Ï¢ÖÎ™© Ï†úÍ±∞
data = data.dropna(axis=1, how="all")

# 4. Ï†úÍ±∞Îêú Ìã∞Ïª§ Î°úÍπÖ
removed = [s for s in symbols if s not in data.columns]
for r in removed:
    print(f"‚ö†Ô∏è  Removed due to all NaN: {r}")

# 5. ÏµúÏ¢Ö Í≤ÄÏ¶ù
if data.empty or data.shape[1] == 0:
    raise ValueError("No valid data left after NaN filtering.")

returns = data.pct_change(fill_method="pad").dropna()


# Sharpe Ratio ÏµúÏ†ÅÌôî Ìï®Ïàò
def objective_sharpe(weights):
    port_return = np.dot(weights, returns.mean()) * 252
    port_vol = np.sqrt(np.dot(weights.T, np.dot(returns.cov() * 252, weights)))
    return -port_return / port_vol  # ÏµúÎåÄÌôî ÏúÑÌï¥ ÏùåÏàò


# CVaR ÏµúÏ†ÅÌôî Ìï®Ïàò (5% VaR Í∏∞Ï§Ä)
def objective_cvar(weights):
    portfolio_returns = returns.dot(
        weights
    )  # ÏàòÏ†ï: np.dot(returns, weights)ÎèÑ Í∞ÄÎä•ÌïòÏßÄÎßå DataFrameÏù¥Î©¥ .dotÏù¥ Îçî ÏïàÏ†Ñ
    alpha = 0.05
    var = np.percentile(portfolio_returns, 100 * alpha)
    cvar = portfolio_returns[portfolio_returns <= var].mean()
    return cvar  # minimizeÏóêÏÑú ÏµúÏÜåÌôî(ÏÜêÏã§ ÏµúÎåÄÌôî) ‚Üí Î∂ÄÌò∏ Î∞îÍøîÏïº Ìï®
    # return -cvar  # CVaR ÏµúÎåÄÌôîÌïòÎ†§Î©¥ ÏùåÏàòÎ°ú Î∞òÌôò


# Sortino Ratio ÏµúÏ†ÅÌôî Ìï®Ïàò
def objective_sortino(weights):
    portfolio_returns = returns.dot(
        weights
    )  # ÏàòÏ†ï: np.dot(weights) ‚Üí returns.dot(weights)
    mean_return = portfolio_returns.mean() * 252
    downside_returns = portfolio_returns[portfolio_returns < 0]
    downside_std = downside_returns.std() * np.sqrt(252)
    if downside_std == 0:
        return 0  # ÎòêÎäî ÌÅ∞ Í∞í Î∞òÌôò
    sortino_ratio = mean_return / downside_std
    return -sortino_ratio  # ÏµúÎåÄÌôî ÏúÑÌï¥ ÏùåÏàò


# Î∂ÑÏÇ∞ ÏµúÏÜåÌôî Ìï®Ïàò
def objective_variance(weights):
    return np.dot(weights.T, np.dot(returns.cov() * 252, weights))


# Las restricciones
cons = {"type": "eq", "fun": lambda x: np.sum(x) - 1}

# Los l√≠mites para los pesos
bounds = tuple((0, 1) for x in range(len(symbols)))


# Optimizaci√≥n
init_guess = np.array(
    len(symbols)
    * [
        1.0 / len(symbols),
    ]
)
if optimization_criterion == "sharpe":
    opt_results = minimize(
        objective_sharpe, init_guess, method="SLSQP", bounds=bounds, constraints=cons
    )
elif optimization_criterion == "cvar":
    opt_results = minimize(
        objective_cvar, init_guess, method="SLSQP", bounds=bounds, constraints=cons
    )
elif optimization_criterion == "sortino":
    opt_results = minimize(
        objective_sortino, init_guess, method="SLSQP", bounds=bounds, constraints=cons
    )
elif optimization_criterion == "variance":
    opt_results = minimize(
        objective_variance, init_guess, method="SLSQP", bounds=bounds, constraints=cons
    )

# Los pesos √≥ptimos
optimal_weights = opt_results.x


# Optimizar todos los criterios
opt_results_cvar = minimize(
    objective_cvar, init_guess, method="SLSQP", bounds=bounds, constraints=cons
)
opt_results_sortino = minimize(
    objective_sortino, init_guess, method="SLSQP", bounds=bounds, constraints=cons
)
opt_results_variance = minimize(
    objective_variance, init_guess, method="SLSQP", bounds=bounds, constraints=cons
)
opt_results_sharpe = minimize(
    objective_sharpe, init_guess, method="SLSQP", bounds=bounds, constraints=cons
)

# Pesos √≥ptimos para cada criterio
optimal_weights_cvar = opt_results_cvar.x
optimal_weights_sortino = opt_results_sortino.x
optimal_weights_variance = opt_results_variance.x
optimal_weights_sharpe = opt_results_sharpe.x

# Graficar la frontera eficiente
port_returns = []
port_volatility = []
sharpe_ratio = []
all_weights = []  # almacena los pesos de todas las carteras simuladas

num_assets = len(symbols)
num_portfolios = 50000

np.random.seed(101)

for single_portfolio in range(num_portfolios):
    weights = np.random.random(num_assets)
    weights /= np.sum(weights)
    returns_portfolio = np.dot(weights, returns.mean()) * 252
    volatility = np.sqrt(np.dot(weights.T, np.dot(returns.cov() * 252, weights)))
    sr = returns_portfolio / volatility
    sharpe_ratio.append(sr)
    port_returns.append(returns_portfolio)
    port_volatility.append(volatility)
    all_weights.append(weights)  # registra los pesos para esta cartera

plt.figure(figsize=(12, 8))
plt.scatter(port_volatility, port_returns, c=sharpe_ratio, cmap="viridis")
plt.colorbar(label="Sharpe Ratio")
plt.xlabel("Volatility")
plt.ylabel("Return")

# Calcular y graficar los retornos y la volatilidad del portafolio √≥ptimo para cada criterio
opt_returns_cvar = np.dot(optimal_weights_cvar, returns.mean()) * 252
opt_volatility_cvar = np.sqrt(
    np.dot(optimal_weights_cvar.T, np.dot(returns.cov() * 252, optimal_weights_cvar))
)
opt_portfolio_cvar = plt.scatter(
    opt_volatility_cvar, opt_returns_cvar, color="hotpink", s=50, label="CVaR"
)

opt_returns_sortino = np.dot(optimal_weights_sortino, returns.mean()) * 252
opt_volatility_sortino = np.sqrt(
    np.dot(
        optimal_weights_sortino.T, np.dot(returns.cov() * 252, optimal_weights_sortino)
    )
)
opt_portfolio_sortino = plt.scatter(
    opt_volatility_sortino, opt_returns_sortino, color="g", s=50, label="Sortino"
)

opt_returns_variance = np.dot(optimal_weights_variance, returns.mean()) * 252
opt_volatility_variance = np.sqrt(
    np.dot(
        optimal_weights_variance.T,
        np.dot(returns.cov() * 252, optimal_weights_variance),
    )
)
opt_portfolio_variance = plt.scatter(
    opt_volatility_variance, opt_returns_variance, color="b", s=50, label="Variance"
)

opt_returns_sharpe = np.dot(optimal_weights_sharpe, returns.mean()) * 252
opt_volatility_sharpe = np.sqrt(
    np.dot(
        optimal_weights_sharpe.T, np.dot(returns.cov() * 252, optimal_weights_sharpe)
    )
)
opt_portfolio_sharpe = plt.scatter(
    opt_volatility_sharpe, opt_returns_sharpe, color="r", s=50, label="Sharpe"
)

plt.legend(loc="upper right")

plt.show()


# Funci√≥n para calcular el drawdown m√°ximo
def max_drawdown(return_series):
    comp_ret = (1 + return_series).cumprod()
    peak = comp_ret.expanding(min_periods=1).max()
    dd = (comp_ret / peak) - 1
    return dd.min()


def detailed_portfolio_statistics(weights):
    portfolio_returns = returns.dot(weights)
    mean_return_annualized = gmean(portfolio_returns + 1) ** 252 - 1
    std_dev_annualized = portfolio_returns.std() * np.sqrt(252)
    skewness = skew(portfolio_returns)
    kurt = kurtosis(portfolio_returns)
    max_dd = max_drawdown(portfolio_returns)
    count = len(portfolio_returns)

    # ‚úÖ Safe TNX fetch with fallback
    try:
        tnx = yf.Ticker("^TNX")
        tnx_data = tnx.history(period="1d")
        latest_yield = tnx_data["Close"].iloc[-1]
        risk_free_rate = round(latest_yield / 100.0, 2)
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to fetch TNX: {e}")
        risk_free_rate = 0.04  # default 4% fallback

    sharpe_ratio = (mean_return_annualized - risk_free_rate) / std_dev_annualized

    # CVaR Í≥ÑÏÇ∞ (5% ÏàòÏ§Ä)
    alpha = 0.05
    sorted_returns = np.sort(portfolio_returns)
    var_index = int(np.floor(alpha * len(sorted_returns)))
    var = sorted_returns[var_index]
    cvar = sorted_returns[:var_index].mean()
    cvar_annualized = (1 + cvar) ** 252 - 1  # Ïó∞Ïú®Ìôî

    downside_returns = portfolio_returns[portfolio_returns < 0]
    downside_std_dev = downside_returns.std() * np.sqrt(252)
    sortino_ratio = (
        mean_return_annualized / downside_std_dev if downside_std_dev != 0 else np.nan
    )
    variance = std_dev_annualized**2

    return (
        mean_return_annualized,
        std_dev_annualized,
        skewness,
        kurt,
        max_dd,
        count,
        sharpe_ratio,
        cvar_annualized,
        sortino_ratio,
        variance,
    )


# Calcular estad√≠sticas para cada portafolio
statistics_cvar = detailed_portfolio_statistics(optimal_weights_cvar)
statistics_sortino = detailed_portfolio_statistics(optimal_weights_sortino)
statistics_variance = detailed_portfolio_statistics(optimal_weights_variance)
statistics_sharpe = detailed_portfolio_statistics(optimal_weights_sharpe)

# Nombres de las estad√≠sticas
statistics_names = [
    "Retorno anualizado",
    "Volatilidad anualizada",
    "Skewness",
    "Kurtosis",
    "Max Drawdown",
    "Conteo de datos",
    "Sharpe Ratio",
    "CVaR",
    "Ratio Sortino",
    "Varianza",
]

# Diccionario que asocia los nombres de los m√©todos de optimizaci√≥n con los pesos √≥ptimos y las estad√≠sticas
portfolio_data = {
    "CVaR": {
        "weights": optimal_weights_cvar,
        "statistics": detailed_portfolio_statistics(optimal_weights_cvar),
    },
    "Sortino": {
        "weights": optimal_weights_sortino,
        "statistics": detailed_portfolio_statistics(optimal_weights_sortino),
    },
    "Variance": {
        "weights": optimal_weights_variance,
        "statistics": detailed_portfolio_statistics(optimal_weights_variance),
    },
    "Sharpe": {
        "weights": optimal_weights_sharpe,
        "statistics": detailed_portfolio_statistics(optimal_weights_sharpe),
    },
}

# 1. Ìè¨Ìä∏Ìè¥Î¶¨Ïò§ ÎπÑÏ§ë Ìëú (Í∞Å Î∞©Î≤ïÎ≥Ñ, Ìã∞Ïª§Î≥Ñ ÎπÑÏ§ë)
weight_rows = []
for method, data in portfolio_data.items():
    for symbol, weight in zip(symbols, data["weights"]):
        weight_rows.append(
            {"ÏµúÏ†ÅÌôî Í∏∞Ï§Ä": method, "Ìã∞Ïª§": symbol, "ÎπÑÏ§ë(%)": round(weight * 100, 2)}
        )
df_weights = pd.DataFrame(weight_rows)

# 2. Ìè¨Ìä∏Ìè¥Î¶¨Ïò§ ÌÜµÍ≥Ñ Ìëú (Í∞Å Î∞©Î≤ïÎ≥Ñ ÌÜµÍ≥Ñ Ìïú Ï§Ñ)
statistics_names_kr = [
    "Ïó∞ÌôòÏÇ∞ ÏàòÏùµÎ•†",
    "Ïó∞ÌôòÏÇ∞ Î≥ÄÎèôÏÑ±",
    "ÏôúÎèÑ",
    "Ï≤®ÎèÑ",
    "ÏµúÎåÄ ÎÇôÌè≠",
    "Îç∞Ïù¥ÌÑ∞ Í∞úÏàò",
    "ÏÉ§ÌîÑ ÎπÑÏú®",
    "CVaR",
    "ÏÜåÎ•¥Ìã∞ÎÖ∏ ÎπÑÏú®",
    "Î∂ÑÏÇ∞",
]
stats_rows = []
for method, data in portfolio_data.items():
    stats_dict = {"ÏµúÏ†ÅÌôî Í∏∞Ï§Ä": method}
    for name_kr, stat in zip(statistics_names_kr, data["statistics"]):
        # Ïà´ÏûêÎäî Î™®Îëê ÏÜåÏàòÏ†ê ÎëòÏß∏ÏûêÎ¶¨Î°ú Î∞òÏò¨Î¶º, Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî Ï†ïÏàòÎ°ú
        if name_kr == "Îç∞Ïù¥ÌÑ∞ Í∞úÏàò":
            stats_dict[name_kr] = int(stat)
        else:
            stats_dict[name_kr] = np.round(stat, 2)
    stats_rows.append(stats_dict)
df_stats = pd.DataFrame(stats_rows)


def autofit_columns_and_wrap(ws, df: pd.DataFrame, workbook):
    # ÌîΩÏÖÄ -> Î¨∏Ïûê Ïàò ÌôòÏÇ∞ (0.1428 Î∞∞Ïú® Í∏∞Ï§Ä)
    pixel_widths = [92, 200, 50, 500, 85, 150]
    char_widths = [round(p * 0.1428) for p in pixel_widths]

    # wrap + top-align Ìè¨Îß∑
    wrap_format = workbook.add_format({"text_wrap": True, "valign": "top"})

    # Ìó§Îçî ÏûëÏÑ± Î∞è Ïó¥ ÎÑàÎπÑ ÏÑ§Ï†ï
    for i, col in enumerate(df.columns):
        width = char_widths[i] if i < len(char_widths) else 20
        ws.set_column(i, i, width)
        ws.write(0, i, str(col), wrap_format)

    # Îç∞Ïù¥ÌÑ∞ ÏÖÄ ÏûëÏÑ±
    for row in range(1, len(df) + 1):
        for col in range(len(df.columns)):
            val = df.iat[row - 1, col]

            # NaN / inf / None -> Î¨∏ÏûêÏó¥ Î≥ÄÌôò
            if isinstance(val, float):
                if math.isnan(val) or math.isinf(val):
                    val = str(val)
            elif val is None:
                val = ""

            # Excel Ïì∞Í∏∞ Ïã§Ìå® ÎåÄÎπÑ ÏïàÏ†Ñ write
            try:
                ws.write(row, col, val, wrap_format)
            except Exception:
                ws.write(row, col, str(val), wrap_format)


def autofit_columns_and_wrap_moat(ws, df: pd.DataFrame, workbook):

    # Ïó¥ ÎÑàÎπÑ ÏÑ§Ï†ï (ÌîΩÏÖÄ Í∏∞Ï§Ä ‚Üí Î¨∏Ïûê Í∏∞Ï§ÄÏúºÎ°ú Î≥ÄÌôò)
    pixel_widths = [92, 500]
    char_widths = [round(p * 0.1428) for p in pixel_widths]  # = [13, 71]

    # wrap + top-align Ìè¨Îß∑
    wrap_format = workbook.add_format({"text_wrap": True, "valign": "top"})

    # Ïó¥ ÎÑàÎπÑ Î∞è Ìó§Îçî ÏÑ§Ï†ï
    for i, col in enumerate(df.columns):
        width = char_widths[i] if i < len(char_widths) else 20
        ws.set_column(i, i, width)
        ws.write(0, i, str(col), wrap_format)

    # Îç∞Ïù¥ÌÑ∞ ÏÖÄ ÏûëÏÑ±
    for row in range(1, len(df) + 1):
        for col in range(len(df.columns)):
            val = df.iat[row - 1, col]

            # NaN / inf / None Ï≤òÎ¶¨
            if isinstance(val, float):
                if math.isnan(val) or math.isinf(val):
                    val = str(val)
            elif val is None:
                val = ""

            try:
                ws.write(row, col, val, wrap_format)
            except Exception:
                ws.write(row, col, str(val), wrap_format)


with pd.ExcelWriter(excel_path, engine="xlsxwriter") as writer:

    # Ï¢ÖÎ™©Î∂ÑÏÑù ÏãúÌä∏ Î®ºÏ†Ä ÏÉùÏÑ±Ìï¥Ïïº Ìï®
    df.to_excel(
        writer, index=False, sheet_name="Ï¢ÖÎ™©Î∂ÑÏÑù"
    )  # df_analysisÎäî Ï¢ÖÎ™©Î∂ÑÏÑù Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ

    # Í≤ΩÏüÅÏö∞ÏúÑ(Moat) ÏãúÌä∏ Ï†ÄÏû• Î∞è Ìëú Ï†ÅÏö©
    moat_df.to_excel(writer, index=False, sheet_name="Í≤ΩÏüÅÏö∞ÏúÑÎ∂ÑÏÑù")
    ws_moat = writer.sheets["Í≤ΩÏüÅÏö∞ÏúÑÎ∂ÑÏÑù"]
    (mr_moat, mc_moat) = moat_df.shape
    ws_moat.add_table(
        0,
        0,
        mr_moat,
        mc_moat - 1,
        {
            "columns": [{"header": col} for col in moat_df.columns],
            "style": "Table Style Medium 9",
        },
    )
    autofit_columns_and_wrap_moat(ws_moat, moat_df, writer.book)

    # Í∏∞Ï°¥ Ìè¨Ìä∏Ìè¥Î¶¨Ïò§ÎπÑÏ§ë ÏãúÌä∏ ÎåÄÏã† Í∞Å Í∏∞Ï§ÄÎ≥ÑÎ°ú ÎÇòÎà†ÏÑú Ï†ÄÏû• (ÏóëÏÖÄ ÌëúÎ°ú)
    for method in ["CVaR", "Sortino", "Variance", "Sharpe"]:
        df_method = df_weights[df_weights["ÏµúÏ†ÅÌôî Í∏∞Ï§Ä"] == method]

        df_method = df_method[df_method["ÎπÑÏ§ë(%)"] != 0]

        df_method.to_excel(writer, index=False, sheet_name=f"Ìè¨Ìä∏ÎπÑÏ§ë_{method}")
        ws = writer.sheets[f"Ìè¨Ìä∏ÎπÑÏ§ë_{method}"]
        (mr, mc) = df_method.shape
        ws.add_table(
            0,
            0,
            mr,
            mc - 1,
            {
                "columns": [{"header": col} for col in df_method.columns],
                "style": "Table Style Medium 9",
            },
        )

    # Ìè¨Ìä∏Ìè¥Î¶¨Ïò§ÌÜµÍ≥Ñ ÏãúÌä∏ÎèÑ ÏóëÏÖÄ ÌëúÎ°ú
    df_stats.to_excel(writer, index=False, sheet_name="Ìè¨Ìä∏Ìè¥Î¶¨Ïò§ÌÜµÍ≥Ñ")
    ws_stats = writer.sheets["Ìè¨Ìä∏Ìè¥Î¶¨Ïò§ÌÜµÍ≥Ñ"]
    (mr_stats, mc_stats) = df_stats.shape
    ws_stats.add_table(
        0,
        0,
        mr_stats,
        mc_stats - 1,
        {
            "columns": [{"header": col} for col in df_stats.columns],
            "style": "Table Style Medium 9",
        },
    )

    # Îâ¥Ïä§ Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ ÏãúÌä∏ ÏÉùÏÑ± Î∞è Ìëú Ï†ÅÏö©
    news_df.to_excel(writer, index=False, sheet_name="Ï¢ÖÎ™©Îâ¥Ïä§")
    ws_news = writer.sheets["Ï¢ÖÎ™©Îâ¥Ïä§"]
    (nr, nc) = news_df.shape
    ws_news.add_table(
        0,
        0,
        nr,
        nc - 1,
        {
            "columns": [{"header": col} for col in news_df.columns],
            "style": "Table Style Medium 9",
        },
    )
    autofit_columns_and_wrap(ws_news, news_df, writer.book)

    workbook = writer.book
    # 1) dfÎ°ú ÌÜµÏùº
    worksheet = writer.sheets["Ï¢ÖÎ™©Î∂ÑÏÑù"]

    currency_format = workbook.add_format({"num_format": "$#,##.00"})

    # 4Ô∏è‚É£ "ÌòÑÏû¨Í∞Ä" Ïª¨Îüº ÏúÑÏπò Íµ¨Ìï¥ÏÑú ÏÑúÏãù Ï†ÅÏö©
    price_col_idx = df.columns.get_loc("ÌòÑÏû¨Í∞Ä")  # 0Î∂ÄÌÑ∞ ÏãúÏûëÌïòÎäî Ïù∏Îç±Ïä§
    for row in range(1, len(df) + 1):  # Ìó§Îçî Ï†úÏô∏, 1Î∂ÄÌÑ∞ ÏãúÏûë
        value = df.at[row - 1, "ÌòÑÏû¨Í∞Ä"]
        if isinstance(value, float) and (math.isnan(value) or math.isinf(value)):
            value = 0
        worksheet.write_number(row, price_col_idx, value, currency_format)

    start_row = 0  # data starts after header row 1
    end_row = len(df)
    start_col = 0
    end_col = len(df.columns) - 1

    def xl_col(col_idx):
        div = col_idx + 1
        string = ""
        while div > 0:
            div, mod = div // 26, div % 26
            if mod == 0:
                mod = 26
                div -= 1
            string = chr(64 + mod) + string
        return string

    first_cell = f"{xl_col(start_col)}{start_row + 1}"
    last_cell = f"{xl_col(end_col)}{end_row + 1}"
    data_range = f"{first_cell}:{last_cell}"

    # 1) Add Excel table for the data
    worksheet.add_table(
        data_range,
        {
            "columns": [{"header": col} for col in df.columns],
            "style": "Table Style Medium 9",
        },
    )

    # 5) Ïª¨ÎüºÎ≥Ñ ÎÑàÎπÑ ÏßÄÏ†ï
    col_widths = {
        "Ìã∞Ïª§": 6,
        "Ï¢ÖÎ™©": 25,
        "ÏóÖÏ¢Ö": 25,
        "ÌòÑÏû¨Í∞Ä": 10,
        "1Í∞úÏõîÎåÄÎπÑ": 10,
    }
    for col_name, width in col_widths.items():
        if col_name in df.columns:
            col_idx = df.columns.get_loc(col_name)
            worksheet.set_column(col_idx, col_idx, width)

    # 6) Í∑∏ÎùºÎç∞Ïù¥ÏÖò Ìè¨Îß∑ÌåÖ Ï†ÅÏö© (Ï¥ùÏ†êÏàò Ïª¨Îüº)
    total_score_col_idx = df.columns.get_loc("Ï¥ùÏ†êÏàò")
    total_score_col_letter = xl_col(total_score_col_idx)
    total_score_range = (
        f"{total_score_col_letter}{start_row + 1}:{total_score_col_letter}{end_row + 1}"
    )

    worksheet.conditional_format(
        total_score_range,
        {
            "type": "3_color_scale",
            "min_type": "min",
            "mid_type": "percentile",
            "mid_value": 50,
            "max_type": "max",
            "min_color": "#FF0000",
            "mid_color": "#FFFF00",
            "max_color": "#00FF00",
        },
    )


def generate_prompt(df_news: pd.DataFrame) -> str:

    news_summary = []
    if {"Í∏∞ÏóÖÎ™Ö", "Í∞êÏ†ïÏßÄÏàò", "Îâ¥Ïä§ ÏöîÏïΩ"}.issubset(df_news.columns):
        grouped = df_news.groupby("Í∏∞ÏóÖÎ™Ö")
        for comp, group in grouped:
            avg_sent = group["Í∞êÏ†ïÏßÄÏàò"].mean()
            recent_summaries = (
                group.sort_values(by="Î∞úÌñâÏùº", ascending=False)["Îâ¥Ïä§ ÏöîÏïΩ"]
                .head(3)
                .tolist()
            )
            summaries_text = " / ".join([s for s in recent_summaries if s])
            news_summary.append(
                f"{comp}: ÌèâÍ∑† Í∞êÏ†ïÏßÄÏàò {avg_sent:.2f}, ÏµúÍ∑º Îâ¥Ïä§ ÏöîÏïΩ: {summaries_text}"
            )

    prompt = f"""
ÎãπÏã†ÏùÄ Í∏∞ÏóÖ Î∂ÑÏÑùÍ≥º Í±∞ÏãúÍ≤ΩÏ†ú Î∂ÑÏÑùÏóê Îä•ÏàôÌïú Ï†ÑÎ¨∏ Ï£ºÏãù Î∂ÑÏÑùÍ∞ÄÏûÖÎãàÎã§.
Ìï≠ÏÉÅ ÌïúÍµ≠Ïñ¥Î°ú ÏùëÎãµÌï¥ Ï£ºÏÑ∏Ïöî.

Îã§ÏùåÏùÄ {date_kr_ymd} Í∏∞Ï§ÄÏúºÎ°ú ÏàòÏßëÎêú {limit}Í∞ú Í∏∞ÏóÖÏùò Îâ¥Ïä§ ÏöîÏïΩÍ≥º Í∞êÏ†ï Î∂ÑÏÑù ÏßÄÏàòÏûÖÎãàÎã§.  
---

üìå Îâ¥Ïä§ ÏöîÏïΩ Î∞è Í∞êÏ†ï ÏßÄÏàò:
{chr(10).join(news_summary)}

---

### Î∂ÑÏÑù ÏöîÏ≤≠:

1. {date_kr_ymd} Í∏∞Ï§Ä Ïù¥Î≤à Ï£º Ï£ºÎ™©Ìï† ÎßåÌïú Í∏∞ÏóÖ Îâ¥Ïä§ (3~5Í∞ú)  
- Î∞òÎìúÏãú **ÏúÑ Îâ¥Ïä§ ÏöîÏïΩÏóêÏÑú Ïñ∏Í∏âÎêú Í∏∞ÏóÖ Î∞è ÎÇ¥Ïö©Îßå ÏÇ¨Ïö©**Ìï¥ Ï£ºÏÑ∏Ïöî.  
- Í∏∞ÏóÖÎ™ÖÍ≥º ÌïµÏã¨ Îâ¥Ïä§, Í∑∏Î¶¨Í≥† **Ìà¨Ïûê Í¥ÄÏ†êÏóêÏÑúÏùò ÏùòÎØ∏**Î•º Í∞ÑÍ≤∞Ìûà ÏöîÏïΩÌï¥ Ï£ºÏÑ∏Ïöî.  

**ÏòàÏãú ÌòïÏãù:**  
- ÏóîÎπÑÎîîÏïÑ: 2Î∂ÑÍ∏∞ Ïã§Ï†Å ÏòàÏÉÅ ÏÉÅÌöå. Î∞òÎèÑÏ≤¥ ÏóÖÌô© ÌöåÎ≥µ Í∏∞ÎåÄÍ∞ê Î∞òÏòÅ.

2. {date_kr_ymd} Í∏∞Ï§Ä Í±∞ÏãúÍ≤ΩÏ†ú ÌôòÍ≤Ω ÏöîÏïΩ  
- Í¥ÄÏÑ∏, Í∏àÎ¶¨, Ïù∏ÌîåÎ†àÏù¥ÏÖò, Í≥†Ïö©, ÏÜåÎπÑ, Ïõê-Îã¨Îü¨ ÌôòÏú® Îì± Ï£ºÏöî ÏßÄÌëúÎ•º Í∏∞Î∞òÏúºÎ°ú Í∞ÑÍ≤∞Ìûà Ï†ïÎ¶¨Ìï¥ Ï£ºÏÑ∏Ïöî.  
- Ïà´ÏûêÎÇò Î∞©Ìñ•ÏÑ± ÏúÑÏ£ºÎ°ú ÏûëÏÑ±Ìï¥ Ï£ºÏÑ∏Ïöî.
- Î∞òÎìúÏãú Í≤ÄÏÉâ Ï†ïÎ≥¥Î•º Î∞îÌÉïÏúºÎ°ú ÏûëÏÑ±Ìï¥ Ï£ºÏÑ∏Ïöî.

3. ÎØ∏Íµ≠ Ï¶ùÏãúÏóê ÎåÄÌïú ÏòÅÌñ• Î∂ÑÏÑù  
- ÏúÑ Í±∞ÏãúÍ≤ΩÏ†ú ÌôòÍ≤ΩÏù¥ **ÎØ∏Íµ≠ Ï¶ùÏãúÏóê ÎØ∏ÏπòÎäî ÏòÅÌñ•**ÏùÑ Í∞ÑÍ≤∞Ìûà ÏÑ§Î™ÖÌï¥ Ï£ºÏÑ∏Ïöî.  
- Í∏àÎ¶¨ Î∞©Ìñ•ÏÑ±, Í∏∞Ïà†Ï£º/Í∞ÄÏπòÏ£º ÏÑ†Ìò∏, Ìà¨ÏûêÏûê Ïã¨Î¶¨ Î≥ÄÌôî Îì±ÏùÑ Ï§ëÏã¨ÏúºÎ°ú ÏöîÏïΩÌï¥ Ï£ºÏÑ∏Ïöî.
- Î∞òÎìúÏãú Í≤ÄÏÉâ Ï†ïÎ≥¥Î•º Î∞îÌÉïÏúºÎ°ú ÏûëÏÑ±Ìï¥ Ï£ºÏÑ∏Ïöî.
"""

    return prompt.strip()


##########################################################################################################


def main(df_news):
    prompt = generate_prompt(df_news)
    print("Prompt sent to Gemini:\n", prompt)

    answer = query_gemini(prompt)
    return answer


answer = main(news_df)

#########################################################################################################

msg = EmailMessage()
msg["Subject"] = f"DeepFund Weekly Insights | {date_kr}"
msg["From"] = Address(display_name="Hyungsuk Choi", addr_spec=EMAIL)
msg["To"] = ""  # or '' or a single address to satisfy the 'To' header requirement

content = (
    f"Í∑ÄÌïòÏùò Ï§ëÏû•Í∏∞ Ìà¨Ïûê Ï∞∏Í≥†Î•º ÏúÑÌï¥ {date_kr} Í∏∞Ï§Ä, "
    f"ÏãúÍ∞ÄÏ¥ùÏï° ÏÉÅÏúÑ {limit}Í∞ú ÏÉÅÏû•Í∏∞ÏóÖÏóê ÎåÄÌïú ÏµúÏã† ÌÄÄÌä∏ Î∂ÑÏÑù ÏûêÎ£åÎ•º Ï†ÑÎã¨ÎìúÎ¶ΩÎãàÎã§. "
    "Í∞Å Í∏∞ÏóÖÏùò Ï¥ùÏ†êÏàòÎäî Î∞∏Î•òÏóêÏù¥ÏÖò Ï†êÏàò, Ïã§Ï†ÅÎ™®Î©òÌÖÄ Ï†êÏàò, Í∑∏Î¶¨Í≥† Í∞ÄÍ≤©/ÏàòÍ∏â Ï†êÏàòÎ•º Î∞òÏòÅÌïòÏòÄÏäµÎãàÎã§.\n\n"
    "Î≥∏ ÏûêÎ£åÎäî ÏõåÎü∞ Î≤ÑÌïèÏùò Ìà¨Ïûê Ï≤†ÌïôÏùÑ Í∏∞Î∞òÏúºÎ°ú, "
    "Í∏∞ÏóÖÏùò Ïû¨Î¨¥ Í±¥Ï†ÑÏÑ± Î∞è Ïã§Ï†ÅÏùÑ ÏàòÏπòÌôîÌïòÏó¨ ÌèâÍ∞ÄÌïú Í≤∞Í≥ºÏûÖÎãàÎã§. "
    "Ìà¨Ïûê ÌåêÎã® ÏãúÏóêÎäî Ï†ïÏÑ±Ï†Å ÏöîÏÜåÏóê ÎåÄÌïú Î≥ÑÎèÑÏùò Î©¥Î∞ÄÌïú Í≤ÄÌÜ†ÎèÑ "
    "Ìï®Íªò Î≥ëÌñâÌïòÏãúÍ∏∞Î•º Í∂åÏû•ÎìúÎ¶ΩÎãàÎã§.\n\n"
    "üìåÏ£ºÏöî Ïû¨Î¨¥ÏßÄÌëú Ìï¥ÏÑ§\n"
    "D/E Î∂ÄÏ±ÑÎπÑÏú® (Debt to Equity): ÏûêÎ≥∏ ÎåÄÎπÑ Î∂ÄÏ±ÑÏùò ÎπÑÏú®Î°ú, Ïû¨Î¨¥ Í±¥Ï†ÑÏÑ±ÏùÑ ÎÇòÌÉÄÎÉÖÎãàÎã§. ÎÇÆÏùÑÏàòÎ°ù ÏïàÏ†ïÏ†ÅÏûÖÎãàÎã§.\n"
    "CR Ïú†ÎèôÎπÑÏú® (Current Ratio): Ïú†ÎèôÏûêÏÇ∞Ïù¥ Ïú†ÎèôÎ∂ÄÏ±ÑÎ•º ÏñºÎßàÎÇò Ïª§Î≤ÑÌï† Ïàò ÏûàÎäîÏßÄÎ•º Î≥¥Ïó¨Ï§çÎãàÎã§.\n"
    "PBR Ï£ºÍ∞ÄÏàúÏûêÏÇ∞ÎπÑÏú® (Price to Book Ratio): Ï£ºÍ∞ÄÍ∞Ä Ïû•Î∂ÄÍ∞ÄÏπò ÎåÄÎπÑ ÏñºÎßàÎÇò ÎÜíÏùÄÏßÄÎ•º ÎÇòÌÉÄÎÇ¥Î©∞, 1Î≥¥Îã§ ÎÇÆÏúºÎ©¥ Ï†ÄÌèâÍ∞ÄÎ°ú Ìï¥ÏÑùÎêòÍ∏∞ÎèÑ Ìï©ÎãàÎã§.\n"
    "PER Ï£ºÍ∞ÄÏàòÏùµÎπÑÏú® (Price to Earnings Ratio): Ïù¥Ïùµ ÎåÄÎπÑ Ï£ºÍ∞Ä ÏàòÏ§ÄÏùÑ ÎÇòÌÉÄÎÉÖÎãàÎã§. ÎÇÆÏùÑÏàòÎ°ù Ïù¥Ïùµ ÎåÄÎπÑ Ï†ÄÎ†¥Ìïú Í∏∞ÏóÖÏûÖÎãàÎã§. ÎÜíÏùÑÏàòÎ°ù ÏãúÏû•Ïùò Í∏∞ÎåÄÏπòÍ∞Ä ÎÜíÏäµÎãàÎã§.\n"
    "ROE ÏûêÍ∏∞ÏûêÎ≥∏Ïù¥ÏùµÎ•† (Return on Equity): ÏûêÎ≥∏ÏùÑ(Î∂ÄÏ±Ñ ÎØ∏Ìè¨Ìï®) ÏñºÎßàÎÇò Ìö®Ïú®Ï†ÅÏúºÎ°ú Ïö¥Ïö©Ìï¥ Ïù¥ÏùµÏùÑ ÎÉàÎäîÏßÄÎ•º ÎÇòÌÉÄÎÉÖÎãàÎã§.\n"
    "ROA Ï¥ùÏûêÏÇ∞Ïù¥ÏùµÎ•† (Return on Assets): Ï¥ùÏûêÏÇ∞(Î∂ÄÏ±Ñ Ìè¨Ìï®) ÎåÄÎπÑ ÏàòÏùµÎ•†Î°ú, Î≥¥ÏàòÏ†ÅÏù∏ ÏàòÏùµÏÑ± ÏßÄÌëúÏûÖÎãàÎã§.\n"
    "ICR Ïù¥ÏûêÎ≥¥ÏÉÅÎπÑÏú® (Interest Coverage Ratio): ÏòÅÏóÖÏù¥ÏùµÏúºÎ°ú Ïù¥ÏûêÎπÑÏö©ÏùÑ ÏñºÎßàÎÇò Í∞êÎãπÌï† Ïàò ÏûàÎäîÏßÄ ÎÇòÌÉÄÎÉÖÎãàÎã§.\n"
    "EPS Ï£ºÎãπÏàúÏù¥Ïùµ (Earnings Per Share): ÏµúÍ∑º 5ÎÖÑÍ∞Ñ 1Ï£ºÎãπ Í∏∞ÏóÖÏù¥ Ï∞ΩÏ∂úÌïú ÏàúÏù¥ÏùµÏùò ÏÑ±Ïû•Î•†Î°ú, ÏàòÏùµÏÑ±Í≥º ÏÑ±Ïû•ÏÑ± ÌåêÎã®Ïóê Ïú†Ïö©Ìï©ÎãàÎã§.\n"
    "Î∞∞ÎãπÏÑ±Ïû•Î•†: ÏµúÍ∑º 10ÎÖÑÍ∞Ñ Î∞∞ÎãπÍ∏àÏùò ÏÑ±Ïû•Î•†ÏùÑ ÎÇòÌÉÄÎÇ¥Îäî ÏßÄÌëúÏûÖÎãàÎã§.\n"
    "ÏòÅÏóÖÏù¥ÏùµÎ•†: ÏµúÍ∑º 5Í∞ú ÏòÅÏóÖÎÖÑÎèÑ/Î∂ÑÍ∏∞Ïùò ÌèâÍ∑† ÏòÅÏóÖÏù¥ÏùµÎ•† ÏÑ±Ïû•Î•†Î°ú, Í∏∞ÏóÖÏùò ÏàòÏùµÏÑ± ÏàòÏ§ÄÏùÑ Î≥¥Ïó¨Ï§çÎãàÎã§.\n"
    "Î™®Î©òÌÖÄ: Ï£ºÍ∞ÄÏùò Ï§ëÏû•Í∏∞ ÏÉÅÏäπ ÌùêÎ¶ÑÏùÑ Î∞òÏòÅÌïú ÏßÄÌëúÎ°ú, Ï£ºÍ∞ÄÏùò ÌÉÑÎ†•Í≥º Ï∂îÏÑ∏Î•º ÌèâÍ∞ÄÌï©ÎãàÎã§.\n\n"
    "Ìï¥Îãπ Î©îÏùºÏùÄ Îß§Ï£º ÌèâÏùº Ïò§ÌõÑ 5ÏãúÏóê ÏûêÎèô Î∞úÏÜ°ÎêòÎ©∞, ÏïàÏ†ïÏ†ÅÏù¥Í≥† ÌòÑÎ™ÖÌïú Ìà¨ÏûêÎ•º ÏúÑÌïú Ï∞∏Í≥† ÏûêÎ£åÎ°ú Ï†úÍ≥µÎê©ÎãàÎã§.\n\n"
    "Í∑ÄÌïòÏùò ÏÑ±Í≥µÏ†ÅÏù∏ Ìà¨ÏûêÎ•º ÏùëÏõêÌï©ÎãàÎã§."
)

msg.set_content(content)
html_content = f"""
<html>
  <body>

    <p><strong>ÏßÄÍ∏à Î¨¥Î£å Íµ¨ÎèÖÌïòÍ≥† AI Ìà¨Ïûê Ïù∏ÏÇ¨Ïù¥Ìä∏Î•º Îß§Ï£º Î∞õÏïÑÎ≥¥ÏÑ∏Ïöî:</strong> <a href="https://portfolio-production-54cf.up.railway.app/" target="_blank">Íµ¨ÎèÖÌïòÎü¨ Í∞ÄÍ∏∞</a></p>
    
    <p>Í∑ÄÌïòÏùò Ï§ëÏû•Í∏∞ Ìà¨Ïûê Ï∞∏Í≥†Î•º ÏúÑÌï¥ <b>{date_kr}</b> Í∏∞Ï§Ä, 
    ÏãúÍ∞ÄÏ¥ùÏï° ÏÉÅÏúÑ <b>{limit}</b>Í∞ú, Îâ¥ÏöïÏ¶ùÍ∂åÍ±∞ÎûòÏÜå(NYSE), ÎÇòÏä§Îã•(NASDAQ), ÏïÑÎ©ïÏä§(AMEX)Ïóê ÏÉÅÏû•Îêú Í∏∞ÏóÖÎì§Ïùò ÏµúÏã† ÌÄÄÌä∏ Îç∞Ïù¥ÌÑ∞Î•º Ï†ÑÎã¨ÎìúÎ¶ΩÎãàÎã§.</p>

    <p>Í∞Å Í∏∞ÏóÖÏùò Ï¥ùÏ†êÏàòÎäî Î∞∏Î•òÏóêÏù¥ÏÖò Ï†êÏàò, Ïã§Ï†ÅÎ™®Î©òÌÖÄ Ï†êÏàò, Í∞ÄÍ≤©/ÏàòÍ∏â Ï†êÏàò, Í∑∏Î¶¨Í≥† Í≤ΩÏüÅ Ïö∞ÏúÑÏùò ÏßÄÏÜç Í∞ÄÎä•ÏÑ±ÏùÑ Î∞òÏòÅÌïòÏòÄÏäµÎãàÎã§. ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ ÏïÑÎûò Ìï¥ÏÑ§ÏùÑ Ï∞∏Í≥†Ìï¥Ï£ºÏãúÍ∏∞ Î∞îÎûçÎãàÎã§.</p>

    <h3 style="margin-top: 30px;"><strong>{date_kr} AI ÏÑ†Ï†ï Ï£ºÏöî Îâ¥Ïä§ Î∞è Í±∞ÏãúÍ≤ΩÏ†ú Î∂ÑÏÑù</strong></h3>

    {markdown.markdown(answer)}

    <h3>üìå Ï£ºÏöî Ïû¨Î¨¥ÏßÄÌëú Ìï¥ÏÑ§</h3>
    <table border="1" cellpadding="8" cellspacing="0" style="border-collapse: collapse; font-family: sans-serif;">
      <thead style="background-color: #f2f2f2;">
        <tr>
          <th>ÏßÄÌëú</th>
          <th>ÌïúÍ∏ÄÎ™Ö</th>
          <th>ÏÑ§Î™Ö</th>
        </tr>
      </thead>
      <tbody>
        <tr><td><b>FCF</b></td><td>ÏûêÏú†ÌòÑÍ∏àÌùêÎ¶Ñ</td><td>Í∏∞ÏóÖÏù¥ ÏòÅÏóÖÌôúÎèôÏùÑ ÌÜµÌï¥ Î≤åÏñ¥Îì§Ïù∏ ÌòÑÍ∏àÏóêÏÑú ÏÑ§ÎπÑ Ìà¨Ïûê Îì± ÏÇ¨ÏóÖ Ïú†ÏßÄÎ•º ÏúÑÌï¥ ÏßÄÏ∂úÌïú ÏûêÍ∏àÏùÑ Ï†úÏô∏Ìïú ÌõÑ, Ïã§Ï†úÎ°ú Í∏∞ÏóÖÏù¥ ÏûêÏú†Î°≠Í≤å ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎäî ÌòÑÍ∏àÏûÖÎãàÎã§. Ïù¥ ÌòÑÍ∏àÏùÄ Ïã†Í∑ú Ìà¨Ïûê Îì± Îã§ÏñëÌïú Ïö©ÎèÑÎ°ú ÌôúÏö©Îê† Ïàò ÏûàÏäµÎãàÎã§.</td></tr>
        <tr><td><b>Ï∂îÏ†ïDCFÎ≤îÏúÑ</b></td><td>Ìï†Ïù∏Îêú ÌòÑÍ∏àÌùêÎ¶Ñ</td><td>ÎØ∏Îûò ÏòàÏÉÅ ÏûêÏú†ÌòÑÍ∏àÌùêÎ¶Ñ(FCF)ÏùÑ Î≥¥ÏàòÏ†ÅÏù∏ Ìï†Ïù∏Ïú®Î°ú ÌòÑÏû¨ Í∞ÄÏπòÎ°ú ÌôòÏÇ∞ÌïòÏó¨ ÏÇ∞Ï∂úÌïú Í∏∞ÏóÖÏùò ÎÇ¥Ïû¨Í∞ÄÏπòÏûÖÎãàÎã§. Î≥∏ ÎÇ¥Ïû¨Í∞ÄÏπòÎäî Î™¨ÌÖåÏπ¥Î•ºÎ°ú ÏãúÎÆ¨Î†àÏù¥ÏÖòÏùÑ ÌÜµÌï¥ Ïó¨Îü¨ ÏÑ±Ïû• ÏãúÎÇòÎ¶¨Ïò§Î•º Í≥†Î†§ÌïòÎ©∞, 95% Ïã†Î¢∞Íµ¨Í∞Ñ Î≤îÏúÑ ÎÇ¥ÏóêÏÑú ÎÇ¥Ïû¨Í∞ÄÏπò Î≥ÄÎèôÏÑ±ÏùÑ ÌèâÍ∞ÄÌïòÏó¨ Í∏∞ÏóÖÏùò Ï†ÄÌèâÍ∞Ä Ïó¨Î∂ÄÎ•º Î≥¥Îã§ Ï†ïÎ∞ÄÌïòÍ≤å ÌåêÎã®Ìï©ÎãàÎã§.</td></tr>
        <tr><td><b>D/E</b></td><td>Î∂ÄÏ±ÑÎπÑÏú®</td><td>ÏûêÎ≥∏ ÎåÄÎπÑ Î∂ÄÏ±ÑÏùò ÎπÑÏú®Î°ú, Ïû¨Î¨¥ Í±¥Ï†ÑÏÑ±ÏùÑ ÎÇòÌÉÄÎÉÖÎãàÎã§. ÎÇÆÏùÑÏàòÎ°ù ÏïàÏ†ïÏ†ÅÏûÖÎãàÎã§.</td></tr>
        <tr><td><b>CR</b></td><td>Ïú†ÎèôÎπÑÏú®</td><td>Ïú†ÎèôÏûêÏÇ∞Ïù¥ Ïú†ÎèôÎ∂ÄÏ±ÑÎ•º ÏñºÎßàÎÇò Ïª§Î≤ÑÌï† Ïàò ÏûàÎäîÏßÄÎ•º Î≥¥Ïó¨Ï§çÎãàÎã§.</td></tr>
        <tr><td><b>PBR</b></td><td>Ï£ºÍ∞ÄÏàúÏûêÏÇ∞ÎπÑÏú®</td><td>Ï£ºÍ∞ÄÍ∞Ä Ïû•Î∂ÄÍ∞ÄÏπò ÎåÄÎπÑ ÏñºÎßàÎÇò ÎÜíÏùÄÏßÄÎ•º ÎÇòÌÉÄÎÇ¥Î©∞, 1Î≥¥Îã§ ÎÇÆÏúºÎ©¥ Ï†ÄÌèâÍ∞ÄÎ°ú Ìï¥ÏÑùÎêòÍ∏∞ÎèÑ Ìï©ÎãàÎã§.</td></tr>
        <tr><td><b>PER</b></td><td>Ï£ºÍ∞ÄÏàòÏùµÎπÑÏú®</td><td>Ïù¥Ïùµ ÎåÄÎπÑ Ï£ºÍ∞Ä ÏàòÏ§ÄÏùÑ ÎÇòÌÉÄÎÉÖÎãàÎã§. ÎÇÆÏùÑÏàòÎ°ù Ïù¥Ïùµ ÎåÄÎπÑ Ï†ÄÎ†¥Ìïú Í∏∞ÏóÖÏûÖÎãàÎã§. ÎÜíÏùÑÏàòÎ°ù ÏãúÏû•Ïùò Í∏∞ÎåÄÏπòÍ∞Ä ÎÜíÏäµÎãàÎã§.</td></tr>
        <tr><td><b>ROE</b></td><td>ÏûêÍ∏∞ÏûêÎ≥∏Ïù¥ÏùµÎ•†</td><td>ÏûêÎ≥∏ÏùÑ(Î∂ÄÏ±Ñ ÎØ∏Ìè¨Ìï®) ÏñºÎßàÎÇò Ìö®Ïú®Ï†ÅÏúºÎ°ú Ïö¥Ïö©Ìï¥ Ïù¥ÏùµÏùÑ ÎÉàÎäîÏßÄÎ•º ÎÇòÌÉÄÎÉÖÎãàÎã§.</td></tr>
        <tr><td><b>ROA</b></td><td>Ï¥ùÏûêÏÇ∞Ïù¥ÏùµÎ•†</td><td>Ï¥ùÏûêÏÇ∞(Î∂ÄÏ±Ñ Ìè¨Ìï®) ÎåÄÎπÑ ÏàòÏùµÎ•†Î°ú, Î≥¥ÏàòÏ†ÅÏù∏ ÏàòÏùµÏÑ± ÏßÄÌëúÏûÖÎãàÎã§.</td></tr>
        <tr><td><b>ICR</b></td><td>Ïù¥ÏûêÎ≥¥ÏÉÅÎπÑÏú®</td><td>ÏòÅÏóÖÏù¥ÏùµÏúºÎ°ú Ïù¥ÏûêÎπÑÏö©ÏùÑ ÏñºÎßàÎÇò Í∞êÎãπÌï† Ïàò ÏûàÎäîÏßÄ ÎÇòÌÉÄÎÉÖÎãàÎã§.</td></tr>
        <tr><td><b>FCFÏàòÏùµÎ•†</b></td><td>-</td><td>ÏûêÏú†ÌòÑÍ∏àÌùêÎ¶Ñ(FCF)ÏùÑ ÏãúÍ∞ÄÏ¥ùÏï°ÏúºÎ°ú ÎÇòÎàà ÎπÑÏú®Î°ú, Ïù¥ ÎπÑÏú®Ïù¥ ÎÜíÏùÑÏàòÎ°ù Í∏∞ÏóÖÏù¥ Ï∞ΩÏ∂úÌïòÎäî ÌòÑÍ∏à ÎåÄÎπÑ Ï£ºÍ∞ÄÍ∞Ä Ï†ÄÌèâÍ∞ÄÎêòÏóàÏùåÏùÑ ÏùòÎØ∏Ìï©ÎãàÎã§.</td></tr>
        <tr><td><b>FCFÏÑ±Ïû•Î•†</b></td><td>-</td><td>ÏµúÍ∑º 5ÎÖÑÍ∞Ñ ÏûêÏú†ÌòÑÍ∏àÌùêÎ¶ÑÏùò ÏÑ±Ïû•Î•†ÏùÑ ÎÇòÌÉÄÎÇ¥Îäî ÏßÄÌëúÏûÖÎãàÎã§.</td></tr>
        <tr><td><b>EPS</b></td><td>Ï£ºÎãπÏàúÏù¥Ïùµ</td><td>ÏµúÍ∑º 5ÎÖÑÍ∞Ñ 1Ï£ºÎãπ Í∏∞ÏóÖÏù¥ Ï∞ΩÏ∂úÌïú ÏàúÏù¥ÏùµÏùò ÏÑ±Ïû•Î•†Î°ú, ÏàòÏùµÏÑ±Í≥º ÏÑ±Ïû•ÏÑ± ÌåêÎã®Ïóê Ïú†Ïö©Ìï©ÎãàÎã§.</td></tr>
        <tr><td><b>Î∞∞ÎãπÏÑ±Ïû•Î•†</b></td><td>-</td><td>ÏµúÍ∑º 10ÎÖÑÍ∞Ñ Î∞∞ÎãπÍ∏àÏùò ÏÑ±Ïû•Î•†ÏùÑ ÎÇòÌÉÄÎÇ¥Îäî ÏßÄÌëúÏûÖÎãàÎã§.</td></tr>
        <tr><td><b>ÏòÅÏóÖÏù¥ÏùµÎ•†</b></td><td>-</td><td>ÏµúÍ∑º 4Í∞ú ÏòÅÏóÖÎÖÑÎèÑ/Î∂ÑÍ∏∞Ïùò ÌèâÍ∑† ÏòÅÏóÖÏù¥ÏùµÎ•† ÏÑ±Ïû•Î•†Î°ú, Í∏∞ÏóÖÏùò ÏàòÏùµÏÑ± ÏàòÏ§ÄÏùÑ Î≥¥Ïó¨Ï§çÎãàÎã§.</td></tr>
        <tr><td><b>Î™®Î©òÌÖÄ</b></td><td>-</td><td>Ï£ºÍ∞ÄÏùò Ï§ëÏû•Í∏∞ ÏÉÅÏäπ ÌùêÎ¶ÑÏùÑ Î∞òÏòÅÌïú ÏßÄÌëúÎ°ú, Ï£ºÍ∞ÄÏùò ÌÉÑÎ†•Í≥º Ï∂îÏÑ∏Î•º ÌèâÍ∞ÄÌï©ÎãàÎã§.</td></tr>
        <tr><td><b>ESG</b></td><td>-</td><td>Í∏∞ÏóÖÏùò ÏßÄÏÜçÍ∞ÄÎä•ÏÑ±ÏùÑ ÎÇòÌÉÄÎÇ¥Îäî ÏßÄÌëúÎ°ú, ÎèôÏ¢ÖÏóÖÍ≥Ñ ÎåÄÎπÑ ÏàòÏ§ÄÍ≥º Ìï®Íªò ÌèâÍ∞ÄÌï©ÎãàÎã§.</td></tr>
        <tr><td><b>CVaR</b></td><td>Ï°∞Í±¥Î∂Ä ÏúÑÌóòÍ∞ÄÏπò</td><td>Ìè¨Ìä∏Ìè¥Î¶¨Ïò§Í∞Ä Í∑πÎã®Ï†ÅÏù∏ ÏÜêÏã§ÏùÑ Í≤™ÏùÑ Í≤ΩÏö∞, ÏÜêÏã§Ïù¥ Î∞úÏÉùÌïòÎäî ÏµúÏïÖ 5% Íµ¨Í∞Ñ ÎÇ¥ÏóêÏÑú ÌèâÍ∑†Ï†ÅÏúºÎ°ú ÏñºÎßàÎÇò ÏÜêÏã§Ïù¥ Î∞úÏÉùÌïòÎäîÏßÄÎ•º ÎÇòÌÉÄÎÇ¥Îäî ÏßÄÌëúÏûÖÎãàÎã§.</td></tr>
        <tr><td><b>Sortino Ratio</b></td><td>ÏÜåÌã∞ÎÖ∏ ÏßÄÏàò</td><td>ÏàòÏùµÎ•† ÎåÄÎπÑ ÌïòÎ∞© ÏúÑÌóò(ÏÜêÏã§ Î≥ÄÎèôÏÑ±)ÏùÑ Í≥†Î†§Ìïú ÏúÑÌóò Ï°∞Ï†ï ÏàòÏùµÎ•†ÏùÑ ÎÇòÌÉÄÎÉÖÎãàÎã§. Í∞íÏù¥ ÎÜíÏùÑÏàòÎ°ù ÌïòÎ∞© ÏúÑÌóò ÎåÄÎπÑ ÏàòÏùµÎ•†Ïù¥ Ïö∞ÏàòÌï®ÏùÑ ÏùòÎØ∏Ìï©ÎãàÎã§.</td></tr>
        <tr><td><b>Variance</b></td><td>Î∂ÑÏÇ∞</td><td>Ìè¨Ìä∏Ìè¥Î¶¨Ïò§ ÏàòÏùµÎ•†Ïùò Î≥ÄÎèôÏÑ±ÏùÑ ÎÇòÌÉÄÎÇ¥Îäî ÏßÄÌëúÎ°ú, ÏúÑÌóò ÏàòÏ§Ä ÌèâÍ∞ÄÏóê ÏÇ¨Ïö©Îê©ÎãàÎã§. Í∞íÏù¥ ÎÇÆÏùÑÏàòÎ°ù ÏïàÏ†ïÏ†ÅÏù∏ Ìè¨Ìä∏Ìè¥Î¶¨Ïò§ÏûÑÏùÑ ÎúªÌï©ÎãàÎã§.</td></tr>
        <tr><td><b>Sharpe Ratio</b></td><td>ÏÉ§ÌîÑ ÏßÄÏàò</td><td>Ìè¨Ìä∏Ìè¥Î¶¨Ïò§Ïùò Ï¥àÍ≥º ÏàòÏùµÎ•†ÏùÑ ÌëúÏ§ÄÌé∏Ï∞®Î°ú ÎÇòÎàà ÏßÄÌëúÎ°ú, ÏúÑÌóò ÎåÄÎπÑ ÏàòÏùµÎ•†ÏùÑ ÌèâÍ∞ÄÌï©ÎãàÎã§. Í∞íÏù¥ ÌÅ¥ÏàòÎ°ù Ìö®Ïú®Ï†ÅÏù∏ Ìà¨ÏûêÏûÑÏùÑ ÎÇòÌÉÄÎÉÖÎãàÎã§.</td></tr>
        <tr><td><b>Sentiment Score</b></td><td>Í∞êÏÑ± Ï†êÏàò</td><td>ÌÖçÏä§Ìä∏Ïùò Í∏çÏ†ï ÎòêÎäî Î∂ÄÏ†ï Ï†ïÎèÑÎ•º ÏàòÏπòÌôîÌïú ÏßÄÌëúÎ°ú, Ìà¨Ïûê Ïã¨Î¶¨ÎÇò Îâ¥Ïä§ Î∞òÏùëÏùÑ Ï†ïÎüâÏ†ÅÏúºÎ°ú ÌèâÍ∞ÄÌï©ÎãàÎã§. Í∞íÏù¥ ÎÜíÏùÑÏàòÎ°ù Í∏çÏ†ïÏ†ÅÏù∏ Ï†ïÏÑúÏûÑÏùÑ ÎÇòÌÉÄÎÉÖÎãàÎã§.</td></tr>
      </tbody>
    </table>

    <p style="margin-top: 20px; font-size: 14px; color: #444;">
    Î≥∏ ÏûêÎ£åÎäî <strong>ÏõåÎü∞ Î≤ÑÌïèÏùò 'Í∞ÄÏπòÌà¨Ïûê'</strong> Ï≤†ÌïôÏùÑ Í∏∞Î∞òÏúºÎ°ú,<br>
    Í∏∞ÏóÖÏùò Ïû¨Î¨¥ Í±¥Ï†ÑÏÑ±Í≥º Ï£ºÍ∞ÄÏùò Ï∂îÏÑ∏Î•º ÏàòÏπòÌôîÌïòÏó¨ ÌèâÍ∞ÄÌïú Í≤∞Í≥ºÏûÖÎãàÎã§.<br>
    Î≥∏ ÏûêÎ£åÎäî Ï†ïÎ≥¥ Ï†úÍ≥µ Î™©Ï†ÅÏúºÎ°úÎßå ÏÇ¨Ïö©ÎêòÎ©∞, Ìà¨Ïûê ÏÜêÏã§Ïóê ÎåÄÌïú Î≤ïÏ†Å Ï±ÖÏûÑÏùÄ ÏßÄÏßÄ ÏïäÏäµÎãàÎã§.
    </p>

    <p><em>Ìï¥Îãπ Î©îÏùºÏùÄ Îß§Ï£º Ìôî, ÌÜ† Ïò§Ï†Ñ 8ÏãúÏóê ÏûêÎèô Î∞úÏÜ°Îê©ÎãàÎã§.</em></p>
  </body>
</html>
"""

msg.add_alternative(html_content, subtype="html")

with open(excel_path, "rb") as f:
    msg.add_attachment(
        f.read(),
        maintype="application",
        subtype="vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        filename="DeepFund_Weekly_Insights.xlsx",
    )

with smtplib.SMTP_SSL("smtp.gmail.com", 465) as smtp:
    smtp.login(EMAIL, PASSWORD)
    smtp.send_message(
        msg, to_addrs=recipients
    )  # send_message's to_addrs param controls actual recipients
